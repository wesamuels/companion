[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Companion to the Nursing Ph.D. & DNP Statistics Curricula",
    "section": "",
    "text": "Preface\n\n\n\n\n\nThis “book” is a companion to the statistics courses offered as part of the Ph.D. in Nursing Research program at Hunter College, CUNY. It also provides supplemental materials for Hunter’s Doctor of Nursing Practice program.\nVery much a work in progress—and anyway intended to be in addition to and not instead of the many resources elsewhere—this companion currently contains:\n\nLectures and essays on topics covered in the applied statistics curriculum:\n\nNURS 60N lectures & materials (Chap. 1)\nNURS 915 & 916 lectures & materials (Chap. 2)\nA hands-on guide to writing Results sections (Chap. 3)\nDiscussions and activities expand upon the statistical concepts of significance (Chap. 4), correlations & partial/semipartial correlations (Chap. 5), effect size (Chap. 6), missing data (Chap. 7), and model building (Chaps. 8 – 10)\n\nLectures and essays on topics covered in the introduction to psychometrics course, NURS 925 (Chaps. 11 & 12)\nStep-by-step guides to using various stat-related software (Chaps. 13 – 17)\nTables of common statistical abbreviations and terms along with their meanings (Apps. A & B)\nA small collection of decision trees (flowcharts) to decide which analysis to conduct (App. C)\n\nColophon\n\nThis companion is created 2022 – present by William Ellery Samuels, Ph.D., under a Creative Commons Attribution–NonCommercial–ShareAlike license. (Note that the “Published” date given above is in fact the date of the most-recent revisions.)\nLight-themed HTML-version typefaces are Bona Nova for text, Ubuntu Mono for code blocks, and Latin Modern Math for formulas. The dark HTML is theme is unmodified solar, which is based on Ethan Schoonover’s eye-friendly Solarized dark theme.\nPDF-version typefaces are TeX Gyre Bonum for both text & formulas and TeX Gyre Adventor for code blocks.\nOpenAI’s ChatGPT has been occasionally used for clarity and content review—and extensively to debug code errors and issues (I’m looking at you, Quarto).\nThis Companion is produced through Bookdown in the RStudio environment with:\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.1 (2024-06-14)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2025-06-17\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n   cli           3.6.3   2024-06-21 [1] CRAN (R 4.4.1)\n   digest        0.6.36  2024-06-23 [1] CRAN (R 4.4.1)\n P evaluate      0.24.0  2024-06-10 [?] CRAN (R 4.4.1)\n P fastmap       1.2.0   2024-05-15 [?] CRAN (R 4.4.0)\n P htmltools     0.5.8.1 2024-04-04 [?] CRAN (R 4.4.0)\n P htmlwidgets   1.6.4   2023-12-06 [?] CRAN (R 4.4.1)\n P jsonlite      1.8.8   2023-12-04 [?] CRAN (R 4.4.0)\n   knitr         1.48    2024-07-07 [1] CRAN (R 4.4.1)\n   renv          1.0.3   2023-09-19 [1] CRAN (R 4.4.0)\n P rlang         1.1.4   2024-06-04 [?] CRAN (R 4.4.1)\n P rmarkdown     2.27    2024-05-17 [?] CRAN (R 4.4.0)\n P rstudioapi    0.16.0  2024-03-24 [?] CRAN (R 4.4.1)\n P sessioninfo   1.2.2   2021-12-06 [?] CRAN (R 4.4.0)\n P xfun          0.45    2024-06-16 [?] CRAN (R 4.4.1)\n   yaml          2.3.9   2024-07-05 [1] CRAN (R 4.4.1)\n\n [1] /home/wes/OneDrive/Taught/2_Statistics/Companion_to_the_Statistical_Curriculum_for_the_Nursing_PhD/renv/library/R-4.4/x86_64-pc-linux-gnu\n [2] /home/wes/.cache/R/renv/sandbox/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\\[\\circledcirc--\\oint============\\succ\\] \n\\[\\text{七転八起}\\]"
  },
  {
    "objectID": "curricula.html",
    "href": "curricula.html",
    "title": "Applied Statistics Curriculum",
    "section": "",
    "text": "The content of the sequence of stat courses is:\nNURS 60N\n\nMajor Principles\n\nRandomness & Variables\nSamples vs. Populations\n\nEvaluation vs. Research\n\nDescriptive vs. Inferential Statistics\n\nParametric vs. Non-Parametric Analyses\n\nRelated to it is this [online Demonstration\n\nFrequencies & Counts\n\nFrequencies & relative frequencies\nProbabilities\n\nRisks & risk ratios\nHazards & hazard ratios\n\nOdds & odds ratios\nContingency / cross tables\n\nFisher’s exact test\n\nImportant distributions\n\nNormal distribution\nχ² distribution\n\n\nMeasuring & Testing Differences\n\nReview of Assumptions in Inferential Statistics\nHypothesis Testing\nSignal-to-Noise Ratio\nCommon Tests: t & F\n\nPower and Effect Size\n\nReview & Elaboration of Hypothesis Testing\nPower\nEffect Size\n\nAssociation & Causation\n\nIndividual Differences and Correlations\nTypes of Correlation Statistics\nPartial and Semipartial Correlations\nConcerning Causality\n\nThe ANOVA Family of Tests\n\nBasic Concepts of ANOVAs\nMain Effects & Interactions\n\nR² & η²\n\nReading Source Tables\nTypes of ANOVAs\nFamily-Wise Error, Post hoc, & Planned Comparisons\n\n\nNURS 915 & 916\n\nOverview & Review\nHandling Data\nPresenting Data\nPower & Significance\n\nPost hoc power\nSample size estimation\n\nIntroduction to Linear Regressions\n\nMethod of Ordinary Least Squares\nModel Assumptions\n\nOrdinary Least Squares & Maximum Likelihood Estimation\n\nGeneral & Generalized Linear Models\n\nTests of Model Fit\n\nInformation Criteria\nResidual Analysis\nStepwise Analysis\nBootstrapping\nMissing Values & Outliers\n\nOccurrence, Association, & Causation\n\nCounterfactuals & Hill’s Criteria\nConfounds, Mediators, & Moderators\n\nLogistic Regression\n\nMultinomial & Ordinal Logistic Regression\n\nHierarchical Regression\nLongitudinal Analyses\n\nPre–Post Differences (“Differences in Differences”)\n(Repeated-Measures) ANCOVAs with Pretest as Covariate\nMultilevel Models of Change\nInterrupted Time Series Analysis\n\nRobust Statistics\n\nBootstrapping\nMissing Values & Outliers\n\nStructural Equation Modeling\n\nNURS 925\n\nFoundations of Measurement and Scaling\n\nPsychophysics & Psychometrics\n\nValidity\n\nTraditional, Trinity View\nThe 1999–2014 Standards & Validity as “Use”\n\nReliability\n\nClassical Measurement Theory View\nAs a Measure of a Unitary Construct\nAs a Measure of Internal Consistency\n\nCronbach’s α\nKuder-Richardson Formulae 20 & 21\n\nOther Forms (Test-Retest, etc.)\n\nFactor Analysis\n\nConcept and Basic Ideas\nEigenvalues\nExploratory Factor Analysis\n\nUses and abuses \n\nConfirmatory Factor Analysis\n\nMeasures of Model Fit\n\n\nReturn to Structural Equation Modelling"
  },
  {
    "objectID": "60N.html#sec-random_variables",
    "href": "60N.html#sec-random_variables",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.1 Major Principles ",
    "text": "1.1 Major Principles \n\nRandomness & Variables\nSamples vs. Populations\n\nEvaluation vs. Research\n\nDescriptive vs. Inferential Statistics\n\nParametric vs. Non-Parametric Analyses\n\n\nRelated to it is this online demonstration of the Central Limit Theorem"
  },
  {
    "objectID": "60N.html#sec-freqencies_and_counts",
    "href": "60N.html#sec-freqencies_and_counts",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.2 Frequencies & Counts ",
    "text": "1.2 Frequencies & Counts \n\nFrequencies & Relative Frequencies\nProbabilities\n\nRisks & Risk Ratios\nHazards & Hazard Ratios\n\nOdds & Odds Ratios\nContingency / cross tables\n\nFisher’s Exact Test\n\nImportant Distributions\n\nNormal Distribution\nχ² Distribution"
  },
  {
    "objectID": "60N.html#sec-hypothesis_testing",
    "href": "60N.html#sec-hypothesis_testing",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.3 Measuring & Testing Differences ",
    "text": "1.3 Measuring & Testing Differences \n\nReview of Assumptions in Inferential Statistics\nHypothesis Testing\nSignal-to-Noise Ratio\nCommon Tests: t & F"
  },
  {
    "objectID": "60N.html#sec-power",
    "href": "60N.html#sec-power",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.4 Power and Effect Size ",
    "text": "1.4 Power and Effect Size \n\nReview & Elaboration of Hypothesis Testing\nPower\nEffect Size"
  },
  {
    "objectID": "60N.html#sec-association_and_causation",
    "href": "60N.html#sec-association_and_causation",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.5 Association & Causation ",
    "text": "1.5 Association & Causation \n\nIndividual Differences and Correlations\nTypes of Correlation Statistics\nPartial and Semipartial Correlations\nConcerning Causality"
  },
  {
    "objectID": "60N.html#sec-intro_to_anovas",
    "href": "60N.html#sec-intro_to_anovas",
    "title": "1  NURS 60N: Foundations of Biostatistics for Nursing Research and Evidence-Based Practice",
    "section": "1.6 The ANOVA Family of Tests ",
    "text": "1.6 The ANOVA Family of Tests \n\nBasic Concepts of ANOVAs\nExample of Main Effects & Interactions\nTypes of ANOVAs\nt- & F-Tests Roles & Nature\nPost hoc Comparisons\nReference Tables for ANOVA Terms"
  },
  {
    "objectID": "915.html#sec-fundamental_01",
    "href": "915.html#sec-fundamental_01",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.1 Review of Some Major Principles & Practices in Biostatistics ",
    "text": "2.1 Review of Some Major Principles & Practices in Biostatistics \n\nVariability & Randomness\nLevels of Measurement\nDescriptive & Inferential Statistics\nSources of Variance and the Signal-to-Noise Ratio\nDesigning and Answering Questions\nHypothesis Testing\n\nRecording of Lecture"
  },
  {
    "objectID": "915.html#sec-presenting_results_lecture",
    "href": "915.html#sec-presenting_results_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.2 Presenting Results ",
    "text": "2.2 Presenting Results \n\nVisualization\n\nSelf Sufficiency\nEfficient Information Transfer\nData-to-Ink Ratio\nFollow Conventions & Readers’ Expectations\n\n\n\n2.2.1 Writing Results \n\nWriting Results Sections\n\nTell a Story\nUse Figures & Tables as Talking Points\nUse Statistics as Citations to Support Assertions\n\n\nAdditional information and resources are given in Chapter 3."
  },
  {
    "objectID": "915.html#sec-testing_hypotheses_lecture",
    "href": "915.html#sec-testing_hypotheses_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.3 Testing Hypotheses ",
    "text": "2.3 Testing Hypotheses \n\nReview of Assumptions in Inferential Statistics\nHypothesis Testing\nSignal-to-Noise Ratio\nCommon Tests: t & F"
  },
  {
    "objectID": "915.html#sec-fundamentals_of_linear_regression_lecture",
    "href": "915.html#sec-fundamentals_of_linear_regression_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.4 Fundamentals of Linear Relationships ",
    "text": "2.4 Fundamentals of Linear Relationships \n\nReview of Correlations & Partial Correlations\n\nAdditional information and resources are given in Chapter 5.\n\nLinear Models vs. Correlation\nLinear Models vs. ANOVAs\nLinear Models: Signal-to-Noise\nGeneralized Linear Models & Link Functions\nEvaluating Distributions: Q-Q Plots\nFurther Concepts in Linear Regression\n\nDummy Variables\nMulticollinearity\nIndependence of Cases\n\n\nRecording of Lecture - A Zoom recording from a previous semester\nAdditional information and resources—including steps to conducting them in SPSS—are given in Chapters 8 and 9"
  },
  {
    "objectID": "915.html#sec-testing_models_lecture",
    "href": "915.html#sec-testing_models_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.5 Testing Models Theoretically ",
    "text": "2.5 Testing Models Theoretically \n\nReview of Linear Regression Model\nPartialling out Variance\nCombining Similar Sources of Variance\nOstensible & Non-Ostensible Variables\nModel Fit\n\nRecording 1 - A Zoom recording from a previous semester, this recording contains a review of linear models and introduction to tests of model fits.\nRecording 2 - An other Zoom recording from a prior semester, this covers an explanation of ANOVAs and their qualities vs. general linear models"
  },
  {
    "objectID": "915.html#sec-longitudinal_analyses_lecture",
    "href": "915.html#sec-longitudinal_analyses_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.6 Analyses of Longitudinal Data ",
    "text": "2.6 Analyses of Longitudinal Data \n\nLongitudinal analyses, including some of their benefits and challenges\nA brief comparison of the merits of pre-post difference scores, including pretest covariates in ANCOVAs, and repeated-measures ANOVAs.\nAn introduction to the sorts of multilevel models of change that Singer & Willett (2003) describe\n\nRecording\nAdditional information and resources—including steps to conducting them in SPSS—are all currently located in Chapter 10."
  },
  {
    "objectID": "915.html#sec-logistic_regression_lecture",
    "href": "915.html#sec-logistic_regression_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.7 Logistic Regresssion ",
    "text": "2.7 Logistic Regresssion \n\nLogistic regression vs. general linear regression\nExplanation of the math\nTesting effects & model fit\nTypes of logistic regression\nExamples"
  },
  {
    "objectID": "915.html#sec-sem_lecture",
    "href": "915.html#sec-sem_lecture",
    "title": "2  NURS 915 & 916: Applied Statistics 1 & 2",
    "section": "2.8 Structural Equation Modeling ",
    "text": "2.8 Structural Equation Modeling \nStructural equation models can be considered a sort of bridge between generalized linear regression and the factor analyses we’ll cover in NURS 925, Psychometrics.\n\nCore Concepts\nMechanics of SEMs\nComparing Models\nExample of SEMs"
  },
  {
    "objectID": "writing_results.html#overview",
    "href": "writing_results.html#overview",
    "title": "3  Writing Results Sections",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nThis chapter is in some ways an abbreviated version of Section 8.3: Introduction to Linear Regression Models in Chapter 8. This sheet is intended to orient one subset of those analyses toward working within Word to write results and less on explaining how to use SPSS to prepare and analyze data. We will thus:\n\nReview a few options in SPSS that will help prepare tables and figures for addition to a Results section created in Word.\nWalk through a set of analyses that—in miniature—emulate the sorts of exploratory analyses one might do with the data on executive functioning slopes and adolescent academic performance & disciplinary actions.\nAdd a few tables and figures from those analyses to a Word document that uses an APA-formatted template\nWrite pieces of a Results section around those tables and figures"
  },
  {
    "objectID": "writing_results.html#setting-global-options-in-spss-and-using-a-word-template",
    "href": "writing_results.html#setting-global-options-in-spss-and-using-a-word-template",
    "title": "3  Writing Results Sections",
    "section": "3.2 Setting Global Options in SPSS and Using a Word Template",
    "text": "3.2 Setting Global Options in SPSS and Using a Word Template\n\n3.2.1 Setting Global Options in SPSS\nThe following two action simply change the fonts for the figures and tables we’ll create. This is, of course, a small change, but most manuscripts use a serif font—and often the vaunted1 Times New Roman font. Now, all of your figures and tables will use that font.\n\nCharts\n\nIn the SPSS Edit &gt; Options menu, click on the Charts tab in the dialogue that appears\nChange the Style cycle preference from Cycle through color only to Cycle through patterns only\nNow, change the Font from SansSerif to Times New Roman.\n\n\n\nTables\n\nUnder Edit &gt; Options menu, now click on the Pivot Tables tab in the dialogue that appears\nUnder TableLook, select APA_TimesRoma_12pt\nNot all table will now be formatted this way, but the pivot tables and a few more will\n\n\n\n\n3.2.2 Using a Word Template\nUsing templates takes some getting used to, but it can be both timed saved in the long run and even required by publishers like Elsevier, SAGE, and Springer.\nTo use a template in Word:\n\nDownload this Word template: APA_7th_Ed_Template.dotx\n\nNote that the extension for that file is .dotx—not .docx. That “t” denotes that this is a template2\n\nAfter saving that file to a more useful location than the downloads folder (ahem), open it in Word.\n\nNote that I’ve added some language into the template to help you both know most of the elements in it and to serve as a structure for manuscripts. You can removed any and all of the text in the template (or, of course, replace it with your own); the “magic” of a template isn’t in the textual content. Indeed, that’s the point of a template: to separate the actual content that’s written from the styling of that content. The\n\nChoose to Save as a .docx file—not a .dotx file—perhaps calling it something like “N916_Results_Exercise.docx.” This will preserve the template and change the extension to what is now the correct format since we will indeed be adding content and making this no longer a template to be used for other document.\n\nI have given a fuller explanation of how to use Word templates in Chapter 13, so I will not cover the details of accessing and updating styles and sections in this guide."
  },
  {
    "objectID": "writing_results.html#data-prep-and-cleaning",
    "href": "writing_results.html#data-prep-and-cleaning",
    "title": "3  Writing Results Sections",
    "section": "3.3 Data Prep and Cleaning",
    "text": "3.3 Data Prep and Cleaning\nThe data that we will be using in the EF_Slope_Data.sav file are pretty clean and ready, but there are nonetheless a few tasks we will perform on them. Although these are not real data, they are loosely based on real data, in any case, please treat them as if they were real.\nLoad those data, either with the initial dialogue box that opens by default when SPSS loads or by choosing File &gt; Open &gt; Data... from the menu bar and selecting EF_Slope_Data.sav wherever you have it stored.\n\n3.3.1 Adding Variable Value Labels\n\nMost of the dummy-coded data have their values labeled, but DBT does not. After shaking your head at the forgetfulness of your instructor, open the Variable View tab of the Data Editor window.\nClick on the ellipsis button that appears when you click on the cell that is intersected by the DBT row and Values column.\nIn the Value Labels dialogue box that opens, enter a 0 (a zero) in the Value: field and Did Not Participate in the Label: field; then click on the Add button.\nNow, enter a 1 in the Value: field and Participated in the Label: field before clicking Add and then OK.\n\n\n\n3.3.2 Changing Measurement Scale\n\nWhen looking in the Variable View tab in the Data Editor window, we see that that same DBT variable is listed in the Measure field as a Scale variable, indicating that SPSS is treating it like a real number. A zero in a dummy variable indicates that that trait is not present (that the student did not participate in the DBT program, the student is not female, does not experience excessive economic hardship, etc.); the zero is not a zero on a continuous scale.\nWe can change this easily enough in the Variable View tab of the Data Editor window. In that tab, click on the Measure cell in the DBT row. From the drop-down menu, choose to change it to Nominal.\nNote that the Type will remain Numeric. Changing the Measure to Nominal lets SPSS know that this number simply indicates the presence of absence of the trait denoted by the dummy variable.\n\n\n\n3.3.3 Creating Standardized Variables\nI computed the slopes from non-standardized scores. The mean for these slopes is nearly zero (e.g., -0.0018 for Beh_Reg_T_Slope), but if we convert them to truly standardized scores, we would occur these advantages:\n\nBeing z-scores, you can get a sense of significant values (and differences): a z-score of ~1.65 is significantly different than zero in a one-tailed test where α = .05 (and two z-scores that differ by ~1.65 are significantly different).\n\nz-Scores of ~1.96 are significantly different at α = .05 in a two-tailed test. Smaller z-score values will be significant when the degrees of freedom for formal tests are larger.\nComparisons between standardized variables can be made directly since they are on the same scale. This is true not only within your data set, but also—to some extent—between sets of data. (This is contingent on the assumption that any measurement biases are similar between the two sets of data.)\nYou can remove the intercept term from models where all variables are standardized. This frees up a degree of freedom and makes for a somewhat easier interpretation of the model terms.\n\nSometimes (actually, often) predictors are correlated with the intercept. Removing the intercept by standardizing our variables lets us “remove” the effect of the intercept and instead let that variance be placed in with the variables of interest. This can clarify our analyses, especially by usually reducing the chance of a Type 2 error.\n\nSimilarly, if some of the predictors are strongly inter-correlated (there is strong “collinearity” as it’s called), standardizing and removing the intercept may help reduce that.\n\n\nThe model weights for each variable will be expressed in the same scale as a correlation; if we square these standardized weights, we obtained the proportion of variance accounted for in the criterion by that particular predictor (more on this later; I just wanted to add it to the list here).\n\n\nGenerating standardized scores in SPSS is a bit counter-intuitive. To create them:\n\nClick on Analyze &gt; Descriptive Statistics &gt; Descriptives\nIn the dialogue box that opens, select the variables you would like to standardize. Let’s select all of the variables that are slopes.\nIn that same dialogue box, select Save standardized values as variables under the list of variables to choose from which to add to the Variable(s) pane:\n\n\n\nThis will generate descriptive statistics in the Output window, but our interest right now is the changes made to the data set itself: At the far right of the matrix—just past Disc_Inc_Y4, is a set of new variables. You will see that these are the slope variables, each now prepended with a Z to indicate that these are z-scores, i.e., standardized.\n(Note that you can also compute standardized scores in the Transform &gt; Compute Variables dialogue. This interface is a lot more flexible, but doesn’t have a simply function to standardize.)\nWe’ll be using ZAll_EFs_SR_Slope more, so let’s move up higher in the list for easier access. To do this, go the Variable View tab in the Data Editor, and scroll down to the set of standardized variables. Single left-click on ZAll_EFs_SR_Slope; holding the mouse button down, drag that variable up, e.g., to right before the non-standardized set, right below Same_Gender.\n(We can move more than one variable this way. We could, e.g., left on the first standardized variable (ZBeh_Reg_T_Slope), then hold down the Shift key and sing left-click on the last one (ZSusp_Slope) to select them all. Then single left-click on this highlighted set; keeping the mouse button down, drag this list up.\nSince we’re also using Economic Distress more, we could left-click on ZAll_EFs_SR_Slope, hold down the Ctrl button, left-click on Economic Distress, and then hold the mouse button down to drag them both up. Finally, note as well that we could have done this in the Data View tab, but I think it’s easier in the Variable View.)\n\n\n\n3.3.4 Creating Dummy Variables\nRecall that a dummy variable is a dichotomous variable that indicates whether something is present. For example, Economic Distress here lets 1 denote the presence of economic distress (beyond a pre-established threshold) and lets 0 denote the absence of that much economic distress. Similarly, Gender here lets 1 denote the “presence” of a female and 0 denote “not female.”\nSome linear regression models require that nominal variables be represented by dummy variables. Even when a model doesn’t require it—like an ANOVA—adding variables as dummy variables can help with interpreting the differences between nominal categories (since we don’t have to then conduct post hoc analyses after the ANOVA to see where the differences were).\nFor most of the relevant variables here, I’ve already transformed them into dummy variables. Again, though, there is one I didn’t: Ethnicity. We’ll do that now.\n\nThere are a few ways to create dummy variables in SPSS; we’ll use the method perhaps easiest for when we are creating a good number of dummy variables to accommodate several categories.\nIn SPSS, choose Transform &gt; Create Dummy Variables\nUnder Dummy Variable Labels, select Use values. This will use what is given in the cells of the Ethnicity variable to create dummy variable names. If we had created variable labels (like we did, e.g., for DBT), then we could consider using those labels instead.\nIn the Variables list that appears in the dialogue box that opens, choose Ethnicity to place in the Create Dummy Variables for: field.\nUnder Main Effect Dummy Variables, leave Create main-effect dummies selected.\nEnter a Root Names (One Per Selected Variable):. These will all be ethnicities, of course, so ethnicity is a logical choice here3. (Note that we cannot make the root name the exact same as the name of another variable; for example, here we couldn’t have used Ethnicity as the root name.)\nWe do not have enough levels to create 2- or 3-way effects. If we did, choosing this would allow us to combine levels to look at more complicated relationships between them.\nThe dummy variables should now appear at the right right side of the data matrix.\nAs you can see when looking at these new variables, when SPSS creates dummy variables, it appends the root name (here ethnicity) with a number, creating ethnicity_1, ethnicity_2, etc. without any other explanation in the data about what level is indicated. Therefore, when creating more than one dummy variable (like we did here), it is important to look at SPSS’s output; this shows us what level is being indicated in each variable. I think it’s useful to change the variable names to make it clearer what is being indicated, so we’ll do that after one more step.\nNormally when creating dummy variables, one must first decide what level is the “reference level,” the level against which all other levels will be compared. In some cases, this choice is easy: For the DBT dummy variable, the control group is the clear choice for the reference level; when we set that as the control, then a significant DBT variable effect in a model means there is a significant effect of participating in the DBT program. This means that there is typically one less dummy variable than there are levels to the original variable.\nFor example, with ethnicity, we could set one ethnic category to be the reference—the one against which all others are compared. There’s no a priori reason to choose any particular ethnic group here; going alphabetically, if we made American Indian the reference group for all other levels, then we would not need a separate variable for American Indian: If a person had 0s for all of the dummy variables, that would mean that that person was American Indian. How SPSS creates dummy variables in this method, though, there was no particular group chosen to be the reference group; there is a dummy variable “yes / no” for each level—including for NA, the data were missing.\nThe variable Labels have been made informative by SPSS, so we can see which is which. Nonetheless, the variable names are not informative. We can use this peccadillo of SPSS to take a moment to clean up these dummy variables.\nTo go quickly from the Data View to the right place in the Variable View, double-click on ethnicity_1’s column heading in Data View. This will take you right to that variable in the Variable View.\nSince there are so few American Indians4, we will not be analyzing them, and so we can right-click on whichever of the dummy variables we just created has Ethnicity=American_Indian as its Label in the Variable View. This may be the first dummy variable for you (i.e., ethnicity_1) or another one, perhaps the second (i.e., ethnicity_2).\nIf you cannot right-click, then you can main-click on row number for the American Indian dummy variable, and then either hit the Delete key or select Edit &gt; Clear. This will delete that variable.\nThere are arguably enough Asian-Americans, and certainly enough African- and Hispanic-Americans, so let’s not delete those three. Instead, change the label of the variable whose Label is Ethnicity=Asian to, e.g,. ethnicity_asian. and the others to, e.g., ethnicity_african and ethnicity_latin.\nStill in Variable View, right-click on the row header for whichever variable’s Label is Ethnicity=Multiracial and choose Descriptive Statistics (or click on Analyze &gt; Descriptives &gt; Descriptive Statisitcs...). There are only 5 (7%) students who reported being multiracial; in “real” analyses, I would keep them, but to keep things tidier here, let’s delete this variable, too; with that row still selected (from looking at the descriptives), hit the Delete button to delete it.\nWe can delete whichever dummy variable has nothing after Ethnicity= in its Label; these are missing values since there are many better ways to evaluate missing data. There are enough Whites to change the name of the variable with the Ethnicity=White label to, e.g., ethnicity_european.\n\nWe have now created a nice set of dummy variables. Again, this set does not exhaustively explain the ethnicities of the students. Instead, we have created one that will allow us only to look at the effects of ethnic categories that we expect to be possibly informative.\nOne final word about these dummy variables. How the school records ethnicity, a student can either be Hispanic or Black, White, Asian, etc. Except perhaps for “Multiracial,” a student could not be both Hispanic and, e.g., Black. However, with these dummy variables—if we knew—we could in fact have a student be Hispanic (ethnicity_latin), Black (ethnicity_african), White (ethnicity_european), and Asian (ethnicity_asian) simply by having a 1 in each of those variables (or by creating 2- or 3-way variables in SPSS, but—frankly—I rarely see the value of that over just having a dummy variable for each)."
  },
  {
    "objectID": "writing_results.html#exploring-the-data",
    "href": "writing_results.html#exploring-the-data",
    "title": "3  Writing Results Sections",
    "section": "3.4 Exploring the Data",
    "text": "3.4 Exploring the Data\n\n3.4.1 Descriptives\n\nGo to Analyze &gt; Descriptive Statistics &gt; Descriptives\nTo the Variables field, add Spec_Ed, DBT, Economic_Distress, All_EFs_SR_Slope, and Disc_Inc_Slope. Note that—like Microsoft—SPSS tends to use a lot of tiny little windows for no good reason. You can expand them, but you have to expand each of them. It may be easier to only look at variable names, not also/instead the variable labels. You can do this by right-clicking on one of the variables in the list and selecting Display Variable Names.\nThe Options are fine, but you can add (a few) more stats. Note that Distribution stats are nearly always under-informative at best, misleading at worst\n\nIncluding just the variable noted above should create a table that looks like this:\n\nWe can prettify the table a bit5:\n\nDouble-clicking into the table so that we may edit it\nLeft-click to select the cells that have a whole bunch of decimals places (i.e., The Minimum through Std. Deviation columns in the All Executive Functions Slope - Student Self-Report and the Discipline Incidents Slope rows)\nRight-click and select Cell Properties\nUnder the Format Value tab, change the Decimals (near the bottom) to 2, and click OK\n\nThe table should now look like this:\n\nWhen the table is generated, we can simply right-click on it and select Copy before then pasting it into Word.\n\n\n\n\n\n\nNote\n\n\n\nOlder convention—and some current journals—have tables and figures relegated to the end of a manuscript. This was done because publishers used to actually take a photograph of these elements and include that photo in the published article. Clearly, this is no longer done, and thus this is no longer needed. APA 7th (p. 43) says that “[t]ables and figures may be embedded within the text after they have been mentioned, or each table and figure can be displayed on a separate page” at the end of the manuscript. I have never met anyone who prefers to read a manuscript while flipping back and forth to something at the end, so I recommend indeed placing them within the Results section6.\n\n\nOnce it’s in Word, add a title to this table, e.g., Descriptions of Demographic and Outcome Variables. To do this, please follow the steps in the Adding and Captioning a Figure or Table section of the Using Templates and a Reference Manager with MS Word chapter.\n\n\n3.4.2 Guided Data Exploration\nSPSS’s Explore function can be rather useful. Part of it replicates what we did using Descriptives, but it adds several other ways to view the data and relationships within them.\n\nGo to Analyze &gt; Descriptive Statistics &gt; Explore\nTo Dependent List, add All_EFs_SR_Slope and Disc_Inc_Slope. These are, of course, the continuous variables from the subgroup we’ve been using. We don’t have to use only continuous variables in the Dependent List, however.\nTo the Factor List, add Spec_Ed, DBT, and Economic_Distres\nUnder Plots, leave Boxplots to Factor levels together and perhaps choose both Stem-and-leaf and Histogram\n\nI don’t see the value here in selecting Normality plots and tests, but it could be useful—if overly sensitive—in other instances\n\nUnder Options select Exclude cases pairwise, remember that excluding listwise is really never advisable. Report values would also be useful, except that here we have so many it would overwhelm us.\nUnder Display in the main dialogue, make sure Both is selected. SPSS is good for exploring data since it can easily generate lots of output.\n\nNote that you might get error messages, mostly related to converting “XML characters.” This is simply from using the data across operating systems with different conventions for reading characters7. It is certainly worth reading any error messages you get—and you will get them from time to time. Here, they are not important, though.\n\nStem-and-Leaf Plots\nA stem-and-leaf plots is frequency distribution, but one that also gives information about the values in each stack or “stem.” Stem-and-leaf plots were more commonly used before the easy graphics of computers. They are still quite useful, though, displaying a good amount of information efficiently.\nThe one detailing the All Executive Functions Slope - Student Self-Report scores for students with No Diagnosed Disability looks like this:\n\nThe “Stem” in the plot is the number to the left of the decimal8. In this plot, the “leaves” are the numbers just to the right of the decimal. Finally, the “Frequency” is the number of times those “leaves” appear in the data. The legend at the bottom notes that the stem width here is .1 (which confusingly spans two .1 digits) and that each leaf denotes one case, here one adolescent.\nFor example, that plot has a “Frequency” of 5 for the -1 stem. This means there are five leaves connected to that stem. Those leaves are:\n\n-1.0\n-1.0\n-1.0\n-1.0\n-1.1\n\nI.e., four -1.0s and one -1.1.\nThe next stem—the first -0 stem—indicates that there are five -0.8s and two -0.9s.\nNote that SPSS has truncated the values beyond the .0 or .1. This is commonly done to facilitate presenting the data. Note, too, that SPSS certainly could have grouped some of these stems together, e.g., making the leaves for the same stem be all of the .6s, .7s, .8s, and .9s together after the same -0. stem. Alternatively, it could have divided them up even further. It’s arbitrary, and so one can use whatever seems best.\n\n\nBox-and-Whisker Plots\nLike stem-and-leaf plots, box-and-whisker plots summarize the distribution of scores without making any assumptions about them. They simply describe the distribution. Box-and-whisker plots provide a bit less information than stem-and-leaf plots, but they may be a bit easier to read at a glance.\nThe one for student self-reported GEC slopes by DBT participation is:\n Box-and-whisker plots are easily confused with figures that have error bars on them, but these box-and-whisker plots are different. The thick, black line in the middle of they (here) grey box indicates the median. The grey box around that thick line indicates the range of scores that contains the middle half of the data; in other words, the space between the top of that grey box and that thick line contains the 25% of the scores above the median, and the space between the bottom of that grey box and the thick line contains the 25% of the scores just below the median.\nThe “whiskers” are those thin “Ts” that extend out above and below the grey box; these contain (just about) the upper- and lower-most 25% of the data. I say “just about” since scores that could be considered outliers are shown individually beyond the ends of those whiskers.\nThis figure thus shows a rather small range for the grey box and the whiskers—especially for those who did not participate. That group also shows a lot of outliers. Together, this suggests that the data for student self-reported GEC slopes among those who did not participate in the program has a high peak and long tails. The concomitant histogram also shows that:\n\nLooking again at the box-and-whisker plots, we can see how much variability there is in these data. There are many data that could be considered as outliers and—seen in relation to the whole range of scores—the differences in the medians are not great. This means that whether an adolescent participated in the DBT program provides relatively little information about the self-reported changes in their executive functions—and that there is a lot more yet to be understood about it, given the great variability. Something is differentiating the students, but little of that is the DBT.\nAnd yet, as we’ll see below, the effect of the DBT is still significant. Which should put into perspective a bit what little it can mean for something to be significant.\nNormally, I wouldn’t include box-and-whisker plots in a Results section since they are so raw, but let’s please add to our Word file that one for student self-reported GEC slopes by DBT participation as well as the box-and-whisker plot for discipline incidents over time by DBT participation:\n\nPlease also give them captions when you copy them into the Word file.\n\n\n\n3.4.3 Copying a Figure to Word\n\nRight-click on the figure, nd choose Copy as\n\nChoosing Image will paste it as is, including the changes we made to the chart’s colors, etc., however, this cannot be further edited in Word\nChoosing either Microsoft Office Graphics Object or EMF will paste it without that formatting, but in a version that can be edited in Word\n\nPaste this into Word after the table of descriptives\n\n\n\n3.4.4 Correlations\nI find it often useful to first look at the bivariate (zero-order) relationships between my variables—both predictors and outcomes.\n\nIn SPSS, click on Analyze &gt; Correlate &gt; Bivariate\nSelect what you want, but please include Spec_Ed, DBT, Economic_Distress, All_EFs_SR_Slope, and Disc_Inc_Slope which have some interesting associations\nUnder Options, make sure Missing values are Excluded pairwise. Here as much as anywhere, excluding listwise both removes useful information and likely biases the data we are looking at unless the data really truly are indeed missing completely at random.\nIn the main dialogue box, make sure Flag significant correlations is selected. You can Show only the lower triangle; personally, I think this can be good to present to others, but I like to see both halves for myself.\nIn that main dialogue, we could select to also present Kendall's tau-b and/or Spearman rho.\n\nIf you only included those variables, the first few rows of the correlation matrix generated should look like this:\n\nIf you added in other variables, the correlation matrix may be huge; in any case, they often will be for whatever analyses you do—especially exploratory ones. Because of this, I find it easier to explore them in a spreadsheet, so:\n\nRight-click on the correlation matrix that’s generated in the output and Copy As an Excel Worksheet (BIFF)9\nIn Excel, paste this into an empty sheet\nAlso in Excel, single-left click on the first cell with a correlation value\nUnder the View tab, click on the Freeze Panes button near-ish the middle of the ribbon. This will “freeze” all cells above and to the left of that cell, letting you scroll around the matrix while keeping the labels for the rows and columns visible.\nYou’ll see that the correlation between changes in student self-reported GEC (“All Executive Functions Slope - Student Self-Report”) and DBT is smaller (r = -.165) but significant (n = 326, p = .003). The correlations between DBT participation and changes in “Discipline Incidents Slope” is also significant (r = -.131, n = 124, p = .020). The correlation between “Discipline Incidents Slope” and self-reported GEC isn’t significant (r = .091, n = 153, p = .262), but its association with DBT may make it affect the DBT – student GEC slope relationship.\n\nPlease add this table as well to the Word file after the box-and-whisker plot, and give it a caption, too.\n\n\n\n\n\n\nNote\n\n\n\nPlease remember that the instrument used to measure executive functions here, the BRIEF, is keyed so that lower scores denote greater executive functioning. (It asks respondents to report how often problems with executive functions happen, so higher scores denote more problems.) I nearly reversed-scored it here so that higher scores on all variables denote better things. However, I think it’s a useful exercise to break the habit of relying on that.\nOne reason I say that is in response to finding many beginning writers of research rely on using “positive” to indicate “good” things (“the intervention had a positive effect on recovery”) and “negative” to indicate “bad” things. Although this works fine for ordinary life, it is not defensibly appropriate for research since (1) it implies an inappropriate value judgement being made by the researcher and (2) there certainly are cases were positive numbers indicate less—or less good stuff. Higher values of blood pressure, BMI, A1C, antinuclear antibodies, etc. are often “bad,” and it would be confusing to say that something had a “positive” effect on their levels and then show negative correlations."
  },
  {
    "objectID": "writing_results.html#creating-figures-in-spss",
    "href": "writing_results.html#creating-figures-in-spss",
    "title": "3  Writing Results Sections",
    "section": "3.5 Creating Figures in SPSS",
    "text": "3.5 Creating Figures in SPSS\nExplore creates several useful figures on its own, but SPSS has a pretty good general functionality to create a range of figures. Let’s review a simple but common example.\n\nSelect Graphs &gt; Chart Bulder\nA dialogue will open asking you to ensure that all variables are put on the correct response scales; you can have this not appear, but I like to leave it as a warning to myself. Here, you can simply click OK\nFrom the Gallery, choose Bar and select the first, simplest option: \nDrag that simple bar-graph icon into the chart preview window\nNow, drag All_EFs_SR_Slope to the y-axis and DBT to the x-axis to generate this figure:\n\n\nWe can see that see the differences between the DBT groups much more clearly—but at the expense of a lot of information we know is there about the variability of the slopes.\nBut first, let’s tweak the figure some more:\n\nUnder the Elements Properties tab to the right, select Bar1; we can make changes here to all of the bars. Here, we’re simply going to select to Display error bars that are Confidence intervals of 95%\nClick OK\nDouble-click on the chart\nSingle-right click on the numbers in the y-axis\nChoose Properties from the window that opens (or type Cntl/Cmd + T)\nThe default range (“scale”) of the chart is all right, but it’s easy to change that under the Scale tab\n\nAlthough I would prefer to select Display line at origin here since the origin here is simply zero—that there was no change in student GEC scores\nTimes when would change the range for the y-axis are when I have more than one chart\n\nSelect the Numbers tab, and change the Decimal Places to 2; click Apply—even before moving to another tab in this dialogue.\n\nThe chart should now look something like this:\n\nNote that the 95% confidence intervals suggest that the self-reported executive functions of students who did not participate in the DBT program became significantly worse over these years (i.e., significantly more positive) since the 95% confidence interval for that group did not overlap zero. The change in executive functions for those who did participate overlaps zero, so we can’t say, with sufficient confidence, that there was a change in those (at least not without taking into account other factors)."
  },
  {
    "objectID": "writing_results.html#analyzing-predictors-of-self-reported-gec-slope",
    "href": "writing_results.html#analyzing-predictors-of-self-reported-gec-slope",
    "title": "3  Writing Results Sections",
    "section": "3.6 Analyzing Predictors of Self-Reported GEC Slope",
    "text": "3.6 Analyzing Predictors of Self-Reported GEC Slope\n\n3.6.1 First Model\n\nIn SPSS, select Analyze &gt; General Linear Models &gt; Univariate10\nAdd All_EFs_SR_Slope to the Dependent Variable field. We are seeing how well we can predict a teen’s growth of executive functioning, so this is our outcome.\nMost critically here, we’re interested in whether participating in the DBT program changed—improved—the development of these teen’s executive functioning. So, add DBT to the Fixed Factor(s) field. Usually any nominal variable can be placed in there.\nAdd Spec_Ed as well to the Fixed factor(s) field.\nPlace Disc_Inc_Slop in the Covariate(s) field11\nClick on the Model tab. When using General Linear Models (GLMs), you will nearly always go here to make sure it looks all right and to modify your model. The default is to analyze the Full factorial model; this includes all main effects and interactions—even higher-order interactions (like 3- and 4-way interactions) that are usually futilely hard to interpret let alone communicate. Instead, select to Build terms.\nUnder Build Term(s) (in the middle, between the Factors and Covariates and Model fields), select Main effects\nNow, under Factors and Covariates select all of the terms (single-left click any, then Cntl/Cmd + A) and move them via that arrow to the Model field.\nNow change Build Term(s) to Interaction. Disc_Inc_Slope correlates with both All_EFs_SR_Slope and DBT, so it may moderate the relationship between those two; given this, let’s add the Disc_Inc_Slope \\(times\\) DBT interaction\nThe terms are all either dummy-coded or standardizes (into z scores), so we can de-select Include intercept in model and pat ourselves on the back for using our stats-fu to (slightly) increase the power of our model. This dialogue box should now look like this: \nThe Contrats of our model are all right since the dummy-coding will do all of that for us.\nAlthough we’re not adding any Plots, they can be useful to investigate relationships among predictors.\nThe EM Means are estimated marginal means. These can be useful for investigating our well our model fits our data, so let’s select to compute them for (OVERALL) and DBT. Residual Plots are also informative, so please select that as well.\nUnder Options, please select estimates of effect size, an option that really should be selected by default.\n\nDescriptive statistics is just SPSS trying to make you think it’s doing a lot when it’s just doing the same thing twice.\nObserved power is nearly always useless, even if some think it means anything.\n\n\n\nSource Table of the First Model\nThe Test of Between-Subjects Effects table summarizes the results of this initial model:}\n\nThe parts of this table are:\n\nSource\n\nThe sources of variance—hence why this is called a source table. Here, these include:\n\nModel: The total amount of variance in the data that is accounted for by the whole model, i.e., all of the variables and any interaction terms\nDBT etc: These are the individual terms in the model, one row for each term. Note that the DBT*Disc_Inc_Slope row presents the results for the DBT \\(\\times\\) Discipline Incidents Slope interaction.\nError: This is the variance in the data not accounted for by the model.\nTotal: The total amount of variance in these data. Data sets with more variance have more information—more to learn, but more to figure out and account for.\n\n\nType III Sums of Squares\n\nWe are computing a linear regression (using a general—i.e., not specified/specialized—formula), which—of course—means that a line is plotted through our data12. Unless the model (and the line drawn by it) perfectly fit our data, then each individual’s data point may be some distance off this line. If, overall, the data points fall far from the line, then that means that the line (and the model we’re testing based on it) don’t account for much of whatever’s going on in the data. We compute these distances from the line by squaring their distance; we then add up—sum—these squared distances. This is the Sum of Squares.\nThere are, in fact, different ways to compute this sum of squares, but the third way is currently the most common (and has been for a long time); it is more often used because it can more flexibly account for a wider range of models with different numbers of main and interaction effects.\n\ndf\n\nThese are the degrees of freedom need to estimate the effects of each term (or, in the Error row, that are left over after estimating the model; and, in the Total row, the total number available in the model).\nThe idea is this:\nAgain, there is only so much information in any set of data. One way of thinking about that information is that every person brings a piece of information. We want to convert that “raw” information into insights. However, there are only so many insights we can make, of course. Each insight costs us. How much does an insight cost? Each insight—each numerical value computed to describe an effect—costs one degree of freedom13.\nHow many “insights” are needed to determine the effect of a given variable/term in a model? Well, it depends on how complex a variable/term is. Determining what’s going on in a variable that itself has a lot of levels costs more than it does to see what’s going on in a simple variable with only a few levels. If, for example, we want to understand what the effect of race is on something, then we actually want to figure out what the effect of each racial category is, and so each level will cost us. Each value for each race would need to be calculated … almost. In fact, we get the estimate for one level for “free” because one level’s value is the default level estimated by the overall model; we really just need to determine how much each of those other levels differs from the default level. This “default” level is called the reference level since all other level’s value are given in reference to that one level. Which level we (or the model) sets as the reference only matters if there’s a theoretical reason14; otherwise it doesn’t, it just saves us a degree of freedom.\nA variable/term that only has a few levels takes less to estimate. A dummy variable, with only two levels (has/hasn’t; is/isn’t; etc.) in fact only needs to make one insight: The overall model already estimates what it’s like not to have/be whatever the dummy variable is measuring. I mean, that’s technically true for everything not included in the model, isn’t it? The effect of, say, having freckles isn’t estimated in the model—the issue is when we want to estimate the effect of having freckles. Of course, we can only estimate the effects of factors that were measured, but it is nice to know that measuring effects that are dichotomized as dummy variables makes for a very efficient way of finding insights15.\n\nMean Square\n\nThis is how much information there is in a variable/term divided by how many “insights” that information has to figure out. If a variable has to estimate several values but doesn’t have much information to use to do so, then there likely isn’t much there to worry about16\nHere, then, the whole model’s Sum of Squares is .056, but that information is divided up among four terms, and we must also spend a degree of freedom to estimate the default level for each variable17; we thus must divide that .056 up five ways, and \\(\\frac{.056}{5} = .011\\), the Mean Square value for the whole model.\n\nF\n\nThis is the ratio of how much information there is explained per degree of freedom in a given term relative to how much information is there left to be explained in these data. It’s the Mean Square for a given term divided by the Mean Square for the Error row. I.e., \\(F =\\frac{\\text{Mean Sqaure}_{Term}}{\\text{Mean Square}_{Error}}\\).\nLooking at the Model term again, the Mean Square is .011. The Mean Sqaure for the Error term is .006. So the Mean Square for the model is about double the Mean Square for the error term. Specifically, \\(F = \\frac{.011}{.006} = 1.987\\). This means that, on average, the “insights”—the piece of information in each degree of freedom—made by our model account for about twice as much information as any other “insight” made by any other piece of information that could have been gleaned from our model. Is that good? Well . . .\n\nSig.\n\nPresents the significance level of that term. This is usually (i.e., outside of SPSS) referred to as p, the probability of a false positive (Type 1 error). Convention in the health & social sciences, of course, is to tolerate making a false positive error 5% of the time. I.e., the p-value—the Sig. column—should have a value \\(\\le\\) .05.\nSo, no, explaining about half of the information in the data isn’t enough. The Sig. value is .084—pretty close, but not significant. Since the overall model isn’t significant, none of the individual terms should be either. In the very off chance any were, we wouldn’t be justified to use them since the overall model itself isn’t.\n\nPartial Eta Squared\n\nAn ascendant idea in statistics (and thus quantitative research in general) is that of effect size. Significance (Sig.) determines if an effect is significant, but not how significant. Heck, one could argue that “significance” itself doesn’t really matter—and there are certainly many cases when it doesn’t. Instead, there is a growing conviction to instead make decisions based not only on significance of an effect but also (or instead) on the size of it.\nPartial Eta Sqaured (or partial η2, the lower-case Greek letter “eta”) is a measure of effect size. It is “partial” in the sense of a partial correlation: It’s the effect, e.g., participating in the DBT program after isolating that effect from the other terms in the model (i.e., of partialing out the other effects)18. (Cf. Chapter 6: Effect Size.)\nThere are different ways to compute effect size, but perhaps the simplest is also what’s (essentially) used here. The effect size measure of, e.g., the DBT program is simply the difference between the EF slopes the of DBT participants and the EF slopes of the non-participants. Like usual, though, we then standard this mean difference so that we can compare effect sizes across outcomes and even across studies. And like is often done, we standardize it by dividing it by the standard deviation. So, the formula here is actually: \\[η^2_{\\text{DBT Program Participation}} = \\frac{\\text{Mean}_{\\text{EF Slope of DBT }Participants}- \\text{Mean}_{\\text{EF Slope of DBT }Non-Participants}}{SD_{\\text{DBT Variable}}}\\]\nCohen (1988) laid the foundation for defining effect size and on establishing general criteria for gauging how big is big (and how small is small). He suggested that for η2 (partial or not):\n\nη2 \\(\\le\\) .01 is “small”\nη2 = .06 is “medium”\nη2 \\(\\ge\\) .14 is “large”\n\nIt should—needs to be—noted that Cohen meant these small, medium, and large monikers to be suggestions—not) absolutes. This is important to note since Cohen’s suggestions are already being ossifed as rules that must be used. Large is large if that’s big enough to matter. Small is small if, well, O.K., 1% total variance is small, but still can matter especially for persistent effects.\n\n\nA few more things to note about this source table. First is that the R Squared noted at the bottom. R2 is also a measure of effect size, and one you’ll see reported often. It’s the effect of an entire model, and also expressed as a proportion of total variance. Therefore, this R2 of .063 means that this model accounts for 6.3% of the total variance in the model19. Cohen would suggest that this is a “medium” size effect—even if it is not a significant one.\nSecond, note that the R2 for the model is the same as the “Partial Eta Squared” for the whole model. This will always be the case (within rounding), and both say that 6.3% of the total variance is accounted for in the model20\nThird, SPSS also provides an Adjusted R Squared parenthetically after the un-adjusted one. This is the R2 for the model after adjusting for the number of terms in it. We can improve the fit of a model simply by adding nearly any other variables from the data to it. Even if the newly-added variable doesn’t account for a significant amount of the variance, it will still account for some. Add enough non-significant terms and eventually the whole model itself will reach significance, even though no term within it is significant (or has a good effect size). To compensate for this, SPSS offers this adjusted value, which is reduced a bit for each term in the model, regardless of whether that term was significant or not.\nFourth, purposely, there are no significant effects. All of the p-values (Sig.s) are greater than .05.\nLet’s play further with it to see what we can find.\n\n\n\n3.6.2 Second (Final) Model\nLet’s remove the Disc_Inc_Slope \\(\\times\\) DBT interaction. Interaction terms are often harder than main effects to find as significant, if for no other reason that the main effects—with which they surely correlate—are often added to the model as well, so the interaction term and the respective main effects are “fighting over” some of the same variance. This turns out not to do anything, but when we then also remove the Disc_Inc_Slope and SelfRefl_Slope main effects, we find that the DBT main effect becomes significant. (Also remove Disc_Inc_Slope and SelfRefl_Slope from the Covariate(s) field or SPSS will yell at you.)\nThe new model’s source table should look like this:\n\nRemoving those variables made for a significant model: The p-value is .011, which is less than .05. The effect size for the model is smaller, but we have narrowed ourselves down to a smaller set of variables that do explain significantly more about these teens’ neurocognitive growth than most other things do. Looking at the terms in it, we see that it’s DBT program participation that’s doing the heavy lifting.\nWere we simply exploring these data to find a parsimonious model that accounts for the most variance, we’d probably remove the Spec_Ed term as well. This term indicates whether a teen has been diagnosed with a special need (whether they’re in special education), and many of the disabilities that lead to this designation are cognitive, so it seems informative and theoretically relevant to leave it in and show it’s weak effect here.\nWhy did the model (and the DBT term in it) become significant? Looking back at the correlation matrix, we see that Discipline Incidents Slope and DBT participation were correlated (rpb = -.189, p &lt; .05). Since DBT and Disc_Inc_Slope shared a significant portion of their variances, partialing out Disc_Inc_Slope removed a significant portion of DBT in our model, leaving less of it to show a significant relationship with All_EFs_SR_Slope. This is not unusual, especially in field-based studies where we can’t create well-controlled (and simpler) situations.\nNote, however, that participation in the DBT program was essentially random. It cannot be that Disc_Inc_Slope, which measure whether a teen is getting in a increasing/decreasing amount of trouble at school, affected whether they participate in the DBT program. It could be that participating in the DBT program affected the number of times they were disciplined for getting into trouble, though. When these data were collected, they had only participated in the DBT program for two years, but if the program affects the development of their executive functions, it may well also affect their propensity to act out. This would make for a great structural equation model testing the causal relationships between those three variables.\nLet’s do one more thing to this table, and then move on to writing parts of a Results section.\n\nRight-click on the table and selection Edit Content\nChoosing either to edit it In Viewer or In Separate Window, right-click again on that table.\nChoose TableLooks...\nIn the dialogue that opens, select APA_TimesRoma_12pt from the list of TableLooks Files. Note that you can Reset all cell formats to the TableLook; this will make any further tables you create also have this formatting. The table should now look like this:\n\n 1. Right-click once again on the table and select Copy and then paste it into the Word file where it will now be a Word-style table."
  },
  {
    "objectID": "writing_results.html#writing-the-results",
    "href": "writing_results.html#writing-the-results",
    "title": "3  Writing Results Sections",
    "section": "3.7 Writing the Results",
    "text": "3.7 Writing the Results\n\n3.7.1 Overall Strategy\nMy general strategy for organizing Results sections is to:\n\nThink of the main points I want to make in my Results. This usually revolves around the research questions (hypotheses, whatever) that are the goals of the manuscript;\nCreate a set of visual displays (tables and figures) that present the main points I want to make;\nOrient the reader to the content of those visuals,\nWhile focusing on the parts of those visuals that relate to my main points,\nAnd supporting what I describe in those visuals with stats (that are usually given only parenthetically).\n\nI can and do discuss other results, both that are presented in those visuals and otherwise. But, I try to maintain the focus of the conversation on the main points of the manuscript. So, if there are additional points that are interesting / need to be explained but aren’t directly related to the main points, I will indeed discuss them. However, I will try to make it clear that those are indeed ancillary points. I will do this sometimes by saying so.\nOften, however, I try to separate the main points from ancillary ones in how I organize the Results. This may simply mean relegating the ancillary points to separate paragraphs. This works especially well if you do indeed ensure that each paragraph has a clear topic sentence, and that each other sentence therein directly relates to—usually simply expands upon—that main sentence. It’s also often best to place the topic sentence first.\nSeparating out ancillary points may, however, also mean creating subsections of my Results, perhaps into an Ancillary Analyses section, if a more explanatory heading doesn’t fit. This is an other time when templates can help, since you need only format a subheading, give it a clear title, and keep on typing. I may be a bit over-zealous in my use of subheadings, but I’ve yet to have anyone complain. to the contrary, I’ve heard that it helps both to understand what it being discussed and to help readers find the topics they’re currently interested in. It sure helps me find them when I revise my writing.\n\nMAGIC Arguments\nIn a complementary view, Abelson (1995) posits that persuasive, scientific arguments contain five, general, MAGICal characteristcs:\n\nMagnitude pertains to Abelson’s contention that “the strength of a statistical argument is enhanced in accord with the quantitative magnitude of support for its qualitative claim” (p. 12). Among the measures of magnitude are effect size and confidence intervals. I wholly agree these should be presented and considered in most Results sections. I find his presentation of “cause size” interesting but unnecessary.\nArticulation refers to Abelson’s recommendation to include as much detail and specificity about the differences (or lack) between groups. His formal presentation of articulation in Chapter 6 is interesting, but I find this example he gives to be sufficient:\n\n[an] investigator is comparing the mean outcomes of five groups: A, B, C, D, E. The conclusion “there exist some systematic differences among these means” has a very minimum of articulation. A statement such as, “means C, D, and E are each systematically higher than means A and B, although they are not reliably different from each other” contains more articulation. Still more would attach to a quantitative or near-quantitative specification of a pattern among the means, for example, “in moving from Group A to B to C to D to E, there is a steady increase in the respective means.” The criterion of articulation is more formally treated in chapter 6, where we introduce the concepts of ticks and buts, units of articulation of detail (p. 11).\n\nGenerality is similar to generalizability in that it “denotes the breadth of applicability of the conclusions” (p. 12) but also contains elements of “triangulating” insights through multiple studies and methods. He argues that “[h]igh-quality evidence, embodying sizable, well-articulated, and general effects, is necessary for a statistical argument to have maximal persuasive impact, but it is not sufficient. Also vital are the attributes of the research story embodying the argument” (p. 12). He further expands upon his ideas in relation to an ANOVA in Chapter 7.\nInterestingness and credibility (covered next) are elements Abelson sees as important components of an effective research narrative. His coverage of it in Chapter 8 is indeed useful. A quick summary of his ideas are:\n\nthat for a statistical story to be theoretically interesting, it must have the potential, through empirical analysis, to change what people believe about an important issue. This conceptual interpretation of statistical interestingness has several features requiring further explanation, which we undertake in chapter 8. For now, the key ideas are change of belief—which typically entails surprising results—and the importance of the issue, which is a function of the number of theoretical and applied propositions needing modification in light of the new results (p. 11).\n\nCredibility “refers to the believability of a research claim. It requires both methodological soundness, and theoretical coherence” (p. 13). Although a bit antiquated, Abelson’s discussion of this is expanded upon quite well in Chapters 5 (of statistical errors) and 9 (of methodological ones). Perhaps it can suffice here to note that he says that:\n\n[t]he requisite skills for producing credible statistical narratives are not unlike those of a good detective (Tukey, 1969). The investigator must solve an interesting case, similar to the “whodunit” of a traditional murder mystery, except that it is a “howcummit”—how come the data fall in a particular pattern. She must be able to rule out alternatives, and be prepared to match wits with supercilious competitive colleagues who stubbornly cling to presumably false alternative accounts, based on somewhat different clues (p. 11)\n\n\n\n\n\n3.7.2 Writing Style\nSome general advise21:\n\nReverse-engineer what you read. If it feels like good writing, what makes it good? If it’s awful, why?\nLet verbs be verbs. “Appear,” not “make an appearance.”\nBeware of the Curse of Knowledge: when you know something, it’s hard to imagine what it’s like not to know it. Minimize acronyms & technical terms. Use “for example” liberally. Show a draft around, & prepare to learn that what’s obvious to you may not be obvious to anyone else.\nAvoid clichés like the plague (thanks, William Safire).\nOld information at the beginning of the sentence, new information at the end.\nSave the heaviest for last: a complex phrase should go at the end of the sentence.\nProse must cohere: readers must know how each sentence is related to the preceding one. If it’s not obvious, use “that is, for example, in general, on the other hand, nevertheless, as a result, because, nonetheless,” or “despite.”\nRevise several times with the single goal of improving the prose.\nRead it aloud.\nFind the best word, which is not always the fanciest word. Consult a dictionary with usage notes, and a thesaurus.\n\n\nStrong & Clear Organization\nThis Stack Exchange question addresses writing the Introduction.\n\n\nStrategies for Discussing Various Anlayses and Research Arguments\nAbelson’s (1995) discussion in Chapter 4 of rhetorical styles and how they relate to different types of data/analyses are readable and sometimes useful. Among the topics of especial interest there are his coverage of:\n\nOne- and two-tailed tests,\nThe roles of parametric and non-parametric tests,\nAbsolute versus realtive effects,\nDifferent ways to frame analyses, and\nHow to write about p-values.\n\n\n\nSimple, Direct Sentences\nWrite simply, striving for language that nearly anyone could understand. Make your point fast, clear, and easily found.\nSimplicity suggests using short, direct sentences. The traditional subject-verb-object (SVO) format is thus also often best; and yes, this does mean writing in the first person for yourself and in terms of what other researchers write and did. APA 7th finally advocates doing so, and it’s about time.\nWriting in subject-verb-object format has several advantages. First, it helps separate out ideas (and actions) that are yours versus those that are others. It thus lets you both give credit where it is due and distinguish subtle differences between perspectives. It makes it easier to show what is your own perspective separate from others, both to show what you are adding to the conversation and to help inadvertently give the impression you’re taking credit where it isn’t due.\nSecond, I do believe it will help clarify the sources of your ideas and evidence, thus allowing for—encouraging—a more sophisticated consideration of the various sources of those ideas and evidence.\nThird, writing in SVO makes it a lot easier to write lit reviews and to avoid the novice phrasing of saying “The literature proves that life is good.”22\nWriting is also mindful practice. Attend to what you are saying—not just to what you are writing. Attend to what it is like to read your writing23. You can practice this by doing the same with what you read. Notice when you do or don’t like someone else’s writing—and figure out how you can do that in your writing.\nYes, writing is re-writing. It is exceedingly rare that I’ve made a manuscript worse by reviewing it. In my opinion, writing and editing are not completely the same set of skills, and so we must practicing both sets to achieve our best writing. Critiquing others’ writing can help develop our own editing skills, as can having others critique our own work and us then actively reflecting on their suggestions. However it works best for you to do it, I suggest you—we—work to develop the skill of revising as well as writing.\n\n\nMind the Reader\nI indeed try to write as if I’m having a conversation. As should be true of any conversation, I try to think about who the other person is, what they know and don’t, what parts of my topic are hardest to understand, and certainly what they likely want to know about the study.\nIt’s safe to assume that in general most people aren’t ever going to be as interested in what you write as you are. Don’t assume they’ll stay engaged.\nThe preponderance of your readers are going to be reader your writing to get a point. So make your point as clearly and simply as you can; help the readers find it fast.\nAnd help them understand it. Please don’t assume your reader understands your works just because you do.\n\n\nVerb Tense\nThe correct tense for discussing writing and ideas is the present tense; thoughts and words are considered to be “alive.” It also sounds better to say “Jones (2020) says life is good.” It also means you needn’t worry about when they wrote it.\nActions, however, are correctly relegated to the past tense. “Jones (2020) conducted a study of neonatal nurses and from that concludes that she believes life is good.” This does mean that you should put the actions you conducted in your study in the past as well (since you’re presumably writing after you did them; kudos to your multitasking mastery if you’re writing while you’re doing them!).\n\n\nFollow Conventions\nSome of the suggestions I’m presenting here are little more than repetitions (or re-packagings) of standard conventions. Some come from the surprisingly-useful and -readable Chapters 4, 5, and 6 of the 7th edition of the APA manual, the first two of which also address beyond simple writing style and grammar, such striving for bias-free writing.\nAdditional, useful guides to following conventions include:\n\nDesmarais, C. (June 11, 2017). 43 embarrassing grammar mistakes even smart people make. Inc. https://www.inc.com/christina-desmarais/43-embarrassing-grammar-mistakes-even-smart-people-make.html"
  },
  {
    "objectID": "writing_results.html#writing-about-these-results",
    "href": "writing_results.html#writing-about-these-results",
    "title": "3  Writing Results Sections",
    "section": "3.8 Writing about These Results",
    "text": "3.8 Writing about These Results\nLet us now begin to write about the results we’ve put into that Word document.\nI generally write about results by first using descriptive statistics to orient the reader to the data. I also use this time to address any issues with them, how I either addressed those issues, and/or how those issues affect analyses and their interpretation.\nI also think ahead of time about the main points I want to make in a Results section, and then both organize the information and orient the writing in the sections towards that. You’re essentially using the Results section to give evidence toward supporting a position you take about your data and what it says. Of course, this position ought to address your research goals/hypotheses.\nThis example was exploratory (exploring the data and seeing what we found that we thought was interesting—not testing an a priori research goal outside of “what’s up with the DBT”), but our exploration did note a few things of interest. To me, these include:\n\nThere were a lot of missing data.\nThere was a lot of variability within the data that we had.\nChanges in executive functioning (and in, e.g., discipline incidents) were often slight.\nThe story of the relationships between the variables is relatively complex.\nNonetheless, the zero-order24 correlations suggest a few interesting relationships in relation to the DBT program.\nA more structured investigation of those relationships—conducted also in light of a dash of theory—finds that participating in the DBT program was associated with changes in student-reported executive functioning.\n\nThose are thus the points—and perhaps a reasonable order to address them in—to cover in this Results section.\n\n3.8.1 Descriptives\nSo, let’s indeed start by creating a level 2 header entitled “Descriptives”:\n\nIn Word, go the Home ribbon, click on the tiny arrow in the bottom right corner of the Styles section: \nA side menu should appear to the right: \nGo the Results section, and—on its own line—write Descriptive Statistics\nWith your cursor anywhere in the line where you wrote Descriptive Statistics, click on Heading 2 in the Styles side menu. If you’re still having issues, consult Chapter 13 where more was given about using a style template and reference manager, including how to instead use LibreOffice Writer25 and RStudio.\n\nWith the Descriptive Statistics table just below where you’re writing, start by mentioning that table to let your readers know outright that that’s what you’re talking about and where to look for more information.\nE.g.:\n\nTable 1 presents descriptive statistics for the main variables investigated.\n\nNow, discuss some highlights noted in that table. Your goals is not to restate what is given in that table. For the most part, either present information in a table/figure or discuss it in the text.\nReaders often expect to be quickly oriented to whom the data represent—who are these participants we’ll be talking about throughout the Results, so that’s a good thing to address near the beginning of the section, i.e., now:\n\nAs we can see in this table, 40% of these middle and high-school students were diagnosed with disabilities that warranted being in special education; 62% of these students were under economic distress.\n\nAgain, the outline we sketched for the Results section above included noting that there were a lot of missing data and that there was a lot of variability within the data we have, so in that first paragraph discussing this table we can write, e.g.:\n\nHowever, it is important to note that although data were available on each variable for up to 670 students, we only had complete data on 152, reducing the generalizability of these findings—including to this particular population26.\n\n\nOn average27, there was little change in an overall composite measure of these students’ self-reported executive functioning (mean change = .0128). The number of discplinary incidents a student was part of tended to decrease slightly (\\(\\overline{x}\\)29 = -.04). However, for both of these variables, the standard deviations were much larger than the means, indicating that there was much variability in these students’ development and behavior30.\n\nAnd yes, I try both to assume that readers know less about the field than one would expect and to use this opportunity to restate or explain concepts. I also tend to write longer, more detailed descriptions both of tables and results early in the Results and then give briefer, more succinct descriptions later; experienced readers can easily skim over the more detailed descriptions given earlier while more novice ones both gain more of a footing and then enjoy seeing themselves able to apply it later in the section.\nNonetheless, it can also be appropriate to write brief and direct pieces. Please add this sentence to the same paragraph:\n\nAbout 20% of these adolescents participated in the DBT program31.\n\nThe percents etc. in that table are presented there separate from each other. In other words, the 357 students for whom we have discipline incidents are not necessarily the 326 students for whom we have executive functioning data. And normally the next table would indeed be to present how much overlap there is between those who have missing data in these variables32, but we’re simply covering the overall strategy for writing Results sections, not all that should go in one.\nNonetheless, it is important to reminder readers of this—and that you are cognizant both of this yourself and of the implications this has on the generalizability of your results. Therefore, something like this is warranted:\n\nIt is important to note from Table 1 that we only have complete data for 152 (22.7%) of the 670 participants.\n\nIn fact, a general rule of thumb is that missing more than 10% of the data is problematic. The data we’re using here has more missing values than the actually data since I didn’t include it all, but even there this is an issue my colleagues and I must contend with. Make the above sentence about missingness a topic (and first) sentence of a new paragraph, and add a few more sentences explaining the ramifications of this degree of missingness on the results and their interpretation.\nAn other aspect about the executive function and discipline incidents variables is their relatively large variance. The table shows this through the magnitude of the standard deviations relative to the means, but I think it’s easier to see this in the box-and-whisker plots we made through the data exploration. we could have talked about this earlier in our discussion of that table, but putting it last lets us segue to those figures. Or rather, since this insight is supported by both that table and those figures, it makes sense to talk about this aspect last:\n\nFinally, the large standard deviations for the all executive functions and the discipline incidents slopes33 relative to their respective means in Table 1 indicate that there is a lot of variability among these slopes. These adolescents displayed a wide range of developmental trajectories.\n\nAgain, I’m trying to point out important piece of information in the tables (and soon figures) and explain what it means while also remaining objective. Remember, for the Results section, just give the facts.\nIn that same paragraph about the variability, we can say:\n\nThis variability is perhaps better seen in Figures 1 and 2 which present box-and-whisker plots of the slopes in self-reported executive functions34 for DBT program participants and non-participants (Figure 1) and slopes for discipline incidents (Figure 2).\n\nAfter this, please describe what these figures say about the distribution of scores, including their variability relative to each other and the implications of this on the results & their interpretation.\nI considered ways of organizing these results so that we could also use the box-and-whisker plots to transition to the bar chart showing the mean difference & confidence intervals35, but it works fine (I think) to instead discuss the correlations first and then use that as a backdrop to jump the mean differences and the linear models testing them.\nIn any case, I do feel it is important to discuss the correlations. First because it’s often useful to present the bivariate relationships between variables to orient the reader to that next level of complexity to the data. Second because it helps us show where that complexity is, thus helping lay the foundation for interpreting models that include combinations of those variables. Indeed, as I mentioned in the section above conducting those analyses, the inter-relationships between the variables affects the significance of the terms in our models; both our readers and we are well-served to stay aware of these inter-relationships when interpreting the linear models.\nLet’s write an other paragraph about them:\n\nThe correlations between the variables presented are presented in Table 2. As we can see, the correlations between the variables all tended to be rather weak (rs = -.004 – -.18936). Despite these small values, participation in the DBT program correlated significantly with experiencing econmic distress (r37 = -.082, p = .048), changes in discipline incidents (r = -.189, p &lt; .001), self-reported changes in executvei functions (r = -.165, p = .003)38. Self-reported changes in executvei functions also correlated significantly economic distress (rpb39 = .168, p = .002).\n\nIt would help to explain to the their what each of these correlations mean—even if executive functions weren’t scored counter-intuitively—so please do that after those sentences. For example, we could say:\n\nAdolescents who were experiencing distress demonstrated significant decreases in their self-reported changes in executive functioning (although the correlation is positive, higher scores on the BRIEF indicate decreases in executive functioning)40.\n\nWe can then simply end that paragraph with:\n\nAll other correlations were not significant (smallest p = .262)41.\n\nNext, please add a paragraph discussing the bar chart presenting the DBT group mean levels of changes in executive functioning. This is in fact a corollary of the point serial correlation between those two variables: The 95% confidence intervals for either group do not contain the mean of the other group (e.g., the confidence interval for the Did Not Participate group spans from about 0.03 to just a little over 0 while the mean for the Participated group is below 0.42). Part of the paragraph could read as either:\n\nThose who participated in the DBT program self-reported significantly greater improvements in their executive functioning than those who did not participate (F3, 323 = 8.998, p = .003), however this effect was rather small (η2 = .027).\n\nor:\n\nThose who participated in the DBT program self-reported significant (F3, 323 = 8.998, p = .003) but rather small (η2 = .027) improvements in their executive functioning relative to those who did not participate.\n\n\n\n3.8.2 Inferentials\nWe can use our discussion of the difference presented in this bar chart to introduce the general linear model.\nI asked you to only paste in the final model. Had this not been a rather atheoretical exploration of the data but instead a set of analyses comparing different theoretically-relevant configurations of the variables, then it may well have been appropriate to include them both. For example, the first models (the one we didn’t paste in Word) included discipline incidents (and whether they interacted with DBT participation), so had we posited as an hypothesis that changes in discipline incidents would predict the effectiveness of the DBT program, then it would be worth showing that model and then also showing the model without it demonstrating that, although knowing changes discipline incidents didn’t improve our understanding of changes in executive functions, information about them does affect our ability to understand the relationship between DBT participation and changes in executive functions.\nBut here, please simply take a crack at describing what is presented in the source table presented in Table 3. Please consider the results in light of the zero-order correlations between those variables and how partialing out having an IEP affects our interpretation of the relationship between DBT participating and changes in executive functioning. Please remember that there are many reasons these adolescents could have been given an IEP; not all disabilities are related to cognitive or emotional functioning (or information processing). In fact, even economics and race/ethnicity affect whether a student is diagnosed—and what sorts of diagnoses they receive."
  },
  {
    "objectID": "writing_results.html#writing_results_resources",
    "href": "writing_results.html#writing_results_resources",
    "title": "3  Writing Results Sections",
    "section": "3.9 Further Resources",
    "text": "3.9 Further Resources\n\nGeneral Writing Tips and Guides\n\nAbelson, R. P. (1995). Statistics as Principled Argument. Hillsdale, NJ: Lawrence Erlbaum Associates Publishers.\n\nThe first couple of chapters are good; the rest starts to ramble.\n\nCochrane, J. H. (2005). Writing Tips for Ph. D. Students\n\nReadable and succinct, Cochrane puts a lot of good, general advice into a dozen pages.\n\nKatz, M. J. (2009). From Research to Manuscript: A Guide to Scientific Writing (2nd Ed.). Springer.\n\nSeveral students have commented on how helpful this book is.\n\nStatistical Writing. UCLA: Statistical Consulting Group.\n\nWith some SPSS_generated output as talking points, this gives specific guidance for particular analyses.\n\n\nWriting about Specific Types of Analyses\n\nChecklists for results in general as well as various types of analyses:\n\nLang, T. A. & Altman, D. G. (2015). Basic statistical reporting for articles published in biomedical journals: The “Statistical Analyses and Methods in the Published Literature” or the SAMPL Guidelines. International Journal of Nursing Studies, 52(1), 5–9. https://doi.org/10.1016/j.ijnurstu.2014.09.006\n\nMixed models and multilevel models:\n\nHow should results from linear mixed models (LMMs) be reported?, StackExchange Psychology & Neuroscience Community\nMonsalves, M. J., Bangdiwala, A. S., Thabane, A., & Bangdiwala, S. I. (2020). LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): recommendations for reporting multilevel data and analyses. BMC Medical Research Methodology, 20(1), 3–3. doi: /10.1186/s12874-019-0876-8\n\nMeta-Analyses\n\nThe PRISMA model for reporting systematic reviews and meta-analyses\n\n\nGuide for Simpler, Clearer Writing\n\nThe Hemingway App lets you enter/paste in text and then suggests ways to make that text better 8 Very General Writing Tips & Guides\nDesmarais, C. (2017) 43 Embarrassing Grammar Mistakes Even Smart People Make. {Inc.}[(https://www.inc.com/) Life.\n\n\n\n\n\n\n\n\n\n\n\nAbelson, R. P. (1995). Statistics as Principled Argument. Lawrence Erlbaum Associates Publishers. https://articles.viriya.net/statistics_as_principled_argument.pdf\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates."
  },
  {
    "objectID": "writing_results.html#footnotes",
    "href": "writing_results.html#footnotes",
    "title": "3  Writing Results Sections",
    "section": "",
    "text": "Worn out, if you ask me↩︎\nDoes this mean that the “c” in .docx denotes a normal document? Of course not, this is Microsoft; they don’t try to be systematic in what they do. As an other aside, the “x” at the end was added by Microsoft because they were forced to. The world of computing once again moved beyond Microsoft’s kludgy programming with growing support for Open Office’s “extensible markup language” (XML), and Microsoft had to fundamentally change how it generated Word files to catch up again with advances in technology. And the advance in technology they had to catch up with that time was simply what we’re doing right now: Separate out the content from the styling. What you say from how it looks. So you can focus on saying it knowing it will look good. To do this, Microsoft had to totally revise the way it made Word (and other types of) files.↩︎\nOne could argue that we should be using “race/ethnicity” throughout.↩︎\nFeh, left in the world after what we’ve done to them, let alone here↩︎\nWell, you can do a lot both within SPSS and Word to make tables look quite good. I’m now simply showing one way that makes a relatively significant change. Nonetheless, you may want to explore the other options presented when you go through the following steps.↩︎\nNote that some journals do still require them to be at the end.↩︎\nAnd yes, this is a rare moment when I didn’t take the obvious chance to bash Microsoft for their lack of compliance to established conventions.↩︎\nOne can use whatever value as the “stem.” Usually, one chooses the number place—ones, tens, hundreds, or even point-ones, point-tens, etc.—that give a good presentation of the data. This Statology page gives a nice example of using different “stems.”↩︎\nI am advocating copying out this table differently than the table of descriptives, above. You can use either method for either, but each of these seems to work best and use the fewest steps for their particular type of table.↩︎\nStatistics terminology is already unintuitive and complicated, and SPSS doesn’t help by using terms that are sometimes uncommon—not wrong, but not ones you’d expect. “General linear model is the correct and common term, but by”univariate,” they mean only one outcome (“dependent”) variable; “multivariate” tests more than one outcome at a time, helping somewhat to control for correlations between the outcomes.↩︎\nHere is an other place where SPSS tries to be helpful, but—to me—fails. Random Factor(s) ought to be for continuous variables, but given how SPSS handles them, they’re better placed under Covariate(s) regardless of how you interpret them.↩︎\nThe y-axis of this plot is the outcome variable (here, changes in student-reported executive functioning). The x-axis is, well, there are one axis per term in the model (here, 4 terms), so it gets pretty hard to visualize.↩︎\nThey’re called “degrees of freedom” because they do represent the number of insights we are free to make. After each insight is made—after each values is estimated—we’re left a little less free to make additional insights.↩︎\nTo continue with the race example, it could be that we’re interested in seeing if, say, Blacks are treated differently than members of other racial categories; in this case, the value for the treatment of Blacks would be the reference level and the values for all others would be given in relation to how Blacks are treated. The values for the others, therefore, would really be for being treated worse/better than Blacks.↩︎\n “But if we divided up, say, race into a series of dummy variables, doesn’t that still take as many degrees of freedom to estimate all of those dummy variables as it does to estimate the effects of one ‘race’ variable with that many levels?” Yes, smarty pants, you’re nearly right. The thing is that we must first use all of those degrees of freedom to estimate the effect of that one, multi-level ‘race’ variable, and then run a post hoc test to see where any differences are, making us run an additional hypothesis test that (re)opens us up to making a Type 1 error.↩︎\nIt’s the mean square because it’s the mean of the sum of squares—the average amount of information (deviance from the regression line) in each “insight” made by each degree of freedom.↩︎\nLuckily, we only need to spend on degree of freedom to estimate the default level for all of our variables/terms, no matter how many we have.↩︎\nSo, what’s partialed out when computing the η2 for the whole model? Nothing. The value in that cell is not a partial η2, but in fact the whole η2 for the model—the size of the effect of everything therein. SPSS just “cheats” and puts the η2 for the model in the same column as the partial η2s for the various terms.↩︎\nWhat?! What about that F of ~2 meaning the dfs in the model give about twice as much insight as any other estimate we could make. The F score talks about the value of each insight (parameter estimate) and how good those are relative, essentially, to any other random estimate we could do on the data (relative to, say, the mean of odd rows is different from the mean of even rows). The effect size measures look at changes in the outcome variable, and whether the different levels of a parameter have different levels of the outcome—whether DBT participants had different EF slopes than non-participants. So, they use the source of information (the variance in the data), but are using it to answer different questions.↩︎\nAnd, again, although it’s in the Partial Eta Squared column, the value for the whole model is not a partial η2, but the “full” η2. SPSS just put it in the same column as the partial η2s to conserve space and confuse newbies.↩︎\nUnfortunately, I don’t remember where I got this, but it’s not mine.↩︎\nUgh! Sure, we can consider “literature” to be a metonym standing in for the authors, but it’s often easier to track the progress of science through those conducting it than through a series of disconnected articles. In addition, “the literature” is such an imprecise and lazy phrasing that nearly prevents any real consideration of the nuances between perspectives. And please, when was the last time anything was proven in science?!↩︎\nAnd yes, that means you have to read it. Which means rewriting it.↩︎\nI.e., simple correlations between two variables—not part or partial correlations.↩︎\nWhere it is much easier to use styles.↩︎\nIt is gauche to editorialize or give opinions in the Results section (those are for the Discussion). However, it can be quite helpful to explain what given results mean objectively both to help more novice readers and to make your point about what in the results you’re focusing on.↩︎\nStarting new paragraph here makes for a few very short paragraphs, but it seems to me that the next topic of changes in EFs & disciplinary incidents is thick enough to warrant separate attention↩︎\nSomewhere earlier in the manuscript—probably in an Analyses or Analytic Strategy subsection of the Methods section, I would have given a careful and detailed explanation of what this variable actually measures. Still, it’s worth giving some review here, both to remind readers and because, in my opinion, a well-written manuscript allows readers to read sections out of order—and even to skip some sections entirely. Nonetheless, it’s a balance everyone must find on their own about how much to restate things at the expense of flow.↩︎\nIt may be confusing to first say “mean change” as I did for EFs and now use x̄ for discipline incidents, so perhaps going with one style or the other is better.↩︎\nAlthough I do strongly advocate writing SVO-style sentences, it seems useful for this one to first explain what it is we’re talking about before then discussing a slightly complex point.↩︎\nIt may be clearer simply to say “19%” than “About 20%” since the former more clearly relates to the table; I went back and forth a bit on this.↩︎\nWe could do this in SPSS by first re-coding the variables into dummy ones based on whether they’re missing with (1) Transform &gt; Recode into Different Variables, then (2) selecting System- or user-missing as the first Old Value and 0 as its New Value, and then (3) choosing All other values as the Old Value and 1 as its New Value. Once the variables are coded into missing/non-missing dummies, we can see how much overlap there is in the missing values by comparing these missing/non-missing dummy variables with Analyse &gt; Descriptive Statistics &gt; Crosstabs.↩︎\nVariables—really everything but the title—is to be put into title case in tables & figures. However, APA 7th (p. 169) has variables (and experimental conditions) put in lower case in the text.↩︎\nYou may have noticed that I don’t stick with a specific format for the variables—and certainly try to not divorce my writing more from the English my readers know by littering it with acronyms. Sometimes I feel it helps to keep the description of the variables in more flexible, lay terms, but sometimes it would help instead to use the same phrasing for them. I do go back and forth a bit trying to find what seems the best way of ↩︎\nSomething like, “These [box-and-whisker plot] figures suggest there may be some differences in the means of these these despite the great variability in scores.”↩︎\nI considered giving the absolute values instead of showing that they’re negative. This may have helped, and we easily could with “…rather weak (|rs| = .004 – .189).” or even “…rather weak (absolute values of rs ranged from .004 to .189).”↩︎\nBoth DBT participation and economic distress are dichotomous (dummy) variables, so in fact a correlation between them is inappropriate. We should use a frequency table (like we get as a Crosstab) and instead compute either the Fisher’s exact test or a χ2 to test for significance. I’m punting on this here for expediency even though what I’m doing is wrong.↩︎\nI presented these three in a different order than they appeared in the table, I did this to make for a slightly easier transition to the next sentence that’s also about self-reported EF slopes. However, were I to prepare this for publication, I would re-arrange the variables in that correlation matrix (and the table of descriptives) to make them all follow the same sequence as is given in the text.↩︎\nThis is appropriate. We can correlated a dichotomous variable with a continuous one. (We can correlate continuous variables with anything else). A correlation between a dichotomous & continuous variable is called a “point biserial correlation” which is abbreviated as rpb. The formula that produces a point biserial correlation is mathematically equivalent to the one that produces correlations between two continuous variables (a “Pearson correlation”), so including it in this table is perfectly legitimate. Still, I really should have, e.g., put “NA” for the cells presenting the DBT–economic distress correlation and instead included a frequency table for that.↩︎\nCourtesy Lisa Gillespie =^3↩︎\nSure, we could also give the correlations along with thep-values, but this seems like enough to me, especially since there’s nothing really there to talk about.↩︎\nAs you may have guessed, yes, the fact that the Did Not Participate group’s confidence interval does not overlap zero indicates that that group’s slopes were significantly greater than zero: The members of this group experienced significant declines in their self-reported executive functioning over these years. Life is tough for these teens.↩︎"
  },
  {
    "objectID": "significance.html#measuring-relationships",
    "href": "significance.html#measuring-relationships",
    "title": "4  Introduction to Measuring Relationships and Building Models",
    "section": "4.1 Measuring Relationships",
    "text": "4.1 Measuring Relationships\nIn a very readable ScienceAlert article, Neild describes a study in which researchers found that, although night owls tend to have shorter lives than morning larks, being a night owl per se doesn’t increase mortality in older adults. Instead, it was the riskier behavior these people of the night prefer more to do. But how could the researchers determine this? If pretty much all night owls didn’t live as long, how could they say that being a night owl didn’t increase mortality? What sort of analysis would allow them to remove—isolate—the effect of being a night owl (more technically, having a nocturnal chronotype)?\nIsolating an effect begins with being able to measure the effect—more specifically, being able to measure the main effect of a variable (here, chronotype—early birds vs. night owls). Measuring it so allows us to circumscribe its effect—to locate and measure the size (and location) of its effect. That then lets us isolate the effect of it on other variables. We can “partial” out the effect, letting us see other relationships with that effect removed.\nWe must thus be able to measure the extent of these relationships well. The better it is measured, the better we can detect its boundaries and thus isolate it. Estimating the relationship between variables thus not only allows us to see how much they are related, but to isolate that relationship from other relationships.\nHow much two variables are associated with each other is often modeled as a linear relationship where the slope of the line (when the two variables plotted on the axes) shows how much they’re related: The greater that slope (the more it deviates from a flat line with a slope of zero), the stronger the relationship between them. We often use some assumptions to estimate the underlying nature of the variables, and then use some criterion (e.g., being 95% sure we’re right) based on those assumptions to decide whether we think the association between those two variables matters—whether it is “statistically significant.”\nIf the two variables are measured on the same scale—say z-scores—then the greatest relationship would be represented by a straight line going up (or down if it’s a negative relationship) at a 450 angle; whenever the line goes one unit to the right, it also goes exactly one unit up. The slope is \\(\\frac{1}{1}\\), or simply 1. Anything less than a perfect relationship results in a slope that is less than 1.\nIf we make no further assumptions about the two variables than that they have a linear relationship and that both sets of data are roughly homoscedastic, then we can compute a correlation coefficient. The exact method we use depends on the measurement level (dichotomous, ordinal, interval, etc.) of the variables, but all correlations are set to range between 0 and 1 (or -1) by convention.\nSo, one way to look at a correlation is the strength of the (linear) relationship between variables."
  },
  {
    "objectID": "significance.html#signal-to-noise-ratios",
    "href": "significance.html#signal-to-noise-ratios",
    "title": "4  Introduction to Measuring Relationships and Building Models",
    "section": "4.2 Signal-to-Noise Ratios ",
    "text": "4.2 Signal-to-Noise Ratios \nAnother way to look at correlations is that they are one of the ways of measuring a “signal-to-noise” ratio—another idea that permeates much of statistics. Here, it’s not just a question of how much two variables are associated, but how much that association accounts for all the information we have in our data about those variables–how much of the total variance is covariance between those things. The amount that the variables move together—the amount they covary—is a measure of the strength of their relationship. The amount they move independent of each other—the amount they do not share variance—is a measure of how weak their relationship is. If they have a weak relationship, then we obviously don’t have a very good representation of what makes these two variables take on whatever values they have."
  },
  {
    "objectID": "significance.html#building-models",
    "href": "significance.html#building-models",
    "title": "4  Introduction to Measuring Relationships and Building Models",
    "section": "4.3 Building Models",
    "text": "4.3 Building Models\nThe models we’re analyzing in the class activities—that, say, participating in a DBT program is linearly related to growths in executive functions—are not very good; they don’t account for much of the total variance in the data. Nonetheless, building models and testing how well they account for our observations (e.g., the data we have on hand and future observations we will make) is a third idea that permeates much of statistics.\nLet’s go back to our example of a very simple model: a correlation between two variables. LEt us also say in this example that it’s a weak correlation, and we want to improve on this model to make it a better representation of what’s really going on in our data. We could do this by utterly throwing out that first model and trying another one (e.g., by seeing if another zero-order correlation is stronger). Or we could try tweaking our model, say, by adding in another, third variable (i.e., testing out some partial or semipartial correlations). This third variable may clarify the relationship between the first two variables or add new and unique information to our model. This third variable may also (or instead) “suck up” the information that exists in that first zero-order correlation thus making the original, direct relationship matter even less1.\nFor example, imagine we want to predict what makes adolescents develop an important set of cognitive skills (those executive functions, EFs). To do this, we start with an outcome—a criterion—of interest, e.g., how much a general, self-report measure of all EFs changes throughout middle and high school. We want to see what is related to that measure of growth–to predict why one teen shows strong development and another in fact becomes worse.\nWe could look into what is related to the EF growth by looking at a bunch of correlations, Indeed, we started by doing just that: just getting a since of what is related to what in the data set, focusing on what is related to EF growth. But then we (I) decided to up our game: We started asking more specific questions of the data; we demanded more precise answers from it at the expense of having to make more precise assumptions.\nIn positioning EF growth as an outcome—a criterion or DV—and any other variables as the input / predictors / IVs, we are assuming that EF growth is a result of the other variables. One way to think about what we’re doing is this: We set EF growth down in our model, add another variable to it as a predictor, use ordinary least squares to “regress” the predictor down to a line, and then see how steep that line is. Mathematically, this is only slightly different than conducting a correlation: In a correlation, we choose a line based on the regression of both variables down to that line; in linear regression, we only regress the predictor down to that line. Although the math is slightly different, the values we derive are the same (as long as the scores are standardized).\nSo why go through the trouble of doing a linear regression at all if a correlation (or even a semipartial) gets us to the same point? Because linear regressions let us ask more precise questions and thus get more sophisticated answers. We can look at specific pieces of the model, for example looking at only the effect of one predictor on the criterion isolated from any influence of other predictors. We can also look at the error term and even run tests on it (e.g., to see if the error indeed approximates the normal distribution we assume it does). Linear models thus represent a more flexible approach that can be adapted to a wider range of data—and still generate more specific answers from it. (In fact, this approach is so flexible that—conceived of as generalized linear models—they undergird perhaps every analysis you will ever make.) \nAnd so in linear regressions, we can test rather specific terms to see if those terms alone are significant. Does an adolescent’s 6th-grade EF score predict EF growth in subsequent years (is the intercept term significant)? Does DBT participation matter (is the β-weight for that term significant)? Even if DBT participation matters, does the economic hardship a student experiences moderate the effectiveness of the DBT program (is a DBT \\(\\times\\) economic distress term significant)?\nIf we already know how much economic distress a student is facing, do we really need to know about the DBT program at all?\nThat last question is more sophisticated than it may first seem. The questions before it (about intercept, a main effect of DBT, and a DBT \\(\\times\\) economic distress interaction) are all answerable through an ANOVA. That last one about whether DBT adds significant and significantly new information to a model that already contains economic distress requires a bit more—and a change in perspective about what we’re doing in these analyses.\nAnother way to think about what we’re doing in linear regression is this: We gather together a set of variables to act as predictors. Using some eldritch process, we produce a value for EF growth that we would expect to see based on these predictors. Then, we see how close the actual EF growth score (for that set of conditions) is to the value we predicted it would be; if our predicted score is close to the actual score, we say we can created a good model that can well explain what determines that level of EF growth we actually see.\nIn other words, not only can we build and test models, we can build two different models and see which model performs (predicts) better. This is an idea that not only permeates (more advanced) statistics, but guides much of experimental design. One way to design a study is to create two groups—and experimental and a control—and test if having something (the treatment given to the experimental group) is better than nothing. One way of testing significance is whether we can argue we have something (can reject the null) or whether we can’t argue that we have something (cannot reject the null)2\nWe can still test if a predictor itself is significant while we think in terms of the model: Is the overall model more significant when that predictor is added to it? The math going on under the hood is different when we think in terms of changes in the overall model, but the end result is the same whether we test the significance of that one term (as we do in an ANOVA) or whether we test the change in significance of the overall model (as we do in this new method). (Given certain assumptions & arrangements) we get the same value for a relationship between two variables when we compute a correlation coefficient a we do when we compute a β-weight in a simple linear regression.\nGiven certain assumptions & arrangements, predictors that we find are significant when testing them as terms alone will also be significant when we test them in terms of changes to model fit. But looking at analyses in terms of changes to model fit gives us more flexibility and precision. Yes, this means we also gain yet another layer of things to learn, but I am hopeful that we can learn how to compute analyses this new way, and thus have you all gain a more powerful and flexible tool to use in your burgeoning careers.\n\nIt may or may not be worth noting that all of the things I’ve discussed in this chapter are situated within the traditional statistical “Zeitgeist” of using models to simulate / test theories. Since the advent of “big data,” however, there has been a nearly countervailing strategy of not worrying (much, if at all) about modeling relationships between variables / constructs. Instead, the focus in this other wheelhouse is making good predictions from one set of data to other sets. This other tradition often worries so little about why their analyses work that they sometimes can’t even tell why it does. This underlies, e.g., the random forests and machine learning strategies that have taken firm root in a growing number of industries. And who knows? Maybe in ten years I’ll be including these in a course like this. But for now, I’ll only give you a few links to read more about this if somehow you’re superhuman enough to have time and temerity to read them:\n\nRaper (2020) Leo Breiman’s “Two Cultures.” Significance, 17: 34–37. doi: 10.1111/j.1740-9713.2020.01357.x\nBreiman (2001) Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author). Statistical Science, 16(3), 199–231.\nBreiman, L, & Cutler, A. Random Forests.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreiman, L. (2001). Statistical modeling: The two cultures. Statistical Science, 16(3), 199–231. https://articles.viriya.net/statistical_modeling_the_two_cultures.pdf\n\n\nRaper, S. (2020). Leo Breiman’s \"two cultures\". Significance, 17, 34–37. https://doi.org/10.1111/j.1740-9713.2020.01357.x"
  },
  {
    "objectID": "significance.html#footnotes",
    "href": "significance.html#footnotes",
    "title": "4  Introduction to Measuring Relationships and Building Models",
    "section": "",
    "text": "That last possibility of “sucking up” the information can be investigated by seeing if the third variable is acting as a mediator or moderator.↩︎\nRemember, though, that a more powerful and sophisticated test is not if we have something that’s better than nothing, but whether what we have is better than an alternative: which drug has fewer side effects, which type of prevention is most effective, which man is a better choice for husband.↩︎"
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#the-roles-of-covariance-variance",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#the-roles-of-covariance-variance",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "5.1 The Roles of Covariance & Variance ",
    "text": "5.1 The Roles of Covariance & Variance \nA standard deviation (SD) describes how far away—on average—individual data points are from the mean. So, to compute the standard deviation, we essentially measure how far each data point is from the mean, and then take the average of those distances while standardizing this to be presented in z-scores (where one z-score equals one standard deviation). We can do this by taking each score’s distance from the mean, squaring those distances, and then taking the square root of that squared distance1, and then dividing it by the number of data points (um, well, actually dividing by the number of data points minus 1)2. The formula for doing all of this can be written as:\n\\[SD = \\frac{\\sqrt{\\sum ( X - \\bar{X})^2}}{n - 1}\\]\nwhere \\(X\\) is the score for any given member of the sample, \\(\\bar{X}\\) is the sample mean of that score, and n is the sample size.\nAlthough we often report the SD in Results sections, many calculations in fact use the variance instead, which is nearly the same formula but without the square root3:\n\\[\\text{Variance} = \\frac{\\displaystyle{\\sum_{i=1}^{n} ( X_{i} - \\bar{X})^2}}{n - 1}\\]\nNote that I added some extra notation to the formula this time. The Xi simply means we’re subtracting a particular instance of the variable X, instance i. Which is instance i? Well, that’s the point: The stuff I now put around the summation symbol (\\(\\Sigma\\)) explains what i means. That extra stuff around the summation symbol says that we’re starting with the first instance of (i = 1, i.e., at the first data point, whatever its value) and continuing until we get the last data point (when i = n, i.e., when we’ve reached the last data point); in other words, “Start at the beginning and go to the end.” So, if we wrote: \\(\\displaystyle\\sum_{i=3}^{n-1}\\) we’d mean to start at the third instance (e.g., third row in a data matrix) and go the to the next-to-last (n – 1) instance. I point this out both so you understand it and so we can notice better what’s going on when we next add another variable, Y, into the mix.\nCovariance, of course, is how much two variables covary. Mathematically, we start by taking how much each pair of instances both vary from the mean:\n\\[ \\text{First variable's difference} = X_{i} - \\bar{X} \\text{; Second variable's difference} = Y_{i} - \\bar{Y} \\]\nNotice how both variables have an i as the subscript, indicating that we take the value for X for the, e.g., row as the take the value of Y. For example, for the first “instance” (e.g., row) in a data set, we say i = 1 and for that row, we have X1 and Y1; for the second instance, we have X2 and Y2 and so on.\nSo, when we compute the covariance:\nwe first take the difference from the respective means and then multiply these differences together (before then adding these up and dividing by n – 1), like so:\n\\[\\text{Covariance} = \\text{Cov}(X, Y) =  \\frac{\\displaystyle{\\sum_{i=1}^{n} (X_{i} - \\bar{X})(Y_{i} - \\bar{Y})}}{n - 1}\\]\nNote three things about this:\n\nCovariances can be negative (as typically conceived, variances and SDs cannot).\nCovariance is maximized when X and Y are both large at the same time. Sure, that also means that there will be times when X and Y are both small at the same time, but overall, those times when both X and Y are large are what make the covariance large4. Therefore, if you want to test the true magnitude of the covariance (e.g., the correlation) between two variables, try to measure a wide range of values to allow for the possibility of times when both values are large together.\nBecause a few instances of X and Y being large together makes for a big covariance, it’s vulnerable to a few outliers. Just saying.\n\nOne more thing to note about the covariance: It’s represented in the same units as the variables. So, e.g., if we looked at the covariance between systolic blood pressure and lung volume, the covariance is in some weird millimeter x cubic centimeter units. This is fine—indeed necessary—if our variables are in those units, but it does often make it hard to conceive of what a covariance value actually means."
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#correlations",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#correlations",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "5.2 Correlations ",
    "text": "5.2 Correlations \nIn stats, we often standardize variables—put them all into units that are the same range—so that we can make comparisons between variables. We could do this any way we want, but commonly, we divide a value by the SD: This puts variables into the same range and the same units5.\nWe do this for covariances too, and often. Now, the covariance is the product of two variables, so we have to divide them by both their units, but that’s easily done:\n\\[\\text{Standardized covariance} = \\frac{\\text{Cov}(X, Y)}{SD_{X}SD_{Y}}\\]\nAs you may have figured out, this is in fact the formula for Pearson’s r, assuming both variables are interval / ratio.\nThis equation also reminds us that the concept of a signal-to-noise ratio that under-girds much of statistics is intrinsic to correlations. Here, of course, it’s the covariance : variance ratio."
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#sec-partial_semipartial_correlations",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#sec-partial_semipartial_correlations",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "5.3 Partial & Semipartial Correlations",
    "text": "5.3 Partial & Semipartial Correlations\nThe concepts that underlie partial correlations are another important fundamental aspects to much of statistics—especially linear regressions. In a nutshell, a partial correlation is the correlation between two variables after first removing the effect a third variable. A semipartial correlation (confusingly sometimes called a “part correlation”) removes the effect of a third variable from just one of the two correlated variables. We often say, e.g., that we’re looking at the correlation between two variables while “controlling for” the effect a third variable. (A correlation between two variables that does not account for a third (or fourth, etc.) variable is also called a zero-order or bivariate correlation.) \nA partial correlation, then, is the correlation between two variables, say X and Y, after removing the correlation each has with a third variable, say Z. Symbolizing the partial correlation between X and Y after controlling for Z as rXY.Z:\n\n\\[r_{\\mathit{XY}.Z} = \\frac{\\text{Correlation between }X \\text{ and } {Y - {({\\text{Correlations between }Z\\text{ and  both }X\\text{ and }Y})}}}{\\text{Residuals left over after removing the correlations between }Z\\text{ and both }X\\text{ and }Y}\\]\nMore formally:\n\\[r_{XY.Z} = \\frac{r_{XY} - r_{XZ}r_{YZ}}{\\sqrt{(1 - r_{XZ}^{2})(1 - r_{YZ}^{2})}}\\] A semipartial correlation, remember, controls for just the correlation of one of the variable with a third variable, say only controlling for the correlation of X and Z but not for the correlation of Y and Z:\n\\[r_{X(Y.Z)} = \\frac{r_{XY} - r_{XZ}r_{YZ}}{\\sqrt{(1 - r_{XZ}^{2})}}\\]\nWe can create more complex partial and semipartial correlations, controlling not just for one other variable (here Z), but for several other, additional variables.\nO.K., the value of a partial correlation should be pretty clear: If I’m interested in the correlation between, say, blood pressure and breathing efficiency, I may want to control for the effects of things like age and exercise.\nBut why would I want to compute a semipartial correlation? In fact, we do this all of the time. Let me explain:\nA zero-order correlation (i.e., a correlation between two variables that doesn’t account for any other variables) does not make a judgment about where the variance lies—in fact, it’s essentially assumed to come more or less equally from both variables. Often however, we’re interested in looking at the variance in only one measure—the outcome measure. We want to know from whence comes the variation in our outcome; we are not interested in (or we try to control) the variance in our predictors. In a linear regression, we typically investigate the effect of predictor X on outcome Y while also controlling for the effect of predictor Z on outcome X. In other words, in linear regressions with one outcome and more than one predictor, we isolate the effects of each predictor on the outcome from the effect of other predictors on the outcome while letting each predictor fully covary with the outcome (i.e., let predictor X covary with outcome Y while removing the effect of predictor Z from the relationship between X and Y—unless we add a term for the covariance of X and Z as an interaction term)."
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#investigating-why-dbt-works-or-doesnt-work",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#investigating-why-dbt-works-or-doesnt-work",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "5.4 Investigating Why DBT Works or Doesn’t Work",
    "text": "5.4 Investigating Why DBT Works or Doesn’t Work\nDialectical behavior therapy (DBT) is an intervention strategy similar to cognitive behavior therapy. It focuses on using mindfulness training to help regulate one’s emotions and behaviors. Developed first for those with borderline personality disorders and subsequently found to be quite effective among those with suicidal tendencies, it is seen by some as holding potential use for more general populations.\nAmong those who believe it may help a broader range of individuals is the head of a local school. This school already has a modified—and more pervasively infused—health curriculum that seeks to help the school’s adolescent students understand and control their emotions, and through that be more in control of their academic and social lives.\nThis school therefore implemented a DBT “curriculum” that is completed by seventh- and eighth-grade students. There are now about four years of data tracking these DBT-participating students from sixth through ninth grade. This gives us a sense of how they were before participating in the DBT program, how they did during it, and if any changes persist a while afterwards. We also have sixth- through ninth-grade data for all students from years before the school implemented the DBT curriculum; these previous years’ students can serve as ersatz controls.\nInitial analyses contend that the DBT program is associated with mild but significant improvements in participants’ executive functions relative to this ersatz control group. DBT participation was not associated with significant changes in academic performance. Of greater concern for the school, though, was that some students benefited much more strongly from the program than other students—and that students seemed not to benefit at all.\nI was recently asked by the school to lend insight into those students who did not respond well to the DBT program. The idea is to understand for whom it does and doesn’t work so that the school can tailor the program to better help the students and perhaps to provide additional help for those who didn’t seem to get enough from the DBT program.\nI was then asked to present my findings to the school’s administration. In fact, I presented it about three times, each time making my explanation (and analyses) simpler and simpler.\n\n5.4.1 Your Task\nHow would you do if asked to do the same thing? Please therefore:\n\nUse simple descriptive statistics—such as frequency counts—as well as zero-order and partial correlations\nTo investigate what factors (in the data described below) predict which students will benefit from the DBT program.\nPlease write this up in a simple 1 – 2-page report that could be understood by non-experts.\n\nThe dataset ef_slopes.ods is accessible in our course’s BlackBoard site.\nPlease note:\nAlthough anonymized, these are real data. Please treat them with the respect, confidentiality, and care.\nNote as well that, as real data, there may be expected relationships between variables are not strong and unexpected relationships that are. In addition, most relationships won’t be statistically significant. Please don’t rely on significance as the only criterion you use to make your decisions: Instead, compare the relative magnitude of relationships to find ones you think hold more promise than other relationships; if something is indeed significant, that's nice, but we don't need to hold ourselves just to that.\n\n\n5.4.2 Description of the Data\nThe variables in the dataset are:\n\nID\nAn identifier for each student.\n\n\nReceived DBT Intervention?\nWhether the given student did (1) or did not (0) participate in the full regimen of the DBT program.\n\n\nTeacher Same Ethnicity?\nWhether the teacher is (1) or is not (0) the same ethnicity as the student.\n\n\nTeacher Same Gender?\nWhether the teacher is (1) or is not (0) the same gender as the student.\n\n\nBehavioral Regulation Slope – Teacher Report\nEach year, teachers at the school are asked to rate a subset of their students on a list of various behaviors believed to indicate a student’s level of executive functioning. The scores on these behaviors are summed to create a score for each student each year. To facilitate analyses here, I computed the normalized slope for the line on which a student’s yearly score regressed. For all slopes, negative values indicate improvements—that a student’s functioning got better.\nBehavioral regulation includes a subset of executive functions most closely related to one’s overt behaviors.\n\n\nMeta-Cognitive Slope – Teacher Report\nMeta-cognition includes a subset of executive functions most closely aligned with how one thinks and feels.\n\n\nAll Executive Functions Slope – Teacher Report\nAll executive functions combines the scores of the behavioral regulation and meta-cognitive sub-domains into a total score.\n\n\nBehavioral Regulation Slope – Student Self-Report\nEach year, in addition to the teachers rating a subset of their students, each student also rates themself on how they feel they’ve behaved vis-à-vis behaviors related to executive functioning.\nThe behavioral regulation slope is the regression line for the scores for each students across these four years on executive functions most related to overt behaviors. Again, negative slopes denote improvements in executive functioning.\n\n\nMeta-Cognitive Slope – Student Self-Report\nAgain, these are executive functions most closely aligned with internal cognitions or emotions—here as self-reported by the students.\n\n\nAll Executive Functions Slope – Student Self-Report\nThe students’ self-reported behavioral regulation and meta-cognitive subscores are first combined here before a slope for the regression line was computed for each student.\n\n\nMindfulness Slope\nRecently, we added a self-reported mindfulness score by asking students to complete another instrument commonly seen to measure just that. Again, negative slopes indicate improvements.\n\n\nEmotional Regulation Slope\nThe students complete a third instrument that measures their emotional regulation per se. Again, negative slopes indicate improvements.\n\n\nDiscipline Incidents Slope\nRecords are kept for whenever something happens on campus that leads to a student being “written up” for something that requires that student being somehow disciplined. Therefore, each student has a number of times that they are “written up” each year. This number could range from zero (for never having been written up for something requiring discipline) to sometimes rather large values.\nThis slope is the linear regression of this number of discipline incidents each year. Positive values therefore indicate growths in the number of times that student “got into trouble.”\nThis is also among the main outcome variables the school was interested in: They wanted to understand who had more discipline incidents, who subsequently got better, and—especially—whom benefited least from the DBT program so that these student (and those like them) could be targeted for more help.\n\n\nSelf-Reflection Slope\nThe linear regression for the number of times a student was put in what is essentially a mild form of in-school suspension. Positive values denote more incidents.\n\n\nIn-School Suspension Slope\nThe linear regression for the number of times a student was put in what is essentially actual in-school suspension. Positive values denote more incidents.\n\n\nOut-of-School Suspension Slope\nThe linear regression for the number of times a student was suspended from school; this is the most severe of the three types of suspensions. Positive values denote more incidents.\n\n\nELA Grade\nA 4-point grade for the student on English / language arts courses for that student’s ninth grade. 4 is the highest grade possible.\n\n\nMath Grade\nA 4-point grade for the student on math courses for that student’s ninth grade. 4 is the highest grade possible.\n\n\nFemale?\nWhether the student does (1) or does not (0) identify as female.\n\n\nFree/Reduced School Lunch Group\nWhether the student is in the economic stratum that was previously used to allow them to be eligible for free (1) or reduced (0) school lunches, or whether they were not eligible (-1) for free / reduced school lunches.\n\n\nEconomic Distress?\nLike Free/Reduced School Lunch Group, this is a measure of the student’s family’s economic situation, simply whether a student does (1) or does not (0) classify as experiencing economic distress.\n\n\nMother at Home?\nWhether the student self-reported that their mother does (1) or does not (0) live with them.\n\n\nFather at Home?\nWhether the student self-reported that their father does (1) or does not (0) live with them.\n\n\nAdult Brother at Home?\nWhether the student self-reported that one or more adult brothers do (1) or do not (0) live with them.\n\n\nAdult Sister at Home?\nWhether the student self-reported that one or more adult sisters do (1) or do not (0) live with them.\n\n\nGrandparent at Home?\nWhether the student self-reported that one or more grandparents do (1) or do not (0) live with them.\n\n\nOther Adult at Home?\nWhether the student self-reported that one or more “other” adults do (1) or do not (0) live with them.\n\n\nTotal Number of Adults st Home\nSimply the sum of each of the other “adults at home” variables.\n\n\nSpecial Education Status\nWhether a student has been (1) or has not been (0) diagnosed with a condition that makes them eligible for special education services.\n\n\nIntellectual Impairment?\nWhether a student has been (1) or has not been (0) diagnosed with an intellectual disability.\n\n\nSocial Emotional Impairment\nWhether a student has been (1) or has not been (0) diagnosed with a social and/or emotional disability.\n\n\nHigh Risk Category\nWhether a student has been (1) or has not been (0) identified as benefiting from interventions related to high-risk behaviors.\n\n\nYears at This School\nThe number of years the student has been at this school.\n\n\nNumber of School Absences\nThe number of times in ninth grade that the student was absent—excused or not—from school.\n\n\nEthnicity: Asian\nA dummy variable indicating whether the student identifies as Asian (1) or not (0). Note that dummy variables are a good way to handle times when participants can identify with more than one group; some participants are identified here by more than one “ethnicity” dummy variable.\n\n\nEthnicity: American Indian\nA dummy variable indicating whether the student identifies as American Indian (1) or not (0).\n\n\nEthnicity: Black\nA dummy variable indicating whether the student identifies as Black (1) or not (0).\n\n\nEthnicity: Hispanic\nA dummy variable indicating whether the student identifies as Hispanic/Latin (1) or not (0)).\n\n\nEthnicity: White\nA dummy variable indicating whether the student identifies as White (1) or not (0)."
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#using-spss",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#using-spss",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "5.5 Using SPSS",
    "text": "5.5 Using SPSS\n\n5.5.1 Accessing SPSS & Data Importation\n\nGo to CUNY Virtual Desktop: https://www.cuny.edu/about/administration/offices/cis/virtual-desktop/\nClick on the SPSS icon to open it. N.b.: You session can time out and suddenly. Make sure you save your work to an actual hard drive (or USB, online account, etc.) so you don’t loose it!\nIn SPSS, with Open another file... highlighted, click Open and navigate to wherever you have ef_slopes.ods saved.\n\nThe first line indeed includes names, so leave that option checked\nThe delimiter is indeed a comma, decimals are periods, and text marked with double quotes, so leave those options chosen\n\nWhen you finish the data importations, SPSS opens an Output window (in addition to the Data Editor window). In this window will appear both the results of any analyses and the code that SPSS used to generate those results. This is worth explaining.\n\nA benefit of SPSS is that it has an efficient and intuitive GUI, but in fact that GUI is used by SPSS to generate code that it actually uses to manipulate data and run operations.\nAt first, then, you can rely on the GUI to run analyses. However, we you gain experience, you can modify the code generated through the GUI before finally simply pasting / writing in code without using the GUI.\nYou can also easily chose parts of SPSS’s output to copy and paste elsewhere, e.g., into a manuscript (for figures, tables, etc.) or a text editor (for code).\n\nFor example, if we had specified formats for the variables in the Advanced Options for the data importation, we could pate out the code generated now into a file that we simply access to re-import those (or similar) data.\nIn the output, you’ll notice that you can navigate from the left-hand menu, as well as delete output you no longer want. You can also right-click to rename many aspects of it and then save / export it.\n\nYou can also access the GUI from either the Data View window or the Output window, letting you run / modify analyses either while looking at your data or the results, respectively.\n\nFocusing instead on the Data Editor window, notice there are two tabs at the bottom, a Data View and a Variable View tab.\n\nThe Data View tab presents the data in the matrix form one is used to in a spreadsheet program.\n\nLike a spreadsheet program, clicking into a cell allows one to directly modify its contents.\nRight-clicking a column allows one to access and modify information, etc. about that variable. (Right-clicking rows has fewer options.)\n\nNote that SPSS thus requires that data be formatted with variables as columns and cases as rows.\n\n\n\n\n\n\n5.5.2 Conducting Correlations and Partial Correlations in SPSS\n\nTo conduct correlations, simply click Analyze &gt; Correlate &gt; Bivariate\nIn the dialogue box that opens, put the variables you wish to correlate. However, we can have SPSS compute zero-order correlations along with partial correlations.\nTo conduct partial correlations, choose Analyze &gt; Correlate &gt; Partial\n\nIn the Partial Correlations dialogue box, choose which variables you wish to correlate, and which you’d like to control for:\n\nClick the Options button, and select, e.g., Means and standard deviations and Zero-order correlations6 checkbox in the Statistics area\nPlease use these correlations along with table of, e.g., frequencies to prepare a report on insights you make into these data."
  },
  {
    "objectID": "Variance_Covariance_Correlations_and_Partial_Correlations.html#footnotes",
    "href": "Variance_Covariance_Correlations_and_Partial_Correlations.html#footnotes",
    "title": "5  Variance, Covariance, Correlations, and Partial Correlations",
    "section": "",
    "text": "You’ll often hear people say the reason we square the distances and then take the square root is to make all of the distances positive; otherwise, all of the distances from the mean would cancel each other out. Although this is true, couldn’t we just take the absolute value of each distance instead?\nYes, we could. A major difference between taking-the-absolute value-of-the-distances and squaring-then-square-rooting-the-distances is that squaring-then-square-rooting-the-distances amplifies the role of data points farther from the mean. And amplifying their effects was indeed one reason that method was chosen: Those who devised all of this wanted to give stronger weight to scores that are farther from the mean. Outliers and other scores that aren’t as well represented by the mean were intentionally made to matter more, in part because they were considered to have more information in them them scores closer to the mean.\nSo yeah, it’s kinda ironic that a lot of consternation is now given to removing outliers from data sets to produce more robust statistics.↩︎\nThe reason we use (n – 1) instead of simply n is a good one—even if it’s a rather nuanced reason. We are using a sample to estimate the values for a population, and since we don’t know the population mean, we take away a degree of freedom to indicate we don’t know that value. In fact, every time we have to estimate something in an analysis, we take away a degree of freedom to estimate that value. Practically, this serves to make the sample SD slightly larger than the population SD since we are dividing by a smaller number (1/2 of something is larger than 1/3 of it). This reflects the fact that we don’t know so well the actual (population) value.\nWe also typically use lower-case n to denote the number in a sample or subset of data set and upper-case N to denote the total number, e.g., in a population or in the whole data set, but this convention isn’t always followed, so we usually just infer what the n/N refers to.\nFinally, by convention, we italicize Roman letters in equations so that we know those letters denote variables; we don’t italicize Greek letters, because, well, Greece is not in Italy. That, and since we don’t usually write things in Greek, it’s pretty clear those Greek letters denote variables. Confusingly, the way I generate the formulas in this book doesn’t follow this convention and italicizes Greek letters as well; please do as I say, not as I do.↩︎\nAnd that’s an other reason we don’t just take the absolute value of the distances of each point from the mean. Variance—the average squared distance—is used “under the hood” in calculations more often than standard deviation.↩︎\nThis formula is the main way that outliers have an out-sized effect on what we do in frequentist statistics. They more strongly affect the variance and covariance that are in turn used in many equations.↩︎\nYeah, we also often first subtract them from the mean, so that a 0 for one variable is comparable to a 0 from another variable.↩︎\nRemember, zero-order correlations is the confusing term for simple, bivariate correlations between two variables.↩︎"
  },
  {
    "objectID": "effect_size.html#common-effect-sizer-statistics",
    "href": "effect_size.html#common-effect-sizer-statistics",
    "title": "6  Effect Size: Explanation and Guidelines",
    "section": "6.1 Common Effect Sizer Statistics",
    "text": "6.1 Common Effect Sizer Statistics\n\n\n6.1.1 Mean Differences\nThese measure the distance between two or more means. Like most effect size statistics, they are also standardized (measured in terms of standard deviations) so they can be compared between studies.\n\nCohen’s d \nOne of the most commonly used effect size statistics is Cohen’s d, which expresses the standardized difference between two group means:\n\\[\\text{Cohen's }d = \\frac{\\text{First Mean}-\\text{Second Mean}}{\\text{Pooled }SD}.\\]\nWe combine (or “pool” the SDs because there are two of them (one SD for each mean). To do this, we essentially take the average of the two SDs2.\nTherefore, Cohen’s d is presented in terms of standard deviations. A Cohen’s d of 1 means that the means are one standard deviation apart.\nYou may remember that z-scores are also presented in terms of standard deviations—that a z-score of 1 means that that person’s score is one standard deviation away from the mean. This isn’t a coincidence and means that Cohen’s d can be looked at as a z-score.\n\n\nCohen’s f and f2 \n\nCohen introduced f as a measure of effect size for F-tests, specifically to quantify differences among three or more means. In contrast, he developed d to measure the effect size between two means. The exact formula for computing f varies slightly depending on the number of levels in the factor and the variance structure.\nTo extend this concept to more complex models, Cohen introduced f², which applies not only to ANOVA-family models but also to general(ized) linear regression. The primary distinction between f and f² is that f² is simply f squared. Cohen recommended using f² for complex models because it aligns with how other parameters, such as variance-explained measures, are typically computed using squared values.\nAn important advantage of f² is its flexibility: it can be used to assess the effect of a single predictor or a set of predictors, whether or not other variables in the model have been controlled for or partialed out.\nMore about Cohen’s f can be found at this Statistics How to page.\n\n\nOther Measures of Maen Differences\nCohen’s d is not the only measure of the effect size of mean differences—although it is the most common. Two others—Hedges’ g and Glass’s Δ —are worth mentioning. All three are all standardized effect size measures used to quantify the difference between two groups in terms of standard deviations, but they differ slightly in calculation and applicability.\n\n\nTable 6.1: Common Effect Size Measures of Mean Differences\n\n\n\n\n\n\n\n\nAspect\nCohen’s d\nHedges’ g\nGlass’s Δ\n\n\n\n\nDenominator\nPooled standard deviation\nPooled standard deviation with small-sample correction\nControl group standard deviation\n\n\nUse Case\nLarge samples, equal variances\nSmall samples\nUnequal variances\n\n\nCorrection Factor\nNone\nCorrects for small sample bias\nNone\n\n\nApplicability\nWidely used in social sciences\nMore accurate for small samples\nBest for heteroscedastic data\n\n\n\n\nSummary\n\nUse Cohen’s d in large-sample studies with equal variances.\nUse Hedges’ g to correct for bias in small samples.\nUse Glass’s \\(\\Delta\\) when group variances are expected to differ substantially.\n\n\n1. Cohen’s d\n\nCohen’s d measures the standardized mean difference between two groups.\nFormula: \\[d = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p}\\] where:\n\n\\(\\bar{X}_1\\) and \\(bar{X}_2\\) are the means of the two groups.\n\\(s_p\\) is the pooled standard deviation: \\[s_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\\]\n\nKey Points:\n\nAssumes equal variances between the groups (homoscedasticity).\nSuitable for large samples.\nCan overestimate the effect size for small sample sizes.\n\n\n\n\n2. Hedges’ g (Correction for Small or Unequal Samples)\n\nHedges’ g is a variation of Cohen’s d that corrects for the upward bias in d when sample sizes are small (usually considered when n &lt; 20).\nFormula: \\[g = d \\times \\left(1 - \\frac{3}{4(n_1 + n_2 - 2) - 1}\\right)\\]\nKey Points:\n\nIncorporates a correction factor to reduce bias in small sample sizes.\nProvides a more accurate effect size estimate when (n &lt; 20).\nFor large samples, Hedges’ g converges to Cohen’s d.\nOften used in meta-analysis where comparisons between studies of very different sizes are made.\n\n\n\n\n3. Glass’s Δ\n\nGlass’s Δ uses only the standard deviation of the control group ((s_2)) as the denominator, instead of a pooled standard deviation.\nFormula: \\[\\Delta = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_2}\\] where:\n\n\\(s_{2}\\) is the standard deviation of the control group.\n\nKey Points:\n\nUseful when variances between groups are unequal (heteroscedasticity).\nMay produce biased estimates if the control group standard deviation is not representative.\nOften applied in scenarios where the experimental treatment group might naturally have a higher variance (e.g., due to a treatment effect).\n\n\n\n\n\n\n6.1.2 Proportions of Variance Explained\nCohen’s d and f measure the (standardized) difference between means. Cohen’s d measures it for two means, while Cohen’s f is used to measure it between three or more means. Both of these statistics can be as small as zero (when there is no difference) to positive infinity. Both simply represent the number of standard deviations between the means, and if the effect size is more than 1 SD, then the effect size will be greater than 1.\nAn other set of effect size measures are standardized differently: They measure proportions, and so can only range between 0 and 1. The ones describe in this section measure the proportion of total variance explained by a particular term in a regression model.\n\n(Squared) Correlations\n\nPerhaps the simplest measure of proportion of variance explained is correlations, specifically squared correlations. Squared correlations are indeed effect size statistics, and they measure the amount of variance explained in each of the two variables that is explained by their relationship compared to all of the variance in each of them.\nFor example, if the correlation between two variables is .50, i.e., if r = .50, then r2 = .502 = .25. In that case, the correlation accounts for 25% of the variance in each of the variables.\n\n\nEta-squared (η2) and Partial η2\n\n\nThe other three “proportion of variance explained” statistics are used to measure the effect size of individual terms in a linear regression model.\nThe first of these is eta-squared (\\(\\eta^2\\)), which quantifies the proportion of total variance in the outcome variable that is explained by a given predictor. It is calculated as:\n\\[\n\\eta^2 = \\frac{SS_{\\text{Effect}}}{SS_{\\text{Total}}}\n\\]\nThis makes \\(\\eta^2\\) conceptually similar to \\(R^2\\), which measures the total proportion of variance explained by all predictors in a regression model. Like the correlation coefficient \\(r\\), eta (\\(\\eta\\)) itself can be understood as the proportion of standard deviation differences in the outcome explained by the predictor, while \\(\\eta^2\\) represents variance explained as a proportion of total variance.\nHowever, \\(\\eta^2\\) has a notable limitation: it does not account for other predictors in the model. As additional terms are introduced, the individual \\(\\eta^2\\) values for each predictor tend to decrease, since they represent only the variance uniquely attributable to each predictor relative to total variance.\nTo address this, researchers use partial eta-squared (\\(\\eta_p^2\\)), which represents the proportion of variance explained by a specific predictor after accounting for other predictors in the model. Partial \\(\\eta^2\\) is conceptually similar to partial \\(r^2\\), as it isolates the unique contribution of a predictor while removing variance shared with other terms.\nIn a one-way ANOVA (i.e., a model with a single categorical predictor), \\(\\eta^2\\) is equivalent to the overall model \\(R^2\\). However, in models with more than one predictor, partial \\(\\eta^2\\) is preferred and the overall \\(R^2\\) will be different than each of the partial \\(\\eta^2\\)s.\n\n\\(\\eta^2\\) Compared to Cohen’s \\(f\\) and \\(f^2\\)\nCohen’s \\(f\\) and \\(f^2\\) serve a similar purpose but differ in how they handle variance:\n\n\\(\\eta^2\\) vs. \\(f\\) (ANOVA): While \\(\\eta^2\\) measures the proportion of variance explained by a factor, \\(f\\) adjusts for unexplained variance, making it more suitable for cross-study comparisons. The relationship between them is:\n\n\\[f = \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\\]\n\nPartial \\(\\eta^2\\) vs. \\(f^2\\) (Regression): Partial \\(\\eta^2\\) describes the proportion of variance explained by a predictor after controlling for other variables, while Cohen’s \\(f^2\\) expresses the incremental contribution of a predictor relative to the unexplained variance:\n\n\\[f^2 = \\frac{R^2}{1 - R^2}\\]\nSince \\(f^2\\) explicitly models the variance explained relative to unexplained variance, it is commonly used in multiple regression, particularly for power analysis and comparing models across studies.\nThus, while \\(\\eta^2\\) and partial \\(\\eta^2\\) are useful for describing within-sample variance explained, \\(f\\) and \\(f^2\\) provide standardized effect size measures better suited for meta-analysis and statistical power estimation.\n\n\nTable 6.2: When to Use \\(\\eta^2\\), \\(f\\), and \\(f^2\\)\n\n\n\n\n\n\n\n\nCriterion\n\\(\\eta^2\\)\n\\(f\\) (ANOVA)\n\\(f^2\\) (Regression)\n\n\n\n\nUse Case\nANOVA (variance explained)\nANOVA (standardized effect size)\nRegression (incremental variance explained)\n\n\nInterpretation\nProportion of total variance explained\nStandardized measure of effect size\nStandardized measure of predictor impact\n\n\nBest for Comparing Studies?\nNo\nYes\nYes\n\n\nUsed in Power Analysis?\nNo\nYes\nYes\n\n\nInflation in Small Samples?\nYes\nNo\nNo\n\n\n\n\nTherefore:\n\nUse \\(\\eta^2\\) to describe the proportion of variance explained in ANOVA and regression models.\nUse Cohen’s \\(f\\) for standardizing effect sizes in ANOVA, making them comparable across studies.\nUse Cohen’s \\(f^2\\) in regression to assess the impact of specific predictors, particularly when measuring incremental effects.\nFor a single dichotomous predictor, Cohen’s d and \\(\\eta^2\\) can be converted into each other, but for more complex models, additional transformations are required.\n\nThis Analysis Factor post gives a good further explanation of η2. Recommendations on interpreting and reporting η2 are given well in this StackExchange Q&A.\n\n\n\nOmega-squared (ω2) \nω2 is very similar to η2. They both measure proportion of total variance accounted for by a given term in a model, but compute it in slightly different ways3. The way η2 computes it makes it systematically overestimate the size of an effect—when it is used to measure the size of the effect for the population (i.e., when inferring from the sample to the population). Although this overestimation gets smaller as the sample gets larger, it always present (until the sample is the same size as the population).\nThe way ω2—and partial ω2—estimate unexplained variance makes them always smaller than η2 (and partial η2). ω2 is therefore a more conservative estimate of effect size than η2. Given this, many prefer ω2 over η2.\n\n\nEpsilon-squared (ε2) \nThe third and final member of our Greek-alphabet soup of stats to measure the proportion of variance explained is ε2. Everyone agrees that η2 overestimates the effect. Some, like Okada (2013), argue that ω2 is sometimes too conservative, underestimating the true size of an effect.\nε2 (and partial ε2) may be closer to “just right,” giving what may be the least biased estimate. Anyway, its value is always between the other two (or equal to them).\nIt’s worth noting that in a one-way ANOVA, ε2 is equal to the adjusted R2.\n\n\n\n6.1.3 Odds & Risk Ratios\n\nOdds ratios (ORs) and risk ratios (RRs) are often treated as standardized measures of effect size. Under appropriate conditions—i.e., comparable outcome definitions and baseline rates—they can be used to compare the magnitude of associations across studies.\nRisk is simply another term for probability, and risk ratios represent the relative likelihood of an event between two groups. Both risks and risk ratios range from 0 to 1, much like proportion-of-variance metrics such as \\(\\eta^2\\) or \\(R^2\\) Section 6.1.2.\nIn contrast, odds and odds ratios are unbounded above and can exceed 1. This asymmetry may make them less intuitive for some audiences, especially when comparing across studies. Nonetheless, it is statistically valid to compare odds or odds ratios across studies—though in some contexts, interpretability may be improved by transforming them to effect size statistics bounded between 0 and 1.\nTwo classic measures that do just this are the φ (phi) coefficient and Yule’s Q. Both are designed to quantify the strength of association between two binary variables—for example, the relationship between disease status (present/absent) and group membership (exposed/unexposed). When variables have more than two categories, related measures such as Cramér’s V are more appropriate.\nThe φ coefficient is defined as:\n\\[\n\\phi = \\frac{AD - BC}{\\sqrt{(A + B)(A + C)(D + B)(D + C)}}\n\\]\nwhere \\(A\\), \\(B\\), \\(C\\), and \\(D\\) refer to the cell counts of a 2 \\(\\times\\) 2 contingency table:\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n& \\text{Present} & \\text{Not Present} \\\\\n\\hline\n\\text{Group 1} & A & B \\\\\n\\hline\n\\text{Group 2} & C & D \\\\\n\\hline\n\\end{array}\n\\]\nDespite its structural differences from the Pearson correlation coefficient, φ is mathematically equivalent to \\(r\\) when both variables are dichotomous. It is also frequently used as an effect size accompanying \\(\\chi^2\\) tests, and can be computed directly as \\(\\phi = \\sqrt{\\chi^2 / n}\\).\nWhile φ is a valid and interpretable measure of association, it has notable limitations. It is sensitive to rare outcomes and can be inflated when marginal frequencies are highly unbalanced. This makes φ less suitable for studies involving rare events—such as mortality rates—where other statistics may provide more stable estimates.\nYule’s Q was developed to address these limitations. It is specifically designed to measure association between odds and is effectively a transformation of the odds ratio into a scale ranging from −1 to +1, similar to correlations. Given a 2 \\(\\times\\) 2 contingency table, it is defined as:\n\\[\nQ = \\frac{AD - BC}{AD + BC}\n\\]\nAlternatively, it can be expressed directly in terms of the odds ratio:\n\\[\nQ = \\frac{\\text{OR} - 1}{\\text{OR} + 1}\n\\]\nThis transformation offers a symmetric, bounded, and more interpretable summary of the magnitude of association when using odds ratios."
  },
  {
    "objectID": "effect_size.html#sec-small-large-effects",
    "href": "effect_size.html#sec-small-large-effects",
    "title": "6  Effect Size: Explanation and Guidelines",
    "section": "6.2 “Small,” “Medium,” & “Large” Effects",
    "text": "6.2 “Small,” “Medium,” & “Large” Effects\n\nLike much of statistics, Cohen’s d in standardized into z-scores/SDs (remember, the formula for it is to divide it by SDs). However, simply reporting Cohen’s d without interpreting what that means has a couple of disadvantages: (a) z-scores are not intuitive for lay audiences, and (b) there are other measures of effect size than Cohen’s d—and they aren’t all measured on the same scale. Given both of these factors, in his seminal book, Statistical Power Analysis for the Behavioral Sciences, Jacob Cohen (1988) gave recommendations for how to interpret the magnitude of various effect size statistics in terms of “small,” “medium,” and “large” effects.\nThese “criteria” for evaluating the magnitude of an effect size have become quite popular. Indeed, the adoption of effect size statistics seems to be regulated by people’s uses and understandings of them in relation to these criteria. They therefore deserve further consideration.\n\n6.2.1 Effect Size Criteria as Percent of Total Variance\nCohen generally defined effect sizes based on the percent of the total variance that effect accounted for4:\n\n“small” effects account for 1%,\n“medium” effects account for 10%, and\n“large” effects account for 25%.\n\nI say that he generally defined them as such because he didn’t see a need to be bound to this definition, in part because he repeatedly noted—as do I here—that these criteria were arbitrary. He defined them based on percent of total variance for d and then chose “small,” “medium,” and “large” values for other effect size statistics that corresponded to those values for d.\nThis meant, for example, that he chose levels for correlations that don’t always match up to what one would expect by squaring the correlations to get the percents of total variances. In other words, his criteria for correlations weren’t that a “small” correlations would be r = .1 (i.e., where r2 = .01), “medium” would be r = .5, and “large” r \\(\\approx\\) .63. In justifying this, he notes) that he is not positing these criteria levels based on strict mathematical equivalences but instead on a concerted attempt to equate the sorts of effects one would obtain with one analytic strategy with an other analytic strategy; for example, the types of effects sizes (experimental psychologists) obtain with t-tests with those they would obtain through correlations.\n\n\n6.2.2 Effect Size Criteria as Noticeability of Effects\nAlthough Cohen was thorough in his descriptions of these effect size criteria in terms of proportions of total variance, he was also careful to couch them in practical and experimental terms.\nA “small” effect is the sort he suggested one would expect to find in the early stages of a line of research when researchers have not yet determined the best ways to manipulate/intervene and when much of the noise had not yet been controlled.\nA “small” effect can also be considered to be a subtle but non-negligible effect: the sorts of effects that are often found to be significant in field-based studies with typical samples and manipulations/interventions. Examples Cohen gives include:\n\nThe mean difference in IQs between twin and non-twin siblings5,\nThe difference in visual IQs of adult men and women, &\nThe difference in heights between 15- and 16-YO girls.\n\nA “medium” is one large enough to see with the naked eye. Example Cohen gives include:\n\nThe mean difference in IQs between members of professional and managerial occupations,\nThe mean difference in IQs between “clerical” and “semiskilled” workers, &\nThe difference in heights between 14- and 18-YO girls.\n\nA “large” effect is one that is near the upper limit of effects attained in experimental psychological studies. So yes, the generalization of this criterion to other areas of science—including nursing research—is certainly not directly supported by Cohen himself.\nExamples include:\n\nThe mean difference in IQs between college freshmen and those who’ve earned Ph.D.s6,\nThe mean difference in IQs between those who graduate college and those who have a 50% chance of graduating high school, &\nThe difference in heights between 13- and 18-YO girls, &\nThe typical correlation between high school GPAs and scores on standardized exams like the ACT.\n\n\n\n6.2.3 Effect Size Criteria for Odds Ratios\nCohen (1988) discussed proportions (aka risks) and presented effect size measures for a proportion’s difference from .5 (Cohen’s g) and the difference between two proportions (Cohen’s h), which could be used to present the magnitude of a risk ratio; even though a risk ratio per se is already a fine effect size stat, Cohen didn’t give size criteria for risk ratios, but instead for his h.\nHe didn’t, however, discuss odds or odds ratios directly, and thus didn’t give his opinion about what could be considered “small,”“medium,” and “large” values for odds or odds ratios. Yule’s Q (Section 6.1.3) can be considered comparable to risk ratios, risk ratios weren’t given size criteria either.\nChen et al. (2010) nonetheless gives some guidance by providing ranges of effect size criteria for odds ratios by comparing values with criteria for “small,” “medium,” and “large” Coden’s ds. Chen et al.’s (2010) rules of thumb for “small,” “medium,” and “large” odds ratios (below) deserve especial explanation.\nThe size of an odds ratio depends not just on the difference in outcomes in a group (e.g., the numbers of Black woman with and without pre-eclampsia), but also the difference in outcomes in a comparison group (e.g., the numbers of non-Black women with and without pre-eclampsia). It is thus not so easy to compute simple (simplistic) rules of thumbs for the sizes of odds ratios7.\nIn addition, the exact values for what to consider as a “small,” “medium,” and “large” effect depend on the overall frequency, with smaller events require larger odds ratios to equate to a given level of Cohen’s d.\nNonetheless, Chen et al. (2010) presents some guidelines that can serve as guides in most cases. Using the median values suggested by their results:\n\n“Small” \\(\\approx\\) 1.5\n“Medium” \\(\\approx\\) 2.75\n“Large” \\(\\approx\\) 5\n\nHowever, those suggestions can vary substantially based on the event rate in the reference group (infection rates in the non-exposed group in Chen et al.’s article):\n\nSome Suggested Odds Ratios Corresponding to “Small,” “Medium,” and “Large” Effect Sizes Based on the Probability of the Event in the Reference Group (from Chen et al., 2010, p. 862)\n\n\nProbability of Event\nin Reference Group\n“Small” OR\n“Medium” OR\n“Large” OR\n\n\n\n\n.01\n1.68\n3.47\n6.71\n\n\n.05\n1.53\n2.74\n4.72\n\n\n.10\n1.46\n2.50\n4.14\n\n\n\nThese estimates are based on simulations assuming a logistic model and are meant as heuristics, not rigid standards. Importantly, they illustrate that the magnitude of an odds ratio is not directly comparable across studies unless the base rates are similar.\n\n\n6.2.4 A Few Words of Caution About Effect Size Criteria\nAs useful as it is to talk about effect sizes being “small” or “large,” I must underline Cohen’s own admonition (e.g., p. 42) that we use this rule of thumb about “small,” “medium,” and “large” effects cautiously8. He notes, for example, that\n\nwhen we consider r = .50 a large [effect size], the implication that .25 of the variance is accounted for is a large proportion [of the total variance] must be understood relatively, not absolutely.\n\n\nThe question, “relative to what?” is not answerable concretely. The frame of reference is the writer’s [i.e., Cohen’s own] subjective average of [proportions of variance] from his reading of the research literature in behavioral science. (pp. 78 – 79)\n\nMany people—including reviewers of manuscripts and grant proposals—take them to be nearly as canonical as p &lt; .05 for something being “significant.” This is a real shame since effect sizes offer us the opportunity to finally move beyond making important decisions based on simplistic, one-size-fits-all rules.\nTherefore, effect size measures, including Cohen’s d, are best used objectively to compare effects between studies—not to establish some standardized gauge of the absolute value of an intervention. This is indeed part of what is done in meta-analyses.\nIt is also what I suggest doing within your own realm of research: Just like Cohen himself did, review what appears to be generally agreed on as “small,” “medium,” and “large” effects within your research realm. These could, for example, correspond to levels of clinical significance9. Unfortunately, though, Cohen’s suggestions for his realm of research have become themselves canonized as the criteria for most lines of research in the health and social sciences.\nIndeed, interventions and factors that have “small” effects can be quite important. This seems especially true for long-term changes, such as those one strives for in educational interventions or for the subtle but persistent effects of racism. Teaching a diabetic patient how to check their blood insulin may have only a small effect on their A1C levels in a given day, but can save their life (or at least a few toes) in the long run.\nGiven this, Kraft (2020) used a review of educational research to suggest different criteria for gauging what should be considered as “small,” “medium,” or “large” effects in education interventions. His recommendations are also presented below.\n\n\n6.2.5 Table of Effect Size Statistics\n\n\nTable 6.3: Effect Size Interpretations\n\n\nStatistic\nExplanation\nSmall\nMedium\nLarge\nReference\n\n\n\n\nd\nDifference between two means\n0.2\n0.5\n0.8\nCohen (1988, p. 25)\n\n\nd\nFor education interventions\n0.05\n\\(&lt;\\) .2\n\\(\\ge\\) .2\nKraft (2020)\n\n\ng\nHedges’ modification of Cohen’s d for small samples\n0.2\n0.5\n0.8\nHedges (1981)\n\n\nh\nDifference between proportions\n0.2\n0.5\n0.8\nCohen (1988, p. 184)\n\n\nw\n(also called φ)\nχ2 goodness of fit & contingency tables.\nφ is also a measure of correlation in 2 \\(\\times\\) 2 contingency tables, and ranges between 0 and 1.\n0.1\n0.3\n0.5\nCohen (1988, p. 227)\n\n\nCramer’s V\nSimilar to φ, Cramer’s V is used to measure the differences in larger contingency tables.\nLike φ (and other correlations) it ranges between 0 and 1.\n0.1\n0.3\n0.5\nCohen (1988, p. 223)\n\n\nr\nCorrelation coefficient (difference from r = 0)\n0.1\n0.3\n0.5\nCohen (1988, p. 83)\n\n\nq\nDifference between correlations\n0.1\n0.3\n0.5\nCohen (1988, p. 115)\n\n\nη2\nParameter in a linear regression & AN(C)OVA\n0.01\n0.06\n\\(\\ge\\) .14\n\n\n\nf\nAN(C)OVA model effect; equivalent to \\(\\sqrt{f^2}\\)\n0.1\n0.25\n0.4\nCohen (1988, p. 285)\n\n\nf\nFor education interventions (i.e., f equivalent for Cohen’s ds suggested by Kraft,)\n0.025\n\\(&lt;\\) .1\n\\(\\ge\\) .1\nKraft (2020)\n\n\nf2\nA translation of R2\n0.02\n0.15\n0.35\n• For multiple regression / multiple correlation, Cohen (1988, p. 413);\n• For multivariate linear regression, multivariate R2, Cohen (1988, p. 477)\n\n\nOR\nOdds ratio; can be used as effect size for Fisher’s exact test and contingency tables in general.\n1.5\n(or 0.67)\n2.75\n(or 0.36)\n5\n(or 0.20)\nChen et al. (2010, p. 862)"
  },
  {
    "objectID": "effect_size.html#sec-effect_size_conversions",
    "href": "effect_size.html#sec-effect_size_conversions",
    "title": "6  Effect Size: Explanation and Guidelines",
    "section": "6.3 Converting Between Effect Size Measures",
    "text": "6.3 Converting Between Effect Size Measures\n\nMost effect size statistics can be converted into other ones, but the process isn’t always possible or direct (or requires additional assumptions). Table 6.5 presents the effect sizes statistics covered here that can be converted (and the conditions/assumptions required for that); Table 6.6 presents the effect size statistics that can’t be meaningfully converted.\nMore usefully, Table 6.4 presents the formulas for convert between the effect size statistics that can be readily & meaningfully done.\nPerhaps even more usefully, this handy Excel spreadsheet can convert between Cohen’s d, r, η2, odds ratios, and area under the curve.\nIn Chapter 7 of their book on meta-analysis, Borenstein et al. (2011) also cover well the conversions between measures. Finally, the effectsize package for R can both compute and convert between many effect size measures, including all those mentioned here.\n\n\nTable 6.4: Formulas to Convert Between Common Effect Size Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom ↓\nTo →\nCohen’s \\(d\\)\nHedges’ \\(g\\)10\nPearson’s \\(r\\)\n\\(\\eta^2\\)\n\\(f\\)\n\\(f^2\\)\n\\(\\phi\\), \\(V\\)\n(2×2 only)\nOR\n(logistic approx.)\n\n\n\n\n\\(d\\)\n–\n\\(g = d \\cdot \\left(1 - \\frac{3}{4N - 9}\\right)\\)\n\\(r = \\frac{d}{\\sqrt{d^2 + 4}}\\)\n\\(\\eta^2 = \\frac{d^2}{d^2 + 4}\\)\n\\(f = \\frac{d}{2}\\)\n\\(f^2 = \\frac{d^2}{4}\\)\n\\(\\phi = \\frac{d}{\\sqrt{d^2 + 4}}\\)\n\\(d = \\frac{\\ln(\\text{OR}) \\cdot \\sqrt{3}}{\\pi}\\)\n\n\n\\(g\\)\n\\(d = \\frac{g}{1 - \\frac{3}{4N - 9}}\\)\nas \\(d\\)\nas \\(d\\)\nas \\(d\\)\nas \\(d\\)\nas \\(d\\)\nas \\(d\\)\nas \\(d\\)\n\n\n\\(r\\)\n\\(d = \\frac{2r}{\\sqrt{1 - r^2}}\\)\nas \\(d\\)\n–\n\\(\\eta^2 = r^2\\)\n\\(f = \\frac{r}{\\sqrt{1 - r^2}}\\)\n\\(f^2 = \\frac{r^2}{1 - r^2}\\)\n\\(\\phi = r\\)\n–\n\n\n\\(\\eta^2\\)\n\\(d = \\sqrt{\\frac{4 \\eta^2}{1 - \\eta^2}}\\)\nas \\(d\\)\n\\(r = \\sqrt{\\eta^2}\\)\n–\n\\(f = \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\\)\n\\(f^2 = \\frac{\\eta^2}{1 - \\eta^2}\\)\n–\n–\n\n\n\\(f\\)\n\\(d = 2f\\)\nas \\(d\\)\n\\(r = \\frac{f}{\\sqrt{f^2 + 1}}\\)\n\\(\\eta^2 = \\frac{f^2}{1 + f^2}\\)\n–\n\\((f)^2 = f^2\\)\n–\n–\n\n\n\\(f^2\\)\n\\(d = 2\\sqrt{f^2}\\)\nas \\(d\\)\n\\(r = \\sqrt{\\frac{f^2}{1 + f^2}}\\)\n\\(\\eta^2 = \\frac{f^2}{1 + f^2}\\)\n\\(f = \\sqrt{f^2}\\)\n–\n–\n–\n\n\n\\(\\phi\\) or \\(V\\)\n\\(d = \\frac{2\\phi}{\\sqrt{1 - \\phi^2}}\\)\nas \\(d\\)\n\\(r = \\phi\\)\n\\(\\eta^2 = \\phi^2\\)\n–\n–\n–\n–\n\n\nOR\n\\(d = \\frac{\\ln(\\text{OR}) \\cdot \\sqrt{3}}{\\pi}\\)\nas \\(d\\)\n–\n–\n–\n–\n–\n–\n\n\n\n\n\n\nTable 6.5: Common Effect Size Statistics That Can Be Converted into Each Other\n\n\n\n\n\n\n\nThis Effect Size Statistic…\nCan Be Converted To…\nUnder These Conditions\n\n\n\n\nCohen’s d\ng, r, η2, f, f2, OR, φ\nAssumes continuous, normally distributed data; OR/φ require dichotomous approximation\n\n\nHedges’ g\nd\nHedges’ g is a modification of Cohen’s d for small sample sizes\n\n\nPearson’s r\nd, f, f2, η2\nAssumes linear relationship\n\n\nη2\nr, f, f2, d\nLimited to ANOVA models\n\n\nCohen’s f\nd, r, η2, f2\nIn ANOVA models\n\n\nCohen’s f²\nR^2, f, r, η2\nIn multiple regression contexts\n\n\nCohen’s w\nφ or V\nIn 2 \\(\\times\\) 2 tables\n\n\nCramér’s V\nφ, w\nOnly for 2 \\(\\times\\) 2; not convertible to d, f, etc.\n\n\nφ\nr, w, V, d (with assumptions)\nIn 2 \\(\\times\\) 2 tables; n.b., this is an approximate d conversion\n\n\nOdds Ratio (OR)\nd (approx.), log-OR\nApproximate only; assumes logistic distribution\n\n\nRisk Ratio (RR)\nd (approx.)\nApproximate only; assumes log-binomial model\n\n\n\n\n\n\nTable 6.6: Common Effect Size Statistics That Cannot Be Converted into Each Other\n\n\n\n\n\n\nPair\nWhy Not Convertible\n\n\n\n\nh \\(\\leftrightarrow\\) d, f, r\nh is based on arc-sine transformed proportions (i.e., a different metric)\n\n\nq \\(\\leftrightarrow\\) d, f, r\nq compares correlations (via Fisher’s z)\n\n\nV \\(\\leftrightarrow\\) f\nV is for categorical data (chi-square); f for continuous\n\n\nOR \\(\\leftrightarrow\\) r, f (directly)\nOnly approximate; depends on baseline prevalence\n\n\nRR \\(\\leftrightarrow\\) anything else (except OR)\nRR has no meaningful transformation outside risk models\n\n\n\n\n\n6.3.1 A Few Notes on Conversions\nIn addition to simply listing the formulas for possible conversions, there are a few more points to make—and a couple more conversions that are worth knowing. Below are further considerations about converting Cohen’s f (and f2) to Cohen’s d and about converting relevant effect size stats into the t-scores and F-scores used to test mean differences.\n\n\n6.3.2 Cohen’s f (and f2) to Cohen’s d\nCohen’s f2 (and f) measures the effect size of an entire model (usually an ANOVA). Cohen’s d measures the effect size between two levels of single variable11. So, in order to convert between f2 and d, we have to know more about the model. For a one-way ANOVA with two groups12, d = 2f = 2\\(\\sqrt{f^2}\\). In this particular case, then, f = \\(\\frac{d}{2}\\).\nMore generally, when there is only one term in the model:\n\\[f^2 = \\frac{d^2}{2k}\\]\nIt gets a bit more complicated when there are more than one terms in the model. This site covers some common situations.\n\n\n6.3.3 Cohen’s d and Student’s t\nThis is the t in t-test. The only additional piece of information we need to know to transform between Cohen’s d and Student’s t is the sample size, N:\n\\[t = d \\times \\sqrt{N}\\]\n\\[\\text{Cohen's }d = \\frac{t}{\\sqrt{N}}\\]\n\n\n\n\n\n\n\n\n\n6.3.4 η2 and F-scores\nThis F-test score that is used in ANOVA-family models. Like the relationship between d and t, the only additional things we need to know to compute η2 from F are degrees of freedom (which are closely related to sample size). Here, though, we have degrees of freedom in both the numerator (top) and denominator (bottom13):\n\\[\\eta^2 = \\frac{F \\times df_{Effect}}{F \\times (df_{Effect} + df_{Error})}\\]\nSo, η2 is dependent on the ratio of the dfs allotted to the given effect and the dfs allotted to it’s corresponding error term. Since we have the effect’s dfs in both the numerator and denominator, their effect will generally cancel out; this suggests that having more levels to a variable doesn’t appreciably affect the size of its effect. However, being able to allot more dfs to error does help us see the size of whatever effect is there. Larger samples won’t really change the size of the effects we’re measuring, but they can help us see ones that are there."
  },
  {
    "objectID": "effect_size.html#additional-resources",
    "href": "effect_size.html#additional-resources",
    "title": "6  Effect Size: Explanation and Guidelines",
    "section": "6.4 Additional Resources",
    "text": "6.4 Additional Resources\n\nPsychometrica offers a wonderful and pretty thorough list of effect size measures along with freeware apps to compute them at https://www.psychometrica.de/effect_size.html\nHojat, M. & Xu, G. (2004). A visitor’s guide to effect sizes: statistical significance versus practical (clinical) importance of research findings. Advances in Health Sciences Education: Theory and Practice, 9(3), 241–249. doi: 10.1023/B:AHSE.0000038173.00909.f6\nReichel, C. (2019). Statistics for journalists: Understanding what effect size means. The Journalist’s Resource.\nPsychometrica.de, this very useful site contains:\n\nEasy functions to compute every, commonly-used effect size measure\nConvert between d, r, f, OR, η2, and common language effect size statistics\nTable of “small,” “medium,” and “large” effects laid out and interpreted somewhat differently than I did here\nList of relevant sources\n\nFasterCapital’s Phi Coefficient: The Phi Coefficient and Yule s Q: Pioneers in Measuring Association provides a very readable and thorough coverage of those two statistics.\n\n\n\n\nCohen’s duck\n\n\n\n\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R. (2011). Introduction to Meta-Analysis (1. Aufl., p. xxix). Wiley. https://doi.org/10.1002/9780470743386\n\n\nChen, H., Cohen, P., & Chen, S. (2010). How big is a big odds ratio? Interpreting the magnitudes of odds ratios in epidemiological studies. Communications in Statistics - Simulation and Computation, 39(4), 860–864. https://doi.org/10.1080/03610911003650383\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nKraft, M. A. (2020). Interpreting effect sizes of education interventions. Educational Researcher, 49(4), 241–253. https://doi.org/10.3102/0013189X20912798\n\n\nOkada, K. (2013). Is omega squared less biased? A comparison of three major effect size indices in one-way ANOVA. Behaviormetrika, 40(2), 129–147. https://doi.org/10.2333/bhmk.40.129"
  },
  {
    "objectID": "effect_size.html#footnotes",
    "href": "effect_size.html#footnotes",
    "title": "6  Effect Size: Explanation and Guidelines",
    "section": "",
    "text": "In this case, it also would assume homoskedasticity. They also assume that samples are independently and identically distributed (“iid”), meaning that (a) the value of each data point in a given variable is independent from the value of all/any other data point for that variable and (b) each of those data points in that variable are drawn from the same distribution, e.g., they’re all drawn from a normal distribution.↩︎\nFor what it’s worth, we actually take the square root of the sum of the variances, and then divide that by 2, i.e.: \\(\\text{Pooled }SD = \\frac{\\sqrt{(SD^2_{\\text{First Mean}}+SD^2_{\\text{Second Mean}})}}{2}\\).↩︎\nIf you’re curious about how the three measures—η2; ω2; and the next one, ε2—are computed (from Maxwell, Camp, & Arvey, 1981, cited in Okada, 2013):\\[\\eta^2 = \\frac{SS_{b}}{SS_{t}}\\] \\[\\omega^2 = \\frac{SS_{b} - df_{b}MS_{w}}{SS_{t} + SS_w}\\] and \\[\\epsilon^2 = \\frac{SS_{b} - df_{b}MS_{w}}{SS_{t}}\\] where SSb is the sum of squares between groups, dfb is the degrees of freedom between groups, SSw is the sum of squares within each group, MSw is mean sum of squares between groups, and SSt is the total sum of squares (i.e., SSt = SSb + SSw).↩︎\nThese percents of variance accounted for are for zero-order correlations (i.e., correlations between two variables). The percent accounted for considered “small,” “medium,” and “large” for model R^2s are slightly higher (2%, 13%, and 26%, respectively).↩︎\nThe source for this—Husén, T. (1959). Psychological twin research: A methodological study. Stockholm: Almqvist & Wiksell—was too old for me to see if he means mono- or dizygotic twins. But I tried!↩︎\nSo, I guess a full higher education career does have a large effect on a person. And, yeah, Cohen does seem a little pre-occupied with IQ, doesn’t he?↩︎\nThis is also true for, e.g., risk ratios, hazard ratios, means ratios, and hierarchical models.↩︎\nCohen also only directly considered these criteria as they applied to experimental psychology—not, e.g., the health sciences. Indeed, he elsewhere notes that what experimental psychologists would call a “large” effect would be paltry in the physical sciences.↩︎\nWith, say, the target level of outcome denoting a “medium” effect. Reaching \\(\\frac{1}{3}\\) of that target could denote a “small” effect, and reaching \\(\\frac{2}{3}\\)s more (167%) a “large” one. (This corresponds to the range between many of Cohen’s criteria. For example, criteria for r are .1, .3, and .5.↩︎\nHedges’ \\(g\\) can be converted to any other effect size that Cohen’s \\(d\\) can be be converted. To convert to Hedges’ \\(g\\) instead of \\(d\\), multiply \\(d\\) in the given equation by \\(\\left(1 - \\frac{3}{4N - 9}\\right)\\).↩︎\nRemember, Cohen’s d is just the difference between two means that is then standardized.↩︎\nWhich is itself really just a t-test but using an ANOVA framework instead↩︎\nMy mnemonic to remember which is which is to think of the saying, “The lowest common denominator.”↩︎"
  },
  {
    "objectID": "missing_data.html#missing-data-more-than-just-a-smaller-sample",
    "href": "missing_data.html#missing-data-more-than-just-a-smaller-sample",
    "title": "7  Missing Data",
    "section": "7.1 Missing Data: More Than Just a Smaller Sample",
    "text": "7.1 Missing Data: More Than Just a Smaller Sample\n\n7.1.1 Sources of Attrition\nThe difference between MAR and MCAR is not trivial. Data may be missing for real and even evident reasons, but these reasons may not necessarily affect the conclusions we draw from them. In a review of attrition among new mothers participating in a 15-year study, Gustavson et al. (2012) found that education levels were lower amount those who dropped out, suggesting important sources of bias for generalizations of their results to other populations. However, those who dropped out did not differ in terms of the psychological and interpersonal/social factors central to their study. They also found that attrition did not affect the associations between those central variables. Their results suggest that even systematic biases in attrition may not significantly affect our conclusions—even if this is normally impossible to test and confirm.\nCiting Davis et al. (2002), Teague et al. (2018, p. 1) similarly noted noted that “[s]ystematic attrition in longitudinal research occurs most often in older, non-white male participants with limited education and/or multiple health problems. Long duration and repeated assessments can also increase attrition due to the significant burden on participants.” Nonetheless, it may be useful to consider more broadly the factors found to be associated with attrition. Gustavson (2012, p. 2) provides a nice review of such factors within field-based health research:\n\nSocio-demographic variables, such as low educational level, being out of work, and not being married, are typically related to increased risk of non-response and attrition in epidemiological studies [2,4,5,8-12]. In addition, unhealthy life style factors, such as smoking, high alcohol consumption, and physical inactivity, are related to non-participation and attrition [8,11-13].\n\n\nHigh levels of psychological distress can predict attrition in high-risk populations, such as psychiatric outpatients and former hospitalized patients [3,14]. In populationbased studies, psychological distress has been found to have no effect or a weak to moderate effect on attrition after adjusting for other variables [2,4,9,10]. Attrition may also be related to social factors, such as support from spouse or friends, and child’s characteristics. Poor relationship quality is an important predictor of mental health problems [15]. However, social networks and support did not predict attrition in a 15-year follow-up study [5], and marital satisfaction and spousal support did not predict attrition in a job satisfaction study [6]. More knowledge is needed about the association between attrition and psychological as well as social factors.\n\n\nStudies with high-risk populations found that externalizing problems and psychopathology in general among children were associated with a higher risk of parents dropping out [16,17], whereas child characteristics such as temperament, anxiety, and attention problems did not predict attrition in population-based studies [18,19]. It may be that the ways different factors affect attrition are dependent on whether the original sample was drawn from a high-risk population.\n\n\n\n7.1.2 Addressing Attrition\nTeague et al. (2018) conducted a meta-analysis of factors contributing to and reducing attrition in a rather wide range of longitudinal, field-based studies. They found that:\n\nafter controlling for study duration and number of waves, studies that utilised any barrier-reduction strategy had higher retention rates than those that did not use a barrier strategy (median retention using barrier strategies = 81.1%; median retention not using barrier strategies = 70.7%; b = 0.61, p = .01). Again after controlling for the study duration and number of waves, surprisingly, articles that reported use of at least one follow-up/reminder strategy had lower retention rates when compared to studies that did not utilise any follow-up/reminder (median retention using follow-up/reminder strategies = 76.4%; median retention not using follow-up/reminder strategies = 86.1%; b = −0.32, p &lt; .01). No relationships were found between retention rate and the use of any community-building or tracing retention strategies” (p. 11)\n\nFurther details about what did and did not affect retention are in this reproduced table:\n\n\n\nMeta-Analytic Regression Results Between Retention Strategy Themes and Retention Rate\n\n\n\nMissing data\n\nGood review of caveats with Little’s test in description\nhttps://www.sciencedirect.com/science/article/pii/S0895435618308710\nhttps://www.researchgate.net/deref/http%3A%2F%2Fwww.talkstats.com%2Fshowthread.php%2F17506-Multiple-Imputation-or-FIML\nhttps://www.researchgate.net/profile/Rafael-Garcia-26/post/What_proportion_of_missing_data_is_too_big_for_multiple_imputation_in_longitudinal_data/attachment/59d63c7179197b80779996d4/AS%3A415701081837568%401476122269218/download/GCA+APS+2007+-+Revision+2013+%281%29.pdf\nhttps://www.researchgate.net/deref/http%3A%2F%2Fwww.personal.psu.edu%2Fjxb14%2FM554%2Farticles%2FSchafer%2526Graham2002.pdf\nChapter 9 in Snijders, T. A. B., & Bosker, R. J. (2012). Multilevel Analysis: An introduction to basic and advanced multilevel modelling. London: Sage\n\nhttps://www.stats.ox.ac.uk/~snijders/mlbook.htm\n\n\n\n\n\n\n\n\n\n\n\n\nDavis, L. L., Broome, M. E., & Cox, R. P. (2002). Maximizing retention in community-based clinical trials. Journal of Nursing Scholarship, 34(1), 47–53. https://doi.org/10.1111/j.1547-5069.2002.00047.x\n\n\nGustavson, K., von Soest, T., Karevold, E., & Røysamb, E. (2012). Attrition and generalizability in longitudinal studies: Findings from a 15-year population-based study and a Monte Carlo simulation study. BMC Public Health, 12, 918. https://doi.org/10.1186/1471-2458-12-918\n\n\nTeague, S., Youssef, G. J., Macdonald, J. A., Sciberras, E., Shatte, A., Fuller-Tyszkiewicz, M., Greenwood, C., McIntosh, J., Olsson, C. A., & Hutchinson, D. (2018). Retention strategies in longitudinal cohort studies: A systematic review and meta-analysis. BMC Medical Research Methodology, 18(1), 151–151. https://doi.org/10.1186/s12874-018-0586-7"
  },
  {
    "objectID": "lrm_01.html#overview",
    "href": "lrm_01.html#overview",
    "title": "8  Linear Regression Modeling with SPSS, Part 1: Introduction",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nThis chapter covers the relationship between partial correlations and linear regressions before exploring and interpreting results of linear regressions conducted on the adolescent executive functioning data."
  },
  {
    "objectID": "lrm_01.html#core-concepts",
    "href": "lrm_01.html#core-concepts",
    "title": "8  Linear Regression Modeling with SPSS, Part 1: Introduction",
    "section": "8.2 Core Concepts",
    "text": "8.2 Core Concepts\n\n8.2.1 Linear Relationships\nVery few relationships in healthcare are truly linear. There are sweet spots in how much or what sorts of care to give, sometimes diminishing returns, sometimes growing returns. This is, in fact, true for much outside of the physical sciences; even the effect of intelligence on income & wealth appears to have a non-linear (diminshing) effect.\nAnd yet, it many cases it’s good enough to assume relationships are linear. It can account for much of the relationship while being easy to model statistically. Even if we suspect that the relationships between predictors and outcome are non-linear, we often first test those relationships against a model that assumes linearity because that may well still be sufficient. In addition, seeing how well the data are fit by a linear model lets us then subsequently see how much better a given non-linear relationship explains it: We can even quantify and test the significance of the improvement of a non-linear model over a linear one.\n\n\n8.2.2 Consider Removing the Intercept\nO.K., removing the intercept isn’t a core concept, but it can be a good idea nonetheless. One of the best predictors of the future is the past1, so simply knowing where participants are when they start participating is often among the most efficient and powerful ways of knowing where they will be later on in the study.\nBut wouldn’t it be nice to know what about their initial state matters most later one? Leaving the intercept in lets it “suck up” a lot of information that could otherwise be explained by other terms in your model. Removing the intercept may allow that information to “flow back” into other predictors to allow those other predictors to explain your outcome.\nRemoving the intercept also frees up the degree of freedom used to estimate its value. This gives our other analyses that much more power. No, that’s not usually a lot, but it does help us maximize the information in our data, making us that much more efficient and conscious of the real value of data."
  },
  {
    "objectID": "lrm_01.html#sec-intro_to_lrm",
    "href": "lrm_01.html#sec-intro_to_lrm",
    "title": "8  Linear Regression Modeling with SPSS, Part 1: Introduction",
    "section": "8.3 Introduction to Linear Regression Models",
    "text": "8.3 Introduction to Linear Regression Models\nI have tried to emphasize the similarity between correlations (including partial and semipartial) and linear regressions. I did this in part to help use your understanding of correlations as a stepping stone to understanding linear regressions and in part to explain interpreting terms in linear regressions. Let us explore that relationship between those two analyses now.\n\n8.3.1 Correlation vs. Simple Linear Regression\nFirst, let’s compare the results of a simple linear regression against the results of a zero-order correlation containing the same variables. A simple linear regression is a linear regression that contains only one predictor (like a one-way ANOVA).\n\nCorrelation\n\nChoose Analyze from the menu bar (from any window), and click on Correlate &gt; Bivariate2.\nSelect Participated in DBT [DBT] and Ball Executive Functions Slope – Student Self-Report [All_EFs_SR_Slope] to add to the Variables field.\n\nThis is a point-biserial correlation (i.e., a dichotomous variable correlated with a continuous), but the formula for that derives to be computationally the same as that for the Pearson, so select (or leave selected) that option under Correlation Coefficients.\nThe default α-value for rejecting the null in SPSS is .05. i.e., we are accepting a 5% chance that any given hypothesis test will be a Type 1 error—here that the correlation is equal to zero (and the Type I error being that we think it isn’t equal to zero when in fact it is). We don’t really have an a priori reason to believe that this correlation will be above or below zero, so we will divide that α = .05 into two pieces, letting us test if it is above zero 2.5% of the time and below zero 2.5% of the time. This is called a two-tailed test, so let’s leave the Test of Significance to Two-tailed.\nUnder the Options dialogue, we can include Means and standard deviations under Statistics. We also should leave the Missing Values option to Exclude cases pairwise. Pairwise exclusion in SPSS means that a given case (row) will be excluded from a given analysis if that row is missing data relevant to that analysis per se. Listwise exclusion in SPSS means that a row will be excluded if any data are missing for that case in any of the variables selected for that family of analyses—even if one values relevant to that particular analysis are both present. Listwise exclusion is nearly always too conservative a criterion, thus opening us up to biases in our analysis that come from biases introduced by variables that aren’t even in that analysis.\n\nIn the Descriptive Statistics the Output window, we see that mean for the DBT variable is .19; since this is a dummy variable (where 1 = participating and 0 = not participating), this is also the proportion of cases that participated in the program: 19% of the students here participated in the DBT program.\nIn the Correlations table, we see that the correlation between DBT and All_EFs_SR_Slope is -.165. This indicates that as we go from a DBT score of 0 to 1, the slope changes -.1653. The Sig. (2--tailed) row in the table indicates that the p-value for that correlation is .003, which is less than the α–level we established (α = .05 / 2 = .025 for each tail), making this correlation significant3. We could write this in a Results section as:\n\n\nThe point-biserial correlation between DBT participation and changes in all executive functions was significant (rpb = -.165, df = 326, p = .003), indicating that participating in the DBT program was associated with significantly more negative slopes (i.e., significantly greater improvements) in total executive function scores.\n\nWhich attempts to explain the relationship in simply terms that rely on little in-article jargon and acronyms. The support for this plainer-English description is supported (parenthetically) by the numerical statistics.\n\n\nLinear Regression\nRemember that in simple correlations, we assume that unique variance / error comes equally from both variables. We formalize this mathematically by having the variance they share—their covariance—divided by the unshared variance from both variables:\n\\[\\text{Correlation} = \\frac{\\text{Cov}{({X,Y})}}{\\text{Var}{(X)}\\text{Var}{(Y)}}\\]\nUnlike a correlation, in a linear regression, unique variance / error is assumed to come from only the predictor variable(s):\n\\[Y = bX + e\\]\nwhere \\(b\\) is the slope of the regression line that best fits the cluster of \\(X\\) values plotted against \\(Y\\) and \\(e\\) is the average distance4 of each individual \\(X\\) value from that line—the variance unique to \\(X\\) that is relegated to error:\n\nAlthough our assumptions are different, we are still doing the same basic function: determining a line of best fit computed by minimizing the unique variance in our data—here in the values of our criterion, \\(X\\)5.\nSeparating out error like is done in a linear regression will eventually allow us more flexibility in how we deal with error. For now, I simply want to show the similarity between a correlation and a linear regression. So, in SPSS:\n\nIn whatever window you have active, click Analyze &gt; Regression &gt; Linear....\nAdd ZAll_EFs_SR_Slope to the Dependent field and DBT to the Independent(s) field. SPSS is calling the outcome “Dependent” and the predictor(s) “Independent(s)”, as in DVs and IVs.\nUnder Statistics, have Estimates and Confidence intervals selected under Regression Coefficients (the latter since confidence intervals are slowly replacing up-and-down significance tests), and Model fit, Part and partial correlations, and—might as well—Descriptives also selected. For now, leave R squared change unselected.\n(Casewise diagnostics lets you evaluate whether there are extreme outliers in the data that may be skewing the results. Durbin-Watson tests whether there is nonignorable autocorrelation between the errors of longitudinal data, with a value of “2” indicating ignorable autocorrelation and values approaching either 0 or 4 indicating that error values are not independent of each other and thus that, e.g., one should consider the nested nature of the data, q.v. Chapter 10.)\nUnder Options, select Exclude cases pairwise under Missing values for the reasons discussed above; Replace with mean is a defensible strategy for handling missing data, although multiple imputation would be preferred in all ways … were it easy performed in SPSS.\nWe will not be use any Stepping Method Criteria, so the default (or really any values since this doesn’t apply) are fine.\nFinally, leave Include constant in equation selected. The constant of which they speak is the intercept since not all of our variables are standardized.\nWe will ignore, e.g., the Method of entering or removing terms from the model for now.\n\nFor Zscore: All Executive Functions Slope -- Student Self-Report in the Descriptive Statistics table in the Output window, we see that the Mean is .00000000 and the Std Deviation is 1.00000000, as they should be since that variable is indeed standardized here6.\nFor Participation in DBT Program?, the mean is .19 (sample size is 670); since this is a dummy variable, this means that 19% of the cases had 1s, i.e., that 19% of the students participated. The results for the Correlation also return the correlation (-.165) and that it is significant.\nAfter the Variables Entered/Removed table (which is not relevant now), the results present a Model Summary table. This table presents statistics about how well the model overall performs when trying to fit the data we fed it. Remember how one way of thinking about linear regression models is that they try to minimize unexplained variance in the data—that they try to account for as much of the variance / information in the data as possible. The R and R Square values in this table do just that; they describe how much variance in the data set are accounted for by this particular model (containing—for now—simply whether the student participated in the DBT program). We see from this table that the R-value is .165. This, of course, is the same absolute value as the correlation between DBT and All_EFs_SR_Slope. A linear regression will generate the same (or nearly same) values as a correlation on those same variables—the difference, though, is in the assumptions we’re making about the data: Correlations assume error is shared equally whereas linear regressions separate out error and explicitly model it s a term among the predictors.\nThe R Square value in that table is just the R value squared. Squaring a correlation coefficient (i.e., computing r2 from r) computes the shared variance between the two variables. Similarly, R Square (i.e., R2) computes the variance within that data set that is accounted for by this model. Capital R2 is used to denote the variance in the data accounted for by all of the model terms—intercept, main effects, interactions, etc., but not error. Lower-case r2 is used to denote the variance shared by just two variables—not the whole model. (The Adjusted R Square reduces the value a bit for each term added to the model since one can improve the R2 of a model even by adding non-significant or uninteresting terms to it.)\nThe next table, ANOVA, presents the results of the linear regression in terms of just that. This presents the familiar Sum of Squares for the DBT term (when DBT = 1, as it shows in the left-most column) as well as the Residual (Sum of Squares) which you should now recognize is the unexplained variance. The F-score and p-value (Sig.) both indicate the significance of the DBT term.\nOne more thing to note about the ANOVA table … is that there even is one: The presence here of an ANOVA table—when we didn’t explicitly tell SPSS to run an ANOVA—underlines the fact that what we’re doing in a linear regression is the same as we would do in an ANOVA. Again, one reason to prefer a linear regression over an ANOVA is because of the greater flexibility of a linear regression. Of course, if you don’t need this greater flexibility, then this also means that running an ANOVA is just fine if that’s all you need; in addition, ANOVA source tables may also be more accessible to audiences without quite as much sophisticated understanding of statistics as you now have.\nThe results of the ANOVA, correlation, and linear regression analyses are all quite similar. Indeed—to the extent that our underlying assumptions hold—the results of all three analyses will become even more similar as the sample size increases. Two things to infer from this that are most relevant here are:\n\nAssumptions matter. Although some assumptions (monotonicity of the data and that data are independently and identically distributed) tend to be more important than others (strict adherence to normality), knowing how well and in what ways our data meet our basic assumptions affect all analyses we do and all inferences we make from them. This remains true for data of all sizes—even if some assumptions become more important as sample size increases and others tend to become less important (e.g., sampling bias becomes more important; adherence to normality even less).\n(Nearly all) linear regressions do the same thing. The fundamental goal of a correlation, ANOVA, multilevel model, logistic regression, and structural equation model are the same. They all test a linear relationship between the variables by computing a slope, determining that slope is determined by computing a loss-limiting functioning (e.g., ordinary least squares or maximum likelihood), and parceling out variance into that which is explained by the model and that which remains unexplained (“error”).\nIn fact, a main goal of demonstrating the relationship between, e.g., a one-way ANOVA and a zero-order correlation is to show that they can be seen as members of the same family of analyses.\n\n\n\nSemipartial Correlation vs. Multivariate Linear Regression\nRemember (e.g., from Section 5.3) that a semipartial correlation removes the effect of a third variable7 from one of the two variables in a correlation. Remember too that as odd as this may seem a thing to do, in fact it’s done all the time: It’s the basis for having two (or more) predictors in a linear model; the effects of each predictor are (mathematically) isolated from each other so that the effect of one is independent of the effect of the other.\nIf the two predictors are strongly correlated with each other, then the model will perform quite differently if only one is included versus if both are. Exactly in what way it will “act differently” is hard to anticipate ahead of time, but act differently it will. Let’s look at examples of that now.\n\n\n\n8.3.2 Conducting a Multivariate Linear Regression Using Forward Term Selection\nA multivariate linear regression is just a linear regression that has two or more predictors.\n\nIn SPSS, compute the correlation between ZAll_EFs_SR_Slope and Adult_Sister_at_Home (a dummy variable that indicates whether the teens if they lived with a sister who was over 18 years old). You’ll see that rbp = -.11 (df = 319, p = .048). This correlation is small but significant (and, yes, cherry-picked for this example).\nLook, too, at the correlations between DBT and both ZAll_EFs_SR_Slope and Adult_Sister_at_Home. The zero-order correlation between DBT and ZAll_EFs_SR_Slope is -.165 and between DBT and Adult_Sister_at_Home is .04. Of course, this correlation between DBT and Adult_Sister_at_Home is not theoretically interesting since there is no reason to believe that participating in the DBT program really affects how many adult sisters one has or vice versa; nonetheless, it serves well as an example of how linear models change when correlated predictors are both included.\nNow, let us rerun the linear regression predicting ZAll_EFs_SR_Slope with DBT, but this time also add in Adult_Sister_at_Home. I.e., go to Analyze &gt; Regression &gt; Linear..., put ZAll_EFs_SR_Slope in the Dependent field, and put both DBT and Adult_Sister_at_Home in the Independent(s) field.\nUnder Statistics..., make sure Estimates, Model fit, R squared change, and Collinearity diagnostics are all selected. These play into what we will be doing now.\nLeave everything under Options the same, viz., leave the Stepping Method Criteria to the default, keep Include constant in equation selected, and Exclude cases pairwise. Most of these inform our investigation here, too.\nNow, turn your attention to the Method: drop-down menu right under the Independent(s) field. This is the method SPSS will use to add or remove terms to the model. I’ll explain this further soon, but for right now, select Forward Selection.\nHit OK.\n\n\nResults\n\nVariables Entered/Removed and the “Stepwise” Strategy\n The Variables Entered/Removed table reports which variables are either entered or removed based on the Method: we selected to determine which variables ought to be selected for our final model. Let me first explain what is being done here and why before we explore more particularly the methods used.\nSecond, we could think of the whole model and whether a predictor makes a significant contribution to the overall fit of the model. This latter method will (usually) produce the same results as the former, but has the advantages of both allowing us to test significance in more ways and of allowing us to test and consider contributions more flexibly and precisely.\nWhat SPSS is doing here is based on that second approach. It is trying to build the best model, picking from those we suggested to find the combination that has the largest number of significant terms8. Here, the only possible predictors SPSS could choose from are just DBT and Adult_Sister_at_Home. However, when exploring larger sets of data, there may be many more potential variables to add.\nThe general strategy SPSS presumes we are following is to choose which variable(s) to add or remove from a model based on whether that variable significantly changes the fit of the overall model. One advantage of this strategy is that is considers whether predictors themselves are inter-correlated. If two predictors are strongly correlated, then it’s unlikely that both will be added to the model; instead, the one that is more strongly predictive of the criterion will be added and the other one won’t make the cut since most of its relationship with the criterion will be accounted for by the other predictor that made the cut first. This tends to create a more parsimonious model that is less affected by multicollinearity and yet is still effective.\nSPSS, in fact, offers five methods for deciding which predictors to add or remove9 given this general strategy:\n\nEnter: The most theory-driven of the methods, Enter lets one add “block” or set of predictors and then add another block, continuing as one wants. In this way, one could, e.g., first add in all of the variables that are not of direct but that one expects will be important to control for when finally looking at the (other) predictors that are of theoretical interest; in other words, one could create a base model with the first block, and then in the second block start adding predictors of theoretical interest to see if those theoretically-interesting predictors prove to be important after all. This is the method I use nearly exclusively.\nRemove: A similar strategy to Enter, SPSS first starts with all chosen predictors added to the model. It then removes those listed in the first block, then those listed in the second block, and continues until there are no more predictors (except the intercept, if present). Enter is used to test whether adding a block of predicts improves the model; Remove is used to test whether removing a block of predictors worsened the model. It’s a subtle distinction, and the choice of which to use is one determines based on one’s theory and research questions.\nStepwise: A method that both adds and removes predictors. SPSS does this in “steps.” In the first step, SPSS starts with no predictors in the model (except the intercept); it then tries out each predictor, seeing which of them would be most significant (has the smallest p-value10); if that predictor with the smallest p-value is also sufficiently significant11, then it is indeed added to the model in that first step.\nIn the second step, SPSS first tries out all of the variables still not in the model, chooses the one with the smallest p-value, and adds that to the model if its p-value is smaller than the pre-set cut-off. Then, still in the second step, SPSS goes through the predictors that have been added to the model; if any of them have become sufficiently non-significant, SPSS removes them from the model. If no predictors were added or removed during the second step (if none met the criteria for entry or removal), then SPSS stops and reports this as the final model.\nThe third and any subsequent steps follow the same procedure as the second step, explained just above. Note that it is possible (unlikely, but possible) that a variable that was previously removed is later re-entered as SPSS fiddles around finding the best set of predictors.\nForward: This method is like Stepwise, but in which predictors are only added (if they meet the pre-set cut-off) at each step; none are removed. (This is the method we used here because it’s simple and worked for what I wanted to show.)\nBackward: This method is also like Stepwise; here SPSS starts with all chosen predictors in the model, and then only removes any at each step, stopping either when no more meet the pre-set cut-off or when there are no more predictors left in it.\n\n\n\nModel Summary\n\nThis is the meat of the output. This table shows the performance of the overall model—not the individual predictors—at each step. The model statistics for the first step are given in the row labeled 1 in the Model column. Notice that the R-value for the first model (which contains only DBT) is the same as the zero-order correlation between DBT and ZAll_EFs_SR_Slope, again showing the similarity of the processes (again, as long as the same assumptions hold).\nRemember that squaring a correlation produces the proportion of variance explained. Similarly the model R2 (here R Square) indicates the proportion of variance in the criterion explained by this model. The Adjusted R Square is the model R2 adjusted for the number of terms in the model (predictors as well as intercept—if present—and error); as you can infer from the values here, it reduces the R2 ; this helps protect against inflated the model R2 simply adding a bunch of terms that have very little—if any—relationship with the criterion.\nAdjusted R2 also adjusted for the sample size; larger sample sizes are adjusted less since it is argued that they better represent the overall population. Given these adjustments, adjusted R2 values are best used as descriptive statistics when reporting how much, e.g., your model’s results and performance may apply to instances outside of your study—when making recommendations to the field, for example. However, adjusted R2 does not serve well for comparing between models within your analysis, as we will soon do.\nThe Std. Error of the Estimate (standard error of the estimate, also called the root mean square error) is the standard deviation of the error term in the model; this simply shows how much residual error (variance) there is in the model. Our R2 value is small—only accounting for 2.7% of the variance in EF change scores—so it’s not surprising that there is a lot of residual error; this column reminds us that that is so.\nThe R Square Change is how much change there is in the R Square value. For the first model, you’ll see that the R Square Change value is the same as the R Square value. Personally, I find this confusing since it’s not really a change in R2, but simply the initial R2 value. The F Change, dfs, and Sig. F Change for this first row are also simply the significance tests for this first model—not the change in the model. They do show that the model is significant (p = .003).\nThe second row (where Model is 2) indeed shows the change in model fit made by adding Adult_Sister_at_Home to the model that already contains DBT. Now, the R, R Square, Adjusted R Square, and Std. Error of the Estimate values are for the whole model (showing that the model with both predictors does account for more of the variance in ZAll_EFs_SR_Slope than the one only containing DBT—even when adjusting for the fact there are simple more predictors there), but the other columns are all analyzing the change in the R2 value between the first and second models. Although the change in R2 is small (.041 – .027 = .014), it is significant (F1, 316 = 4.51, p = .034); adding the Adult_Sister_at_Home term improved our model—and thus our understanding of EF changes; DBT and Adult_Sister_at_Home both make significant contributions to our understanding of changes in student-reported executive functioning, and these predictors—although mildly correlated (rpb = .04)—make otherwise unique contributions to predictions of EF changes.\nIt’s worth reiterating how testing for relationships in this way allows for more precise and nuanced insights into the relationships between our variables. We still test whether both predictors are significant, but do so through how much they contribute to our overall understanding: It’s not just if a term is significant, but how much it matters in light of everything else we know.\n\n\nANOVA Table \n\nFinally something familiar. The ANOVA table presents an ANOVA run on each of the models. This table doesn’t, however, show the tests of each of the terms in the model—just the overall model As such, this table really does little more than what was shown by the F-scores in the Model Summary table, just above.\n\n\nCoefficients\n\nThe Coefficients table also takes us back to more familiar ground, testing the effects of the predictor terms per se. Given how we’ve coded our variables, this table is a bit more confusing than it would otherwise be, though: Both DBT and Adult_Sister_at_Home are dichotomous variables, so the Unstandardized Coefficients don’t give us the insights that variables that have meaningful units would give; the main insights from the Unstandardized Coefficients is that the intercepts in both models are not significant, meaning that the sixth-grade EF scores for students were not different between those who participated or didn’t participate in the DBT program (t = 1.28, p = .20), even when accounting for whether they had adult sister(s) at home (t = -0.25, p = .80).\nNote, though, that the \\(beta\\)-weight for the DBT term in the first model is the zero-order correlation between it and ZAll_EFs_SR_Slope. As the column heading makes clear, \\(beta\\)-weights are the standardized regression weights, thus here the semipartial correlation (semipartialing out the intercept). In the second model, the DBT \\(beta\\)-weight is -.169; this is the semipartial correlation between DBT and ZAll_EFs_SR_Slope; semipartialing out slightly improves—clarifies—the relationship between DBT participation and EF changes (as we knew from the R2 change tests, above). The zero-order correlation between Adult_Sister_at_Home and ZAll_EFs_SR_Slope is -.11 while the semipartial correlation between them is .117—stronger and in the opposite direction; the adult sister thing is hard to explain or interpret here, but what we can say is that its relationship with EF changes is certainly mediated by DBT participation, underlining the importance of considering other variables in one’s analyses.\nThe Collinearity Statistics columns report two common tests of (multi)collinearity between predictors in the model:\n\nTolerance ranges from 0 to 1, with numbers closer to zero indicating that that variable is stronger related to other predictors in the model. By convention (more than reason), tolerances of less than .1 are seen as problematic and should be addressed, e.g., by removing predictors from the model or explaining why there is such high multicollinearity.\nThe variance inflation factor (VIF) measure the effect of collinearity on the model. VIFs range from 1 to infinity, and values greater than 10 are typically seen as indicating that collinearity is unignorably affecting the performance of the model. This affect is usually to make the model terms unstable, meaning we can’t speak confidently about not only the absolute values of the b- or \\(beta\\)-weights but that we can’t even be sure of significance tests on them.\n\nThere is only one predictor in the first model, so the Collinearity Statistics expectedly show there is no collinearity. In the second model, both statistics are remain very good, indicating that the contributions of DBT and Adult_Sister_at_Home are largely independent of each other; the weak correlation between those two variables (φ = .04) makes us expect that any collinearity would be quite small.\n\n\nExcluded Variables\n\nThe Excluded Variables table reports what the statistics would have been for each term if they had indeed been added to it. We only have two predictors and the second predictor was all that was added to the second model, so there is no new information gained here fro this table. Has we been conducting a more data-driven investigation into a larger set of variables, this table could be used to look at how the model would have performed under different combinations of predictors, even if SPSS hasn’t selected to include them.\n\n\nCollinearity Diagnostics\n\nWe had told SPSS to include collinearity diagnostics above, so this table also doesn’t present much new, but it does give some more information about what we know already. The Eigenvalue column can be used to ascertain where most of the collinearity in a model resides; this is especially useful if there are more than two variables that share unignorable levels of multicollinearity. In investigating multicollinearity, Eigenvalues greater than 15 are generally seen as important; Eigenvalues less than 1 are always ignorable. The Condition Index essentially measures the cumulative effect of the various sources of multicollinearity; values greater than 15 for condition indices are seen as problematic."
  },
  {
    "objectID": "lrm_01.html#multivariate-linear-regression-with-three-predictors-using-enter-term-selection",
    "href": "lrm_01.html#multivariate-linear-regression-with-three-predictors-using-enter-term-selection",
    "title": "8  Linear Regression Modeling with SPSS, Part 1: Introduction",
    "section": "8.4 Multivariate Linear Regression with Three Predictors Using Enter Term Selection",
    "text": "8.4 Multivariate Linear Regression with Three Predictors Using Enter Term Selection\nNow that we (hopefully!) have some understanding of multivariate linear regression, we can look at a slightly more complex one. Here, we are considering three variables, two of which we know are themselves mildly but significantly interrelated: DBT participation and economic distress (φ = -.08, p = 0.48). We’ll also consider adult sisters at home to build upon what we did above.\nWe will also move to what I believe is a more generally-defensible method of building and comparing models. Presuming we are interested in testing if DBT participation affects changes in students’ executive functioning, we are probably interested not only if DBT participation matters, but if it matters more than other, theoretically-uninteresting factors—like how many adult sisters one lives with. Controlling for economic distress also removes an important source of variance we couldn’t control through experimental design, and so is worth including here for that reason12. Therefore, we will add all (both) of the theoretically-uninteresting predictors to the first model. Although we can (and should) investigate the statistics of this first model, it is primarily intended to serve simply as the null model—the baseline of comparison—for testing the effect of DBT participation. Looked at this way, we see if DBT participation predicts changes in EF—while controlling for known sources of variance that could otherwise mislead our interpretation of the effects of the DBT program.\n\nTo conduct our analyses, again evoke the main linear regression dialogue box, e.g., via Analyze &gt; Regression &gt; Linear...\nAgain let ZAll_EFs_SR_Slope serve as the Dependent(s), but this time first add Economic_Distress and Adult_Sister_at_Home to the Independent(s) field.\nChange the Method: to Enter.\nClick on the Next button just above the Independent(s) field. This allows us to now enter other predictors into what will be the second block of predictors. Add DBT to the now-blank Independent(s) field.\nEnsure that in the Statistics dialogue box Model fit, R squared change, and Collinearity diagnostics are selected as is Estimates under Regression Coefficients.\nIn the Options dialogue box, makes sure Include constant in equation and Exclude cases pairwise are also selected.\nHit OK.\n\n\n8.4.1 Results\n\nVariables Entered / Removed\n\nThe Variables Entered/Removed table summarizes our steps and that we used the Enter method allowing us—not the data—to decide which predictors to add and when.\n\n\nModel Summary\n\nGiven our goals here, this is the most telling part of the output. The base model in step one in fact predict EF changes well (for field-based data). More interesting, though, is that adding DBT to this already-significant model further improves it, increasing the proportion of variance explained by 2.4% (R Square Change = .024), which is a significant contribution here (F1, 315 = 8.16, p = .005).\n\n\nANOVA\n\nThe ANOVA table simply reinforces what the Model Summary table contains, showing, e.g., not only that adding DBT made a significant improvement in model fit, but that the model containing all three predictors (Model 2) significantly predicted ZAll_EFs_SR_Slope scores. (F3, 315 = 7.01, p &lt; .001).\n\n\nCoefficients\n\nPerhaps the most interesting thing to note from the Coefficients table for this family of analyses is that the DBT term’s Beta-weight is lower when including Economic_Distress (and Adult_Sister_at_Home), reflecting that the shared variance between DBT and Economic_Distress is also shared by ZAll_EFs_SR_Slope: Some of the effect of the DBT program is mediated by the adolescents’ levels of economic distress.\nThe small levels of collinearity between DBT and Economic_Distress suggests that the variance they share is nearly entirely itself associated with ZAll_EFs_SR_Slope—that very little of their shared variance is left to create collinearity between them.\n\n\nExcluded Variables\n\nSince we chose which predictors to add and when—and since there were only two steps—the Excluded Variables table is again uninteresting.\n\n\nCollinearity Diagnostics\n\nThe multicollinearity between the predictors is greater in this family of models than it was in the previous family. Nonetheless, it is negligible."
  },
  {
    "objectID": "lrm_01.html#footnotes",
    "href": "lrm_01.html#footnotes",
    "title": "8  Linear Regression Modeling with SPSS, Part 1: Introduction",
    "section": "",
    "text": "An interesting take on this—albeit one that’s tangential to what we’re talking about—is given in this quote from Funk’s (2023) New York Times article:\n“A world in which computers accurately collect and remember and increasingly make decisions based on every little thing you have ever done is a world in which your past is ever more determinant of your future. It’s a world tailored to who you have been and not who you plan to be, one that could perpetuate the lopsided structures we have, not promote those we want. It’s a world in which lenders and insurers charge you more if you’re poor or Black and less if you’re rich or white, and one in which advertisers and political campaigners know exactly how to press your buttons by serving ads meant just for you. It’s a more perfect feedback loop, a lifelong echo chamber, a life-size version of the Facebook News Feed. And insofar as it cripples social mobility because you’re stuck in your own pattern, it could further hasten the end of the American dream. What may be scariest is not when the machines are wrong about you — but when they’re right.”↩︎\nNote that in SPSS the “A” in Analyze is underlined, that the “C” in Correlate is underlined, and the “B” in Bivariate is underlined. You will notice that all menu options have one letter underlined; once you are let enough to use keyboard instead of the mouse, this is the key you will type to select that option. So, to access this analysis, you could simply hold down the Alt key and type A then C then B instead of using the mouse.↩︎\nThe reverse interpretation—that a 1–point decrease in slope makes it 16.% more likely that that student was a member of the DBT group—is plausible for a correlation, but doesn’t really make sense practically since whether a child participated depended only on the year they were admitted to the school.↩︎\nIn ordinary least squares, it’s the square root of the mean squared distance—just as the standard deviation is square root of the deviance, which is itself the sum of squared distances from the mean.↩︎\nBy minimizing the variance relegated to error, we are trying to minimize the amount of information in the data that is lost—unexplained—by the model. The better that a model is at explaining the variance—the information—in the data, then the less information is lost to error.↩︎\nNote that the mean and standard deviation for standardized variables won’t always be reported by stats software as always equal to exactly 9 and 1, respectively. Sometime there is rounding error or only a subset of the standardized values are being used to re-calculate the mean & standard deviation.↩︎\nOr the effect of both a third and fourth variable, etc.↩︎\nThere are other ways to go about doing this than what SPSS is doing here—ways I prefer—but what we’re doing here is easy and still useful strategy to know and use.↩︎\nMore about these methods can be found, e.g., in IBM’s Knowledge Center.↩︎\nInstead of choosing the predictor with the smallest p-value, SPSS can choose the value with the largest F-value. A small distinction, but worth a footnote.↩︎\nThe criterion used to decide whether the p-value is sufficiently significant is determined by the values used under Options... &gt; Stepping Method Criteria. The default is to include a predictor if the p-value is less than .05 and to exclude a predictor if its p-value is greater than .10.↩︎\nThere are really three, general ways to address noise in one’s studies: group assignment (e.g., experimental vs. control), randomization, and what we’re doing here: adding possibly-confounding variables to a model to isolate their effect on the variables of interest. What we’re doing here tends to get short shrift when discussing experimental design, and—in my opinion—that’s too bad since we can’t always create the groups we want and randomization doesn’t always work and isn’t always easy to tell if it did.↩︎"
  },
  {
    "objectID": "lrm_02.html#overview",
    "href": "lrm_02.html#overview",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "9.1 Overview",
    "text": "9.1 Overview\nThis chapter seeks to further demonstrate how to two correlated predictors can be handled with both an ANOVA and—more generally—with a linear regression model. It also presents more details about conducting an ANOVA and about interpreting dummy variables."
  },
  {
    "objectID": "lrm_02.html#data",
    "href": "lrm_02.html#data",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "9.2 Data",
    "text": "9.2 Data\nWe will use the EF_Slope_Data.sav dataset for these additional analyses, focusing on a different set of variables within that data set. We’ll now be looking at the effects of both gender and special education status on English / language arts (ELA) grades.\nPlease download this data file again from BlackBoard for use here. These are “synthetic” data, based on real data but changed to further help ensure the participants’ confidentiality. In addition to making it more secure, I have manipulated the data to make the relationships between gender, special education status, and ELA grades stronger for instruction here."
  },
  {
    "objectID": "lrm_02.html#relationships-between-dummy-variables-crosstabs-and-χ2-tests",
    "href": "lrm_02.html#relationships-between-dummy-variables-crosstabs-and-χ2-tests",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "9.3 Relationships Between Dummy Variables: Crosstabs and \\(χ\\)2 Tests ",
    "text": "9.3 Relationships Between Dummy Variables: Crosstabs and \\(χ\\)2 Tests \nLet us first look at the relationship between gender and special education status.\nBoth gender and special education status are dummy variables. Gender is set here to indicate whether a student is female, so 0 = not female1 and 1 = female. Special education status here indicates whether a student has an individualized education program (IEP), so 0 = no IEP and 1 = has an IEP.\nWe could compute a correlation between these two dummy variables (correlations per se are just descriptive), but more information and a more accurate representation of the variable is obtained by looking at a frequency table showing, e.g., how may females have or don’t have an IEP.\n\nIn SPSS, go to Analyze &gt; Descriptive Statistics &gt; Crosstabs\nPlace Gender in the Row(s) field and Spec_Ed in the Column(s) field. Since they are both nominal, SPSS knows to populate the table with frequency counts.\nIn Statistics... ensure the Chi-square is selected; none of the other options pertain here, so click Continue\nUnder Cells..., in the Counts area, make sure that Observed is selected. The rest of the options in the Cells... dialogue can be interesting, but are pretty straight forward aren’t needed here.\n(Among the other options, I would normally also select Expected under Counts to see for myself how different the actual (observed) counts are from the expected: I can categorically say that it’s always a good idea to see the data for yourself and not just rely on a test to tell you what’s matters. However, I want to keep the output a bit clean to facilitate interpretation.)\nUnder the list of variables to choose from, select Display clustered bar charts and make sure Suppress tables is not selected.\nClick OK\nIn the Output, let us first look at the Bar Chart. The chart shows exact values, so there is no need for confidence intervals.\n\nNote that we could prettify this chart if we wanted to want to publish it, e.g., by creating better titles, including for the axes.\nThe output starts with a summary of complete and missing data, including the Gender * Spec_Ed Crosstabulation (“crosstab”)\nThe Chi-Square Tests table contains the following:\n\nPearson Chi-Square2 is an uncorrected test for whether the counts in the cells differ from “expected” values. “Expected” here means that the proportions are the same, viz., that the proportion of students with IEPs is the same among the boys as it is among the girls. So, there may be more boys than girls, but the percent of boys with IEPs is not discernibly different than the percent of girls with IEPs.xxx\nBoth of these variables have two levels (male/female, has / doesn’t have an IEP), so comparisons of counts between them craetes a 2 \\(\\times\\) 2 table. To compute the degrees of freedom for this test, we subtraddt 1 from the number of levels for each variable. We then multiply those values, or: \\[(2\\ df_{Gender} - 1) \\times (2\\ df_{IEP\\ Status} - 1) = 1 \\times 1 = 1\\] We are therefore testing against a \\(\\chi\\)2 distibution of 1 df. The mean and standardi deviation of a \\(\\chi\\)2 is determined by the degrees of freedom; more specifically, the mean of a \\(\\chi\\)2 is the df and the standard deviation is 2 \\(\\times\\) df. So, We are testing this Pearson Chi-Suare values against a null \\(\\chi\\)2 with a mean of 1 and a SD of 2. Remember that values greater than about 2 SDs away from a mean3 are usually considered significant. Since the mean for this null \\(\\chi\\)2 is 1 and the SD is 2, two SDs away would be 4 points away from the mean of 1. Therefore, any \\(\\chi\\)2 value that is greater than 5 would be considered significant.\nThe \\(χ\\)2 value here is 7.06, which is indeed larger than the critical value for a \\(\\chi\\)2 for 1 df; the counts are significantly different. We could report this by saying, e.g., “The proportion of students with IEPs was significantly different among those who identified as female than among those who did not (Pearson \\(χ\\)2 = 7.06, p &gt; .001)” or perhaps more simply: “The proportion of students with IEPs differed between the genders.”\nLooking at the bar chart we generated shows us more clearly what this difference is: Fewer girls have been diagnosed with disabilities warranting IEPs than boys, and we could certainly report that instead. The Pearson \\(χ\\)2 (and other \\(χ\\)2 tests here) are inherently non-directional4, but we can use that figure (or the table) to argue what this difference is.\nHaving now seen how the proportion of IEPs differed, we could instead describe this and the \\(\\chi\\)2 test as, e.g., “A larger proportion of males students had IEPs than did female students (Pearson \\(χ\\)2 = 7.06, p &gt; .001).”\nContinuity Correction reports the Yates’ correction. This is only presented in SPSS 2 \\(\\times\\) 2 tables (and is only appropriate for such tables), like we have here. The Pearson \\(χ\\)2 test tends to be biased “upwards” meaning it is overly optimistic and generates Type 1 (false positive) errors. Yates’ correction attempts to adjust for this by making the test more conservative. You’ll see here that the Value (the \\(χ\\)2) under Continuity Correction is slightly smaller than that under Pearson Chi-Square. Both of these tests are appropriate when all cell counts are greater than 10 (some suggest greater than 5), and the Yates’ correction is generally advised.\nThe Linear-by-Linear test is the Mantel-Haenszel test, which is useful when one wants to look at the association between two nominal variables while controlling for the effect of a third variable—akin to partialing out that third variable. We’re not doing that here, though, so this statistic isn’t interesting.\nThe Likelihood Ratio test, also called the G-test, uses odds ratios to determine the likelihood that the given frequencies in the cells occur by chance. As you can see, this computes a very similar value to the unadjusted (Pearson) \\(χ\\)2 value. We won’t consider this any more here, but will revisit likelihood ratios when we look at tests of whole linear models.\nFisher's Exact Test does not have a statistic like a \\(χ\\)2 that is computed; it is simply a probability test of those frequencies themselves. Here as with the \\(χ\\)2 tests, a p &lt; .05 (or whatever one sets for significance) indicates a significant difference in the actual cell counts from the expected.\n\n\nWe have seen that whether a student has an IEP depends in part on the student’s gender. In other words, gender and IEP status are related; they share variance. Therefore, when I talk, e.g., about IEPs, I know I should also consider gender.\n\n9.3.1 Relationships with ELA Grades\nLet us now investigate whether gender and IEP status are related to students’ grades.\n\nGo to Analyze &gt; Correlate &gt; Bivariate... and add Gender and ELA_Grade to the Variables field. We’ll be computing point biserial correlations (nominal vs. continuous) which are computationally equivalent to Pearson’s correlations, so leave that option selected under Correlation Coefficients.\nThe resultant output shows that rpb = .103 (n = 204, p = .144). These variables are not significantly correlated here, and gender accounts for ~1% (.1032 = .0106 \\(\\approx\\) .01) of the variance in ELA grades, what Cohen (1988) would call a “small” effect (q.v., Chapter 6).\nLooking now at IEP status, let’s remove Gender from the Variables field in Analyze &gt; Correlate &gt; Bivariate..., leave in ELA_Grade, and add Spec_Ed.\nThe correlation is even stronger, rpb = -.36. IEP status accounts for about 10% (-.362 = .13) of the variance in one’s grade. However, from our crosstabs work above, we know that the variance in IEP status is itself related to a student’s gender. In other words, some of that .13 variance is due to gender.\nSo, we know that some of the variance in IEP status is due to gender. We also know that some of the variance in ELA grades is also due to gender. We don’t yet know, however, if the effect of gender on IEP status is from the same aspects of gender as the effect of gender on grades. The shared variances between these three variables could kind of be like this:\n\nOr perhaps more like this5:\n\nA little less abstractly, one reason why boys tend to be diagnosed with disabilities more often than girls is because boys tend to “act out” more than girls: Boys display externalizing behaviors more frequently and intensely than girls, and this encourages schools to try to figure out ways of helping the boys be less disruptive. Girls tend to suffer in silence.\nHowever, it may well be that acting out isn’t what it is about being a boy that affect his grades. A boy may be the class clown or trouble maker, but he may be quiet bright and do well despite his disruptions of others—or at least attracting attention to oneself may not be what gets one a particular grade. Indeed, boys of any level of disruptiveness tend to be praised for successes in math courses while girls tend to be praised for successes in ELA courses—even the out-spoken ones.\n\nWe will next look at the relationships between these three variables. We’ll first look at them through an ANOVA; the ANOVA should help set the stage since this is an analysis you’ve become familiar with and since this is a very common analysis used.\nAfter we review the ANOVA, we’ll look at the relationship through a more general linear regression analysis. We’ll see how it’s similar to an ANOVA and how it differs. The overall goal here is to help you learn the pros and cons of analyses you’re familiar with (viz., ANOVAs & t-tests) and the reasons consider times to use some other linear regression analyses."
  },
  {
    "objectID": "lrm_02.html#using-an-anova-to-predict-ela-grades-with-gender-iep-status",
    "href": "lrm_02.html#using-an-anova-to-predict-ela-grades-with-gender-iep-status",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "9.4 Using an ANOVA to Predict ELA Grades with Gender & IEP Status",
    "text": "9.4 Using an ANOVA to Predict ELA Grades with Gender & IEP Status\n\n9.4.1 ANOVA Review\nRemember that an ANOVA is used to test whether one or more nominal variables (IVs) significantly predict a continuous outcome variable (DV). More specifically, an ANOVA tests whether the mean outcome score differs between one or more of the levels of a predictor. (Here, for example, if the mean ELA grades differ between girls and boys.)\nThe ANOVA itself can’t say which levels of the predictor are different, though. (For example, it can say that there is a significant effect for gender, but not which has greater scores). To find which levels differ, we usually conduct a post hoc analysis.\nDone well, a reason conduct an ANOVA first is to help control for Type 1 errors: Instead of running a whole bunch of pairwise tests between all levels of a variable (in all variables added to the ANOVA)6, we first run a few, overall tests. We then only conduct post hoc tests on variables that the ANOVA found significant, further limiting the number of tests we run and thus the chances of a false positive effect7.\nOf course, since we only have two levels for each of the predictors here, the ANOVA can tell us if there is a significant difference and then we can simply look at the variable’s means to see which is greater.\n\n\n9.4.2 Graphical Review of the Variables\nLet’s start with looking at graphical representations of these variables and then explore them through an ANOVA.\n\nSPSS’s Graph &gt; Chart Builder interface is quite useful, even if spreadsheet programs like Excel & Calc have mostly caught up to it.\nIn that dialogue box that opens, drag the beige bar graph near the bottom under Choose from into the main window under Variables:\n\n\n\nDrag ELA_Grade to the Y-Axis? box in the bar graph that appears in the main window, drag Gender to the X-Axis?, and drag Spec_Ed to the Cluster on X: Set colors area to the upper right of the graph. That main area should now look like this:\n\n\n\nThe current bar graph purposely resembles the one we created previously while building our crosstabs. However, then we presented absolute counts whereas now we’re showing means, so it’s worth also showing how well the means represent the sample. In the Edit Properties of: area, select Bar1. The area below that will change, allowing you now to select to Display error bars. Confidence intervals is selected by default; leave that selected, and leave Level (%) set to 95, the value every social scientist (and—more importantly—reviewer) knows and loves.\nBoth the Element Properties and the Chart Appearance tabs have reasonable sets of options for customizing figures, but more can be done when it is generated in the Output window and via syntax.\nWays of handling missing data appear under the Options tab. Excluding User-Missing Values is nearly always advisable—the only time I can think to Include them is if you want to report information about the missing cases in the figure.\nUnder Summary Statistics and Case Values, select to Exclude variable-by-variable, which is tantamount to excluding missing data pairwise instead of listwise.\nClicking OK will generate this figure:\n\n\n\nThe figure shows the difference IEP status made and that girls may have had higher ELA grades than boys—at least among those without IEPs. The 95% confidence interval bars suggest which of these differences are significant8.\nIn the Output window, we can modify the figure more. Double-click on it in the Output, to open the figure up in another window with many (Excel-like) options to modify parts of it.\nThe bar chart will now appear as well in its own window; in that window, single-click on one of the No Diagnosed Disability bars (double-clicking can highlight all of the bars, including the Has Diagnosed Disability ones).\nIn the menu bar, choose to change the Fill Color to, e.g., grey9:\n\nWe could also change the No Diagnosed Disability bars to white and increase the thickness of the borders (with the next menu item to the right) to create a slightly more manuscript-ready figure:\n\nWe can similarly change the fonts, the title contents, etc. Note that once you’ve tweaked your figure to your (and your committee’s) liking, you can click on File &gt; Save Chart Template to create a template that you can later File &gt; Apply Chart Template to other figures to create a nice, consistent look.\n\n\n\n9.4.3 Using an ANOVA to Test Variables\n\nGenerating the ANOVA Model \n\nSPSS categorizes ANOVAs under general linear models (Analyze &gt; General Linear Models)10, which can be taken to emphasize that they are a type of linear regression. The Univariate option under Analyze &gt; General Linear Models is for any model that has one outcome (criterion) variable; Multivariate is for when there are more than one criterion (e.g., a MANOVA). We have one criterion, ELA_Grade, so choose Univariate and add ELA_Grade to the Dependent Variable field.\nChoosing whether to place predictors in the Fixed Factor(s) field or the Covariates field matters affects the assumptions that are made by the model about that variable and a bit how we interpret the results. It suffices to say that nominal variables should be added as fixed factors and that ordinal, interval, and ratio variables should be added as covariates11. Since both of our variables are fixed factors, place Gender and Spec_Ed in the Fixed Factor(s) field.\nBy default, SPSS adds in interaction terms for all fixed effects. (So, if we had three fixed factors—say A, B, and C—SPSS would include the A\\(\\times\\)B, B\\(\\times\\)C, A\\(\\times\\)C, and A\\(\\times\\)B\\(\\times\\)C interactions.) We do indeed want to look at both the main effects and the gender \\(\\times\\) IEP status interaction, so we want a “full” model. Even though SPSS would create that by default, let’s build it anyway just so you can see how to do it (and thus how to build other models):\n\nUnder the Model dialogue, first click on the Build Terms button.\nChange the Build Term(s) Type to Main Effects and then move both Gender and Spec_Ed to the Model field.\nNow change the Build Term(s) Type to Interactions. With both Gender and Spec_Ed selected, click on the arrow under Type to create a Gender \\(\\times\\) Spec_Ed interaction term. That dialogue box should now look like this:\n\n\nLet me reiterate that, by default, SPSS creates a full factorial model for all fixed factors, so we didn’t need to do this here (and could have done it more automatically through this dialogue). I did want to show you how so you can modify your models term-by-term rather easily through this particular dialogue.\n\nThe Contrasts dialogue lets us determine if and how SPSS tests differences between levels of the variables. The default is to compare None (and since ours are dichotomous (dummy) variables, any effect of a variable is a difference between those two levels). The options are explained in more detail, e.g., here, but suffice it to say that Deviation—in which each level is compared against the overall mean—is common, that the other options depend which differences matter most of a given study, and that contrasting differences between levels is often better handled via post hoc analyses anyway.\nPlots would allow us to create figures quite like we did with Graphs &gt; Chart Builder, but with fewer options made though a more streamlined interface.\nThe Post Hoc dialogue allows one to compute those. Again, Kao & Green (2008) provide nice, terse explanations and recommendations of commonly-used ones.\nThe EM Means dialogue is useful for our purposes here. This area lets us generate estimated marginal means; these are the means for one factor when other variable(s) are partialed out.\nUnder Options, please select Estimates of effect size and Observed power.\nClick OK.\n\n\n\nANOVA Output \n\nAfter reporting the numbers of cases for each variable, SPSS outputs the source table as Tests of Between-Subjects Effects:\n\n\n\nA familiar sight (I hope), we can see from this table that all of the terms—the intercept, gender, IEP status, and the gender \\(\\times\\) IEP status interaction—are all significant at α = .05. Now, however, a few other parts of this table are of interest (and others perhaps simply worth explaining / refreshing):\n\nThe Corrected Model term is a test of the whole model—yes, like we are doing with linear regressions.\nThe Type III of Sum of Squares indicates how the terms were added to the model. The math is a bit eldritch (even I have to look it up to remember it), but a summary should suffice. The type used here, Type III, is computed by having all of the terms added to the model at once so that their variances are computed in light of all other terms; in essence all terms are partial regressions, even the intercept and interaction. Given our interests here in partial regressions, this is appropriate12.\nThe Partial Eta Squareds are measures of effect sizes for the given terms. As researchers move uneasily away from up-or-down significance tests, they are often using effect sizes as rather sturdy canes for support. Personally, I’m among them, and I report them even as I regularly report p-values, e.g., as “The main effect for gender was significant (F1, 155 = 2.28, p = .049, \\(\\eta\\)2 = 0.025).”\nA bit ironically, people have looked for “tests” of effect sizes. Nearly always, this is a hearkening to the original work on them by Jacob Cohen (1988), where he suggested for \\(\\eta\\)2 that .1 could be considered “small,” .25 could be considered “medium,” and .4 considered “large”13. By that standard, gender has a rather small effect that is nonetheless significant here.\nThe Observed Power is the estimated power of the F-test of the given parameter based on the data. It is the estimated probability that a real effect would be detected; it is affected by the sample distribution and size. SPSS computes the Observed Power with the Noncent. Parameter, a statistics that follows a, well, non-central (skewed) distribution that is a composite of \\(χ\\)2 and Poisson distributions. In other words, don’t worry about it, just know it’s used to compute Observed Power—which itself is really a useless statistic.\nMost importantly right now, note that the R2 reported under the table is .195 and that the adjusted R2 = .17914.\n\nAlthough we could generate parameter estimates and marginal means (means for variable levels adjusted for other variables in the model), they are easier to interpret when we compute the results through a linear regression which we will do now."
  },
  {
    "objectID": "lrm_02.html#linear-regression",
    "href": "lrm_02.html#linear-regression",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "9.5 Linear Regression ",
    "text": "9.5 Linear Regression \n\n9.5.1 Creating an Interaction Term\nWe will use the same model for a linear regression that we did for an ANOVA. SPSS doesn’t automatically compute interaction terms for linear regression models like it does ANOVAs. Fortunately, this is quite easy to do:\n\nClick on Transform &gt; Compute Variable.\nIn the Target Variable field, type, e.g., Gender_IEP_Interaction.\nMove Gender to the Numeric Expression field.\nClick on the * (asterisk) button in the “number pad” below the Numeric Expression field.\nMove Spec_Ed to the Numeric Expression field. The top of that dialogue box should now look like this:\n\n\n\nClick OK to create this variable. It will appear at the far end (right of the Data View, bottom of the Variable View) of the data matrix; you may want to move it to the left / top of the set for easier access.\n\nYes, all we did was multiply Gender by Spec_Ed. That is all an interaction term is: the two variables multiplied by each other8. For a dummy variable like this, of course, 0 \\(\\times\\) 0 = 0, 1 \\(\\times\\) 0 = 0, 0 \\(\\times\\) 1 = 0, and 1 \\(\\times\\) 1 = 1, so the values for this interaction term are all zeros except when for females (Gender = 1) who also have IEPs (Spec_Ed = 1). I’ll explain this later, but simply note it now.\n\n\n9.5.2 Computing a Linear Regression with an Interaction Term\n\nGenerating the Linear Regression Model\nTo present a similar model to the ANOVA above, let’s enter all of the terms together.\n\nClick on Analyze &gt; Regression &gt; Linear.\nEnter ELA_Grade in the Dependent field and Gender, Spec_Ed, and Gender_IEP_Interaction to the Independent(s) field (and setting the Method is Enter).\nUnder the Statistics area, make sure Model fit and R squared change are both selected.\nUnder the Options... area, make sure Include constant in equation and Exclude cases pairwise are selected.\nClick OK.\n\n\n\nLinear Regression Output\n\nThe Model Summary shows that the model does account for significant amount of the variance in the data (F3, 157 = 12.6, p &gt; .001):\n\nAlso note the the R2 and adjusted R2 are the same as we found with the ANOVA: This is the same model, just looked at in terms of the model fit instead of the significance of model parameters.\nThe ANOVA table in the linear regression about shows similar statistics to the Corrected Model row in the source table for the ANOVA above: In the source table above, the F-score for the Corrected Model was 21.69; here, the similar statistic is the F = 12.643 in the Regression row:\n\n\nThis is because the sums of squares are computed a bit differently here. Nonetheless, the outcome is the same.\nTo show that the outcome is the same, remember that the model R2 is the proportion of total variance in the data that is accounted for by the model; in other words, R2 is the variance in the model divided by the total variance.\nNow, remember what a “sum of squares” here is: It’s the squared differences between the expected value and the actual value, all added up. So, if the blue line in the figure below is the regression line estimated by an entire model (not this ELA model, but just a made-up one between two z-score variables):\n\nNow, if we didn’t have that regression line to help us—if we had no information except the column of ELA grades—then the best guess we could make about the grade for each student would be the mean ELA grade for the whole sample. In that figure, both variables are z-scores, so the means are zero: If I didn’t use the values of the predictor to estimate that line, then the best guess we would have for that person’s score on the criterion would be the mean, zero. In this case, to get the “sum of squares,” we’d first get the difference of each predictor from the mean, then square and sum those values—this would be the sum of squares if we didn’t use any information in the predictor(s): This would be the Total Sum of Squares in the table: 89.556.\nThen the red line shows one of the distances from an actual data point from that estimated line. If we squared this distance—and all of the distances of the dots from the line—and then added up those values, we would get the Regression (or, computed slightly differently, the Corrected Model) Sum of Squares, which here is 12.643.\nRemember that the R2 is the model sum of squares divided by the total sum of squares: Here, that is 17.426 / 89.556 = 0.195. In the ANOVA we computed above, this is 21.69 / 89.556 = 0.195. They both produce the same R2 value.\n\n\nScatterplot of the Data \nWhat’s that? You say you’d rather see what a chart for these data would look like than an mock one of two made-up z-scores? Well, all right then:\n\nClick on Graphs &gt; Chart Builder and select Scatter/Dot from the Gallery tab in the bottom left corner. (You may well see a warning dialogue when opening the Chart Builder saying that “Before you use this dialog, measurement level should be set properly…”; this is a good thing to check, and has been for these data, so it’s fine to click OK.)\nDrag the first figure image, Scatter Plot, up into the main field just under where it says Chart preview uses example data.\nJust as we did for the bar graph at the beginning of this handout, put ELA_Grade in the Y-Axis field, Gender in the X-Axis. Also add Spec_Ed into the Set color? field in the top right.\nClick OK. The default difference in color\nDouble-click on the figure that’s generated, and click on the Add interpolation line button, which is the third button from the right:\n\nThis will add a regression line for the IEP status of the males and another for the IEP status of the females. It’s not so easy to tell, but the female’s line is the upper one.\nAfter clicking on elements and using either the tool bar or Properties dialogue (accessed, e.g., via Ctl + T), we can create a figure like this:\n\nshowing that having an IEP has more of an effect on males’ than females’ ELA grades (indeed, there is little overlap between the grades of males with and without IEPs, unlike the females). It also shows that there is a wider range of grades among the males—including that the best (and worst) ELA grades were earned by males.\nThe output also provides the coefficients for the model terms:\n\n\n\n\nUse of Dummy Variables to Estimate Outcomes \nThe coefficients for the model terms allow us to estimate the group means—and (hopefully) help explain a bit more about dummy variables. The equation for the linear model we analyzed can be written as:\nEstimated ELA Grade = Intercept + Gender + IEP Status + (Gender \\(\\times\\) IEP Status)\nor a bit more abstractly as:\nELA_Grade' = b0 + b1Gender + b2Spec_Ed + b3Gender_IEP_Interaction\nwhere the tiny apostrophe (') at next to ELA_Grade denotes that we are estimating—predicting—ELA_Grade, not simply reproducing it. So, the better the model, the more the predicted scores will replicate the actual ones.\nIn that second equation, b0 is what is given in the Unstandardized B column15 of the (Constant) row of the Coefficients table, so we could rewrite that equation as:\nELA_Grade' = 2.983 + b1Gender + b2Spec_Ed + b3Gender_IEP_Interaction\nWe can similarly fill in the values for b1, b2, and b3 from the Unstandardized B column to produce:\nELA_Grade' = 2.983 – 0.212(Gender) – 0.870(Spec_Ed) + 0.804(Gender_IEP_Interaction).\nNow, remember that Gender, Spec_Ed, and Gender_IEP_Interaction all have values of only either 0 or 1. Gender_IEP_Interaction is 1 if the student is a female (Gender = 1) with an IEP (Spec_Ed = 1); otherwise it’s a 0 since 0 \\(\\times\\) 1 = 0, 1 \\(\\times\\) 0 = 0, and 0 \\(\\times\\) 0 = 0.\nSo, if we want to estimate the ELA_Grade score for a boy (Gender = 0) without an IEP (Spec_Ed = 0), the equation is:\nELA_Grade' = 2.983 – 0.212(0) – 0.870(0) + 0.804(0)\nor:\nELA_Grade' = 2.983 – 0 – 0 + 0\nor simply:\nELA_Grade' = 2.983.\nSo, since we coded our variables as dummy variables, then the (Constant) coefficient is the estimated ELA_Grade score for boys without IEPs. Whatever condition in a set of data that has all 0s for all dummy variables is called the reference group: It is the group against which all effects are compared.\nIf we wanted to estimate the ELA_Grade score for girls (Gender = 1) without an IEP (Spec_Ed = 0), the equation is:\nELA_Grade’ = 2.983 – 0.212(1) – 0.870(0) + 0.804(0)\nor:\nELA_Grade’ = 2.983 – 0.212 – 0 + 0\nor:\nELA_Grade’ = 2.771.\nIt is unexpected that girls would have an estimated lower ELA grade than boys, but we also know from the bar graphs, scatterplot, and analyses that gender in fact has a rather weak effect: This estimated score is likely not strongly predictive (accurate) for any particular case.\nIEP status, however, was more predictive, having an \\(\\eta\\)2 = 0.129, compared to gender’s \\(\\eta\\)2 = 0.25. The estimated grade for a boy (Gender = 0) with an IEP (Spec_Ed = 1) is:\nELA_Grade’ = 2.983 – 0.212(0) – 0.870(1) + 0.804(0)\nELA_Grade’ = 2.983 – 0 – 0.870 + 0\nELA_Grade’ = 2.113.\nHaving an IEP had a relatively strong effect on a boy’s ELA grade.\nThe effect of an IEP on a girl’s grade must take into account not only that she’s a girl and that she has an IEP, but also the interaction effect of being a girl with an IEP:\nELA_Grade’ = 2.983 – 0.212(1) – 0.870(1) + 0.804(1)\nELA_Grade’ = 2.983 – 0.212 – 0.870 + 0.804\nELA_Grade’ = 2.705.\nWe knew from our initial investigations into the correlations between these variables that gender and IEP status were themselves related, and this is where that is represented in a linear model.\nSo, to summarize how the dummy variables here are set to work to create an estimated ELA grade (and to change the equation notation a bit):\n\n\nTable 9.1: Example Interpretation of Dummy-Coded Variables\n\n\n\n\n\n\nBoy without IEP (Reference group):\nELA_Grade’ = bConstant\n\n\nGirl without an IEP:\nELA_Grade’ = bConstant + bGender\n\n\nBoy with IEP:\nELA_Grade’ = bConstant + bSpec_Ed\n\n\nGirl with IEP:\nELA_Grade’ = bConstant + bGender + bSpec_Ed + bGender_IEP_Interaction\n\n\n\n\nThe b-weights for the terms in each model determine the predicted score for that condition.\n\n\n\n\n\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nKao, L. S., & Green, C. E. (2008). Analysis of variance: Is there a difference in means and what does it mean? Journal of Surgical Research, 144(1), 158–170. https://doi.org/10.1016/j.jss.2007.02.053\n\n\nVisalakshi, J., & Jeyaseelan, L. (2014). Confidence interval for skewed distribution in outcome of change or difference between methods. Clinical Epidemiology and Global Health, 2(3), 117–120. https://doi.org/10.1016/j.cegh.2013.07.006"
  },
  {
    "objectID": "lrm_02.html#footnotes",
    "href": "lrm_02.html#footnotes",
    "title": "9  Linear Regression Modeling with SPSS, Part 2: More about ANOVAs and Dummy Coding",
    "section": "",
    "text": "In collecting these data, students were asked to indicate whether they were male or female, so gender is dichotomized here.↩︎\nKarl Peasron is the person who first devidsed using \\(\\chi\\)2 distributions in statistics, so SPSS calls this a Pearson \\(\\chi\\)2. This is nonetheless just the same \\(\\chi\\)2 we used anywhere else. In other words, it’s redundant—or superfluous—to call this a “Pearson \\(\\chi\\)2” instead of just “\\(\\chi\\)2”.↩︎\nAssuming it’s a normal or \\(\\chi\\)2 distribution—or one of the other distributions that are like them, such as the t or F distributions used to test t- and &F&-scores.↩︎\nIn fact, it’s a one-tailed test, testing whether the proportion (viz., of IEPs) is the same or different, but it doesn’t test whether any differences in the proportions are due to larger or smaller proportions here. For our uses—and likely any you will encounter—considering it a non-directional test of differences somewhere suffices.↩︎\nA moment’s reflection will reveal that my Venn diagrams aren’t really reflective of the point I’m trying to make, but I decided to go with a simplified representation that hopefully still works.↩︎\nAnd yes, reducing Type 1 errors by conducting fewer significance tests can be seen as an ironic reason to conduct an ANOVA since it’s pretty common for researchers to run post hoc analyses on many or all of the nominal variables, and thus end up conducting more analyses after counting un-necessary post hocs.↩︎\nKao & Green (2008) provide an excellent review both of ANOVAs in general and of the uses of the several post hoc analyses.↩︎\nIt’s worth noting as well that there is a growing trend—at least among leading statisticians if not general researchers—to rely more on more on less definitive measures like confidence intervals to convey one’s results than to rely on up-or-down significance tests. Right now, at least though, my experience has been that reviewers are not comfortable when one excludes significance tests, so I recommend presenting data both with, e.g., confidence intervals and with p-values. Note, too, that confidence intervals and p-values are not equivalent: The confidence intervals are computed making few assumptions about the data and do not consider, e.g., if the data are skewed unless you modify the intervals in light of skewness. If you want to account for skewness, the preferred methods is currently to use bootstrapped intervals as describe in, e.g., Visalakshi & Jeyaseelan (2014) or to use log transformations. The latter is more well-known and accepted, but the former is likely preferred since it doesn’t make any assumptions about the underlying population and is more generally use-able whereas log transformations are only useful for data that are nearly normal but simply skewed.↩︎\nColors can be useful—and sometimes necessary—but grey scales print in hard copies well and often are more easily seen by people with colorblindness.↩︎\nThe naming of analyses gets capricious and confusing from there, though. A “general linear model” is a type of “generalized linear model.” Multilevel models and logistic regression are also types of generalized linear models, but let’s leave it at that. The list of terms in Table C.1 in Appendix B is intended to help clarify this and other confusions.↩︎\nThe non-simplified answer is that designating a variable as a fixed factor means we’re assuming that all possible levels of that variable are present. Both our variables are fixed factors since we have dichotomized them into “Is female” or “Is not female” (and “Has an IEP” or “Doesn’t have an IEP”). Whenever the levels of our variable exhaust all possible options, then a variable is fixed. The null we’re testing against is that the means are the same for all levels of the variable.\nWith random factors, we’re assuming that not all levels are present in our data. For example, we may be testing differences between hospitals: We have data from a few hospitals, but certainly not all hospitals. The null hypothesis we’re testing against is that there is no variance between any levels in the population (i.e., it’s a test against inferred population variance, not differences in the sample means).\nFor covariates, we are computing the slope of that variable with the criterion—whether the slope differs from zero. Since we compute a slope, we could estimate the values on the criterion for values of the covariate that were not included in the model.\nIt is bit unfortunately that SPSS calls this a covariate since we often think of a covariate as something we are controlling for I a model—something we want to partial out so that we can see the effect of another variable more clearly. Covariates here can certainly be used to do that, but they don’t need to be: Variables placed in the covariates field can be interpreted as the main variables of interest and the fixed ones could be ones we’re partialing out. The math is the same regardless of which term we’re interpreting as the variable of interest and which we’re adding to the model to partial out its effect.\nA further point to make is that SPSS doesn’t compute random factors efficiently in Analyze &gt; General Linear Models. It would be better to use the Analyze &gt; Mixed Models &gt; Linear dialogue for models with variables that are continuous a. Nonetheless, this isn’t absolutely necessary to do, and the output you get from adding random factors here won’t likely ever greatly differ from the results gained from the Mixed Models analyses.↩︎\nType II sum of squares is similar in that the terms are all added together, but the main effects are partialed in light of each other but not in light of the interaction; if there are no interaction terms, then Types II and III are computational the same. In Type I, the terms are each added one after the other, like we did in the last handout for the linear regression model; the order they are entered is the order they’re listed in the Fixed Factor(s) field, and then the Random Factor(s) field, and finally the Covariates(s) field.↩︎\nPlease see Chapter 6 for more on effect size and guidelines for “small,” “medium,” and “large.”↩︎\nRemember that adjusted R2 is adjusted for the number of terms in the model since having more terms—even non-significant ones—can increase the model R2.↩︎\nIf we were predicting the standardized ELA_Grades—which we’re not—we would use the values from the Standardized Coefficients Beta column.↩︎"
  },
  {
    "objectID": "mlm.html#overview",
    "href": "mlm.html#overview",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.1 Overview",
    "text": "10.1 Overview\nAfter reviewing a few common ways to analyze longitudinal data, this chapter then focuses on conducting a rather sophisticated approach, variously called hierarchical linear regression, multilevel modeling, or mixed models. These models can be used to analyze more than just longitudinal data, but that will be our focus here.\nAs is usual for these chapters on using software, I will try to present two things related to the relevant analyses. This time, the first (in Section 10.3) will be a bit more “down-and-dirty” data preparation using both a spreadsheet program and SPSS. The second (in Section 10.11) will be using SPSS to review these data and to conduct a multilevel model (MLM) of change on them."
  },
  {
    "objectID": "mlm.html#comparison-of-analyses-of-longitudinal-data",
    "href": "mlm.html#comparison-of-analyses-of-longitudinal-data",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.2 Comparison of Analyses of Longitudinal Data",
    "text": "10.2 Comparison of Analyses of Longitudinal Data\nThere are four ways that time-dependent data are typically analyzed:\n\n10.2.1 Explanations of Longitudinal Analyses\n\nPre-Post Difference Scores\nAlong with repeated-measures ANOVAs, analyzing pre-post difference scores is probably the most common method of analyzing changes over time.\nOf course, pre-post difference score analyses require only two waves of data collected before and after some event. One first usually subtracts a given participant’s1 pretest score from their posttest score to compute this difference. These differences scores are then compared between groups using, e.g., t-tests to see whether the mean difference between groups differs significantly.\n\nAdvantages\nThey are easy to compute and intuitively easy to interpret.\n\n\nDisadvantages\nOtherwise, they suck. First, any measure that is vulnerable to ceiling or floor effects will suffer from a bias in difference scores: Those who start and/or finish near the limit of the scoring range risks being affected by score restrictions: any values outside of the measurable range will be restricted2. Therefore, it is harder to detect a difference among those who either start or end the study with extreme scores—those we may well be most interested in.\nSecond, difference scores use only part of the information in the data. Not only can they not test for range restrictions (or other biases in the measurements of any of the waves), but they also omit any of the information available in the actual measurement of the phenomenon at each level; they are, after all, only the difference in the scores—not the scores themselves. Data are expensive and differences scores are wasteful.\nOutside of any decisions about ethics and sophistication of analyses is the simple fact that difference scores are more susceptible to both Type 1 and 2 errors (and often Type VII errors). Since difference scores omit a large a large part of the measurement that contains the true, underlying score (e.g., a person’s actual confidence in their ability to treat an illness—not just how confident they say they are), they are more likely to miss a true effect (and thus commit a Type 1 error). Since difference scores (as explained further just below) are more susceptible to error, we may also believe we’ve found an effect when in fact it was just a random difference due to chance alone.\n\n\nOther Problems with Interpreting only Two Waves of Data\nIt is worth also raising another point—not about pre-post difference scores, but about using only two waves of data to test changes over time. As argued by Singer and Willett (2003, p. 10), the interpretation difference between only two waves of data—such as pre- & posttest scores—is difficult to disentangle from error or biases in the responses. The old adage (if that’s what it is) that it takes three points to test a line applies here: That third point of data can help establish a trend in the time-varying data.\nNow, as one who has rather frequently used only two waves of data, I have to let my cognitive dissonance have its say. I don’t disagree with Singer and Willett’s argument that it’s hard to tell if the difference between two waves is chance—or even a bias in one or both waves—but I don’t see how this necessarily differs from any comparisons between two groups that differ in some continuous measure.\nI do agree, though, that the answer to the problem of having only two waves is to have more data. I also agree that those additional data are most efficient if they are taken at other points in time—especially data measured farther out from the other waves3. However, I honestly do believe that problems of disentangling differences between two waves or between two groups can be addressed with more data—especially data that has a lot of “good” variance to it.\n\n\n\nANCOVA with a Pretest Covariate\nOften when we take pre- and posttest measures, what we’re really interested in is whether groups differ in their posttest scores; we’re not interested in whether they differ at pretest—in fact, we hope they don’t differ at pretest. Of course, for interesting and uninteresting reasons alike, they may differ at pretest.\nPre-post difference scores attempt to account for any group differences at pretest by eliminating the information available about participant’s pretest scores. There is an other strategy, though: partial out the pretest scores from tests of differences between posttest scores.\nTo do this, we conduct an ANOVA in which, e.g., the groups are the IV (predictor) and the posttest score is the DV (Outcome), thus testing for group differences in the posttest scores. However, we can’t defensibly just do that since it may well be that part of an individual’s posttest score is determined by that person’s pretest score4. So, we also include pretest scores as a covariate in our model—making our ANOVA into an ANCOVA and thus effectively accounting for any effect of one’s pretest score on the posttest score.\n\nAdvantages\nANCOVAs with pretest covariates accurately and efficiently account for relationships between pre- and posttest scores, letting any group differences in posttests be isolated from effects of the pretest. Essentially, this type of ANCOVA lets us use math to take care of any problems with our baseline that our study design could not.\nAdding pretest data as a term in our model not only isolates its variance from other terms, it also lets us tests that pretest term. A significant main effect for the pretest term would indicate that pretest scores do indeed significantly predict posttest scores. A significant group \\(\\times\\) pretest interaction would mean that pretest scores deferentially affected the groups’ outcomes (whether or not they started with differences in the pretest levels).\nIndeed, to the best of my knowledge, ANCOVAs with pretest covariates are the best option outside of MLMs to test for group differences at posttest.\n\n\nDisadvantages\nOf course, these models can only be used to test just that: group differences at posttest.\nIf one has more than two posttest waves, one could combine this approach with the next one and create a repeated-measures ANCOVA. This method could work, but suffers from all of the disadvantages of a repeated-measures ANOVA while also removing analyses of differences from baseline/pretest that are often made in repeated-measures ANOVAs.\nOther disadvantages of this approach are related to the needs of ANOVAs. ANOVAs, of course, assume that data—and error—are homoscedastic. However, this is often not the case for time-varying measures; often people who start at a similar place take different paths over time. This increase in variance—both in true scores and error—is not handled well by ANOVAs. Like with having only two waves of data, ANOVAs aren’t designed to account for this extra variance, so it can easily either eclipse a true effect or lead us to think we have an effect when we don’t.\n\n\n\nRepeated-Measures ANOVA\n\nBasic Concepts\nStrictly speaking, a repeated-measures ANOVA per se is only tests differences within participants across different times. So, if we first administer one treatment to a patient and then administer a different treatment to that same patient (measuring outcomes each time), then we are conducting a repeated-measures ANOVA.\nOf course, we often have more than one group of participants, and want to compare outcomes about them at different times. This is more accurately called a two-way (or two-factor) repeated-measures ANOVAs5, but they are often simply referred to as a repeated-measures ANOVA without the additional clarification about there also being non-repeated factors in the model, too.\nLike any other ANOVA, a repeated-measures ANOVA conducts an omnibus test for each factor in the model to see if there are any significant differences between the levels somewhere without testing where; specific comparisons between each of the levels are detected, e.g., with post hoc comparisons6. The difference, here, of course, is that a set of those comparisons are for different points in time that are nested within each participant.\n\n\nAn Example\nI think a pre-post design with an experimental and control group serves as a great example to concretize these concepts. The executive functioning data don’t work so well for this example, so let’s instead use a different one. Let’s say we have a health literacy program, one goal of which is to improve patients’ confidence in asking probing questions with their health care provider about their condition. We measure this confidence (through some self-report instrument) for all participants before experimental-group participants complete the program (control-group participants compete an unrelated program during this time); we then re-administer this confidence instrument after the program is done, getting pre- and posttest scores from both the experimental-group participants and control-group participants.\nIn this design, we have two factors: the treatment—whether the participants completed the health literacy program—and time—whether measurements were made at pre- or posttest. So, in our model, we can include a main effect term for treatment; this will test whether there is an overall difference between c0nfidence scores between the experimental and control groups. This will also use the typical treatment and error sum of squares values that one thinks of for an ANOVA.\nWe can also include a main effect term for time; this will test whether there is an overall change in confidence scores from pretest to posttest. The sums of squares will be partitioned a bit differently since we will be looking at differences within each participant7.\nMost importantly, we will include a treatment \\(\\times\\) time interaction term. This will test whether any differences in pre-post scores are themselves different between the groups8. With a significant interaction, we could then use post hoc analyses to see, e.g., if the groups significantly differed at posttest but not at pretest and if experimental-group participants’ posttest scores differed from their own pretest scores, but those of the control group did not. (We could also add directionality constraints, seeing if the experimental group’s mean posttest scores were higher then the control group’s; this would allow us to us one-tailed significance tests and increase our power for that test.)\nSo, at its heart, a repeated-measures ANOVA tests differences in levels. It’s just that some of those levels are within participant, so—under the hood—the sums of squares are computed differently for with-participant (time-varying) terms.\n\n\nModel Assumptions\nRepeated-measures ANOVAs must meet the same assumptions of non-longitudinal ANOVAs—mainly that data and error are normally distributed and independent. However, because repeated-measures ANOVAs span multiple waves should also meet extensions of those two assumptions. First, not only should each variable be defensibly similar to a normal distribution, but the relationships between the variables should also approximate normality; this manifests as what’s called “multivariate normality,” and can be investigated by looking at the distribution of difference scores between variables (and especially the difference scores between waves for a given variable): If the difference scores are reasonably normal, then your data are more or less multivariate normal.\n\nThe Sphericity Assumption\nSecond, the data should display what’s called “sphericity,” which is essentially homogeneity (and independence) of variances across the waves9. If the data do not show enough sphericity, then we run the risk of under-estimating our ability to detect significant differences: The larger error variance in some waves may be hide a real effect between other waves.\nIt’s not unusual for longitudinal data to contain non-ignorable departures from sphericity, so it is common to test for these when conducting repeated-measures ANOVAs. In fact, SPSS does this by default, reporting the results of Mauchly’s test, which tests for a difference between the variances. In other words, Mauchly’s test looks for a significant difference a lot like a χ2-test does. If it finds a significant differences between the variances at different waves—if the p \\(\\le\\) .05 for Mauchly’s test—then the assumption of sphericity may be violated9 and one should modify the analyses to accommodate for this violation.\nTo deal with violations of sphericity, one can use either the Greenhouse-Geisser or the Hyund-Feldt correction; both of these adjust the F-test’s degrees of freedom as much as needed based on the extent of the departure from sphericity. The Greenhouse-Geisser correction is a bit more commonly used—and may be more accurate especially for larger departures from sphericity—but both tend to return similar results for data that are good enough to analyze. Generally, you should be safe reporting the the Greenhouse-Geisser correction unless one of them indicates significance and the other doesn’t, in which case use the Greenhouse-Geisser correction if the epsilon value in SPSS is lesser than .75 and use the Hyund-Feldt correction if epsilon is equal to or greater than .75.\nBoth the Greenhouse-Geisser and the Hyund-Feldt corrections are also given by default in SPSS when one chooses Analyze &gt; General Linear Model &gt; Repeated Measures. They appear in the Tests of Within-Subjects Effects table, where the adjusted p-values in that table (given as Sig.) can be used to test significance of the repeated variable; if this value is less than .05, then there is at least one significant difference somewhere between the waves that we can be relatively sure is real, so we can then, e.g., conduct post hoc analyses to find the difference(s).\n\n\n\nAdvantages\nIts comparisons between scores at different waves is intuitive, and many readers and reviewers are accustomed to interpreting it. If one simply wants to test differences between fixed points in time (and if the assumptions of the model hold)–like we did in our example of confidence pre- and post-participation in a health literacy program—then repeated measures work well. We can also run repeated measures analyses on multilevel (i.e., nested) data. For more about doing this in SPSS, UCLA’s Institute for Digital Research & Education has a good guide.\n\n\nDisadvantages\nRepeated-measures ANOVAs are vulnerable to missing data, and missing data tends to be a bigger problem in longitudinal designs (believe me!). If a participant is missing data for any of the waves, then we remove their data from all of the waves; essentially missing data are handled with listwise deletion in them.\nRepeated-measures ANOVAs make equal comparisons between waves—a test of a difference between, e.g., wave 1 and 2 is tested the same as a test for a difference between wave 2 and 3; given this, tests between waves may not be valid if the waves are not all equally spaced.\n\n\nConclusion\nRepeated-measures ANOVAs are good for what they do—if you can keep the study together long enough to maintain equally strong measurements and throughout it. If you can keep variability, sample sizes, and distances between waves reasonably equal—and if you can compensate for any lack of sphericity between the waves—then repeated-measures ANOVAs are a good way to test differences between waves and between groups within a given wave.\n\n\n\nMultilevel Models of Change\n\nBasic Concepts\nBoth repeated-measures ANOVAs and multilevel models of change parcel out variance and covariance based on which terms are nested within which other terms. Since we make multiple measures of a given participant (over time), we are nesting those measurements within that individual in both types of analyses. Multilevel models (MLMs—also called hierarchical linear models, HLMs, and linear mixed models, LMMs) allow for one variable to be “nested” within another variable like a repeated measure ANOVA, but they also allow us to model time differently—and more fully and flexibly10. In a repeated-measures ANOVA, the various waves of time are modeled as levels of a nominal variable—just like any other nominal variable in an ANOVA. MLMs model time as a continuous variable. As we discussed in class, from this continuous variable of time, MLMs compute both a slope—to measure how the outcome changes over time—and an intercept—to factor in the effect of where participants’ initial levels begin and how that may affect other things in the model.\nBoth repeated-measures ANOVAs and MLMs parcel out variance and covariance in the model based on what level a term is at. So, both model time as being nested within the participant (what Singer and Willett call a level 1 model) and both model between-participants effects at a different level (Singer & Willett’s level 2 model). MLMs, however, cut the variances and covariances into smaller, more precise pieces. This allows us to model both relationships between terms more precisely and to account for error more specifically.\n\n\nAssumptions\nMLMs typically assume that the error terms (or more specifically, the model residuals) are normally distributed with means of zero.\nWe also assume that the residuals are unrelated to the model’s predictors and that the residuals for one term (say rate of change within a participant) are unrelated to residuals in another term (say a between-participant treatment term), but these are usually taken care of by correctly adding both terms to the model.\nNote that we do not assume sphericity. In fact, MLMs are designed with the expectation that a participant’s score at one point in time will be related to that person’s scores at other points in time—and that a participant’s residuals at one point in time will be related to residuals at other points in time. They also are robust against differences in variance across time, i.e., that scores may well spread out (or contract) across time.\nWe do still want to try to minimize our error terms. We also still want to ensure that we correctly model ways in which people or situations are similar—such as being treated at the same hospital or being in the same research group.\nSo, for MLMs, it is good practice to look at the residuals for the various terms in the model to see if (1) they appear to be normally distributed, (2) they are not skewed, (3) and that they do not correlated with any predictors or criteria in the model. Of course, if any of the residuals are correlated with any of the term (predictor or criteria—or perhaps even other residuals) then good! There is something else interesting going on in your studies that may produce novel and fruitful insights in future research. But for now, any residuals that are egregiously non-normal or correlated with model terms would simply encourage one to try out different combinations of terms to look for a better-fitting model, and then to report, say, this in the Results and then conjecture about it in the Discussion before putting that thought into an induced coma until you can revisit it more systematically.\n\n\nAdvantages\nModeling time as a continuous variable has several implications for the model. First, we don’t need to have the waves equally separated from each other. It’s fine to have the waves at unequal intervals—just make sure to measure “where” that wave is in time, e.g,. the number of days that wave occurred into the study.\nSecond, the participants don’t need to have their waves at the same time. Because the slope is estimated within each participant, measuring outcomes at different times for different participants will not adversely affect the analyses. Of course, it may affect our interpretations if we’re trying to infer events happening at one time for one participant with events happening at a very different time for another participant, but that’s more a matter of interpretation, not computation.\nThird, I don’t need to have the same number of waves for each patient. Having missing time data in a repeated-measures ANOVA removes all other data for that participant. For a MLM, having fewer waves for a given participant simply means that estimates for that participant’s slope11 are less powerful since there is a smaller sample size.\n\n\nDisdvantages\nNot all time-varying events change in a simple, linear fashion. For example, although simply plotting a regression line through each these two groups would show changes over time (i.e., have different slopes):\n\n\n\n\n\nit would miss the important detail that it’s only in May that the groups start to differ. (Repeated measures ANOVA would be a better choice for here, assuming the waves of time were evenly spaced, error is homoskedastic, etc.)\nAnother disadvantage is that MLMs do take more sophisticated understanding to conduct—and to understand as a reader. Sometimes, the best analysis is the one your readers understand best.\n\n\n\nRegression Discontinuity in Time\nRegression discontinuity in time (RDiT)….\n\nFurther Resources\n\nRDiT versus other methods, including comparisons of it with traditional regression discontinuity designs\n\nHausman, C., & Rapson, D. S. (2018). Regression discontinuity in time: Considerations for empirical applications. Annual Review of Resource Economics, 10(1), 533–552. https://doi.org/10.1146/annurev-resource-121517-033306\nHausman, C., & Rapson, D. S. (2017). Regression discontinuity in time: Considerations for empirical applications. National Bureau of Economic Research Working Paper Series, No. 23602. https://doi.org/10.3386/w23602.\n\nThis version is a little longer and more detailed than the 2018 version published in the Annual Review of Resource Economics, just above."
  },
  {
    "objectID": "mlm.html#sec-mlm_data_prep_spss",
    "href": "mlm.html#sec-mlm_data_prep_spss",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.3 Data Preparation and Manipulation",
    "text": "10.3 Data Preparation and Manipulation\nThe data we will be those from which the data we used previously (viz., EF_Slope_Data.sav) were derived. The data we’ll use here also contain more participants from the school.\nMore importantly, they are also less prepared for the analyses here; they are both spread out over multiple files and are not all in the “long” format required for MLMs. It is not uncommon for data to come in the “wide” format that are not conducive to MLMs, so covering what wide and long formats are and how to change data between these formats is nearly necessary here.\nIn addition, data may come in multiple files like I’m presenting here; it’s how I store these data in part because I get them from different sources. In addition, the sets of data related to this line of research already include over 5 million data points, so I rarely evoke them all together (and hardly have the RAM for it even if I wanted to). Instead, I merge data stored in different sets as needed. Although this means keeping track of different sets, I’ve found it helps keep the data clean since each set is simpler, and—as I hope you’ll see—allows me to store them in ways more appropriate for those particular data.\nWe’ll be combining three sets of data here:\n\ndemographics.sav, a matrix of data that includes gender, race/ethnicity, economic distress, English language learner status, birth date, and IEP status;\nef.sav, a “long-”formatted data set that contains information about executive functioning scores; and\ndiscipline.sav, a “wide-”formatted data set that contains the number of discipline incidents of nearly any kind (ranging from a uniform infraction to assault).\n\nRemember that, although anonymized, this is information about real people—teens who are struggling to grow and succeed—so please treat these data kindly and confidentially.\nLet’s first look at each of these data sets before merging them together."
  },
  {
    "objectID": "mlm.html#understanding-the-demographics.sav-data",
    "href": "mlm.html#understanding-the-demographics.sav-data",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.4 Understanding the demographics.sav Data",
    "text": "10.4 Understanding the demographics.sav Data\nThis set of data is most like those you’re accustomed to: The id identifying each adolescent appears in the first column; also in each row are data related to that particular teen. There is one row for each teen, and each column contains data related to aspects of that teen’s demographics. The first of these is a binary conception of gender, where—again—males are coded as 0 and females as 1. The next column contains a text description of that student’s race/ethnicity (“African-American,” “Asian-American,” etc.). The next five rows recode race/ethnicity into a set of dummy variables; note that I kept a dummy variable for all races/ethnicities—I didn’t exclude one ethnicity to serve as the default (reference) comparison (“Multiracial” was coded out into the races/ethnicities that comprise that group).\nAfter the dummy race/ethnicity variables are a set of other dummy variables. These are for economic distress, whether the teen is an English language learner, and whether the teen has an IEP—all variables you’re used to working with by now.\nThe final variable, julian.birth, is in fact the teen’s birthday. It is expressed in the Julian system, and is simply the number of consecutive days since a standardized, arbitrary date12. Although the dates can’t be read without using a conversion, it is useful here since we will be looking at longitudinal data for which measuring things on a scale of days can be useful13."
  },
  {
    "objectID": "mlm.html#understanding-the-discipline.sav-data",
    "href": "mlm.html#understanding-the-discipline.sav-data",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.5 Understanding the discipline.sav Data",
    "text": "10.5 Understanding the discipline.sav Data\nThis file also contains id in the first row, so that rows with the same id in the demographics.sav and discipline.sav files relate to the same teen. The next three columns in the discipline.sav data set give the number of times each year that that student was reported to the main office for disciplinary actions related some sort of rule violation. So, the second column, AY_20162017_Incidents, is the number of times a student was disciplined for violations during the 2016-2017 academic year.\n(Note that I did not separate out the severity of the infraction or the discipline. Therefore, the discipline could have been for something rather small, like talking back to a teacher, or fairly large, like getting in a physical fight. However, most very-small infractions are not relayed to the administration, and very serious incidents result not in discipline but in, e.g, expulsion. Therefore, the infractions here tend to be middling ones, like multiple absences or verbal altercations with other students.)\nThe data in this set are presented as one often sees for chronological data: Each row is a different person/case, and each column a time-related datum for that moment in time and for that person/case. One advantage of data presented in this “wide” format is that it is rather easy to see for whom there are data and to get a sense of patterns across the time moments for a given person. We see, for example, that the fourth student from the top, 219983160, had 3 incidents in academic year (AY) 2016-2017, 5 in the next year, and then none in the final year: Perhaps after a couple rougher years of adjustment, this student in now catching their stride. It is also the form data should be in for one to compute a repeated-measures ANOVA.\nAs we will see, though, there are analysis-related advantages to re-arranging the data from this “wide” format to a “long” one."
  },
  {
    "objectID": "mlm.html#understanding-the-ef.sav-data",
    "href": "mlm.html#understanding-the-ef.sav-data",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.6 Understanding the ef.sav Data",
    "text": "10.6 Understanding the ef.sav Data\nThe ef.sav file contains data related to each student’s executive functioning. The right-most columns are metacog.index.tchr, beh.reg.index.tchr, and global.exec.comp.tchr: meta-cognitive, behavioral-regulation, and the composite global executive composite functioning scores, respectively, as measured by one of that student’s teachers; and metacog.index.sr, beh.reg.index.sr, and global.exec.comp.sr, scores on similar domains as self-reported by that same student. The meta-cognitive, behavioral-regulation, and global executive composite scores are all obtained from the same instrument, so these three scores are obtained on the same day for each occasion.\nBefore these columns containing EF-related scores are two columns related to time. The column right after id is a good place to start to understand these. In the first row of data, we see the student-reported EF scores for student 2 for the 2010-2011 academic year. In the second row, we see the EF scores also for student 2 for the 2011-2012 academic year.\nYou can therefore see that I have organized the data in this set so that each row includes any and all data collected at a given moment in time. Data were collected on student 2 on two occasions: They completed the instrument (the BRIEF-SR) to generate EF-related data during AY 2010-2011, and we were able to get STEM14 grades for student 2 in AY 2011-2012.\nData that are presented in this format—with each moment in time for each person given in a different row—are called “long” data; that this data set spans nearly 8,000 rows explains why it’s called that.\nThe other time-related column, wave, contains the Julian-style date on which those data were collected. This number is even less intuitive than the julian.birth field. In my own set of these data, I call this field julian.wave.semester.centered, which belies some of why these values are so abstruse: It is a day near the end of a given semester, that part isn’t so hard to understand. It is a Julian date, but obviously not the number of days since 4713 BCE; instead they have been “centered” so that we are counting the number of days since the school opened, 304 days before we first collected data on any students. I also use the convention suggested by Singer and Willett (2003) and others to call each moment in time when data are collected a “wave.”"
  },
  {
    "objectID": "mlm.html#restructuring-and-merging-the-data-sets",
    "href": "mlm.html#restructuring-and-merging-the-data-sets",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.7 Restructuring and Merging the Data Sets",
    "text": "10.7 Restructuring and Merging the Data Sets\nIn order to merge these three sets of data in preparation for a multilevel model of change, we must convert the wide discipline.sav set to a long format and then match each set by id. Since the discipline data and the EF/grade data both have time-varying data within the teens, we will need to merge them both by id and by a wave (i.e., time) term.\nNote that I have included in BlackBoard the final data set that is created after the restructuring and mergers for your convenience. Nonetheless, it may be instructive to go through the steps that produced it."
  },
  {
    "objectID": "mlm.html#wide-to-long-and-long-to-wide-data-restructuring",
    "href": "mlm.html#wide-to-long-and-long-to-wide-data-restructuring",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.8 Wide-to-Long and Long-to-Wide Data Restructuring",
    "text": "10.8 Wide-to-Long and Long-to-Wide Data Restructuring\nConverting data from wide to long and from long to wide can be relatively easily done in SPSS. Note, however, that since SPSS uses several, sequential dialogues to prepare the final restructuring, we may not always know how it will turn out until it’s over and too late to Undo; therefore, I typically save a backup of my data before doing something like this. Here we won’t since I’ve already tested it, but I do suggest both doing that and always having an original version of all your data that you leave pristine and instead only modify a duplicate of that original data. (I also set the file name for that original version to, e.g., discipline_ORIGINAL.sav so it’s clear to me that I mustn’t mess with that file—and only make copies of it and work with those copies.)\n\n10.8.1 Wide-to-Long Data Transformation\n\nWith either tab of the Data Editor window focused on the discipline.sav data set,\nClick on Data &gt; Restructure\nWithin the Restructure dialogue, select Restructure selected variables into cases and then click Next.\nIn the next Variable to Cases: Number of Variable Groups dialogue, Under How many variable groups do you want to restructure?, select One (for example, w1, w2, and w3) and click Next again. (By selecting More than one, we could convert several different sets of time-varying variables at once)\nIn the next Variable to Cases: Select Variables dialogue, under Case Group Identification, select Use selected variable and enter id in the Variable: field that appears.\nUnder Variables to be transposed, select trans1 in the Target Variable field and replace with a new name, e.g., disc_incidents.\nSelect AY_20162017_Incidents, AY_20162017_Incidents, and AY_20162017_Incidents and move them to the field under Target Variable. and click Next.\nIn the next Variable to Cases: Create Index Variables dialogue, under How many index variables do you want to create?, select One and click Next.\nIn the Variable to Cases: Create One Index Variable dialogue, under What kind of index values?, select Variable names. Under Edit the index variable name and label, replace index1 with ay, leave Label empty (to avoid trouble with merging data sets later on), and click Next.\nIn the Variables to Cases: Options dialogue, we could set how to handle other variables in the data set that were not being restructured into long format. We don’t have any, though, so simply click Next.\nIn the Finish dialogue, select to Restructure the data now and click Finish.\nSPSS will return an error message saying Sets from the original data will still be in use in the restructured data. Open the Use Sets dialog [sic] to adjust the sets in use. This is letting you know that variables in the pre-restructuring data set (viz., id) will still be used in the newly-restructured data. This is O.K. here, so click OK.\n\nThe newly-structured data set now has three columns: one for id, one for ay, and one for disc_incidents. We restructured these data as a first step in merging them with the other, long-form data (viz., ef.sav); in the ef.sav data set, the ay variable has values of, e.g., 2010-2011, not AY_20172018_Incidents as it does here. To allow us to easily merge these two sets of data, we will now change the values in discipline.sav’s ay variable to be like those in ef.sav.\n\nClick on Transform &gt; Recode into Same Variable\nSelect ay to be added to the Variables field, and click Old and New Values\nIn the Value: field under Old Value; type (or copy and paste from here) AY_20162017_Incidents. In the Value: field under New Value, type 2016-2017, and then click Add.\nRepeat this for transforming AY_20172018_Incidents to 2017-2018 and AY_20182019_Incidents to 2018-2019, click Add after each.\nClick Continue when these three transformations have loaded into the Old --&gt; New field; Click OK when you return to the original dialogue window for this transformation.\nSave these data; we’re done with them for the moment."
  },
  {
    "objectID": "mlm.html#long-to-wide-data-restructuring",
    "href": "mlm.html#long-to-wide-data-restructuring",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.9 Long-to-Wide Data Restructuring",
    "text": "10.9 Long-to-Wide Data Restructuring\nIf you want to change data from a long format to a wide one:\n\nSince we won’t be using these wide data, we will first make a copy of them to play with. So, with the restructured discipline.sav data focused in either tab of the Data Editor window, click on Data &gt; Copy Dataset. This will create a new .sav file called something like Untitled2.sav. We will work with this data set for the rest of this example.\nWith this new set focused, click on Data &gt; Restructure\nIn the dialogue that opens, select to Restructure selected cases into variables, before clicking Next.\nAdd id to the Identifier Variable(s) field and ay to the Index Variable(s) field; click Next.\nUnder Sorting the current data? dialogue in the Cases to Variable: Sorting Data dialogue that next appears, select Yes -- data will be sorted by the identifier and index variables, and then click Next.\nIn the Cases to Variable: Options dialogue, select Group by original variable and click Finish.\nWe again get the same Use Sets error, which again is simply a warning and can be ignored here.\n\nThe incidents will now be put back into wide format, with AY 2016-2017 incidents likely called v1, etc. Voilà, we can now simply close and not save this Untitled2.sav file and move on."
  },
  {
    "objectID": "mlm.html#one-to-many-match-merge",
    "href": "mlm.html#one-to-many-match-merge",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.10 One-to-Many Match Merge",
    "text": "10.10 One-to-Many Match Merge\nThe ef.sav file contains the most rows for each student, and nearly all of the ones that the discipline.sav file would populate further, so we will merge discipline.sav into ef.sav.\n\nWith ef.sav focused, click on Data &gt; Merge Files &gt; Add Variables…\nIn the dialogue that opens, select discipline.sav under An open dataset (or select it via As external SPSS Statistics data file if it isn’t open).\nIn the Merge Method tab in the next dialogue:\n\nSelect One-to-one merge based on key values. We will match cases based on both id and ay, but here each id and ay combination is unique, so it’s a one-to-one merge.\nIn the area below that, make sure to select Sort files by key values before merging. (Both files should be sorted already as need be, but it’s always good to ensure they are since the match merge in SPSS requires this to work correctly.)\nMake sure that both id and ay are populating the Key Variables field further down; we will be merging based on both a given student’s id and by the academic year for the given number of incidents.\n\nIn the Variables tab, there are no variables to exclude, so click OK.\n\nThis should work fine, but let’s just check it to be sure:\n\nIn the ef.sav file, right-click on the disc_incidents variable and choose Descriptive Statistics from the drop-down menu. Do this for the disc_incidents variable in the discipline.sav file as well.\nIn the Output window, ensure that both instances of the disc_incidents variable have 1364 valid cases (of course, the number of invalid ones will differ) with a mean of 5.99 incidents. Checking the rest of the descriptives further supports that this process proceeded correctly.\n\nWe will now merge the demographics.sav file in with this merge set of data. The demographics.sav data set has one row for each adolescent while the ef.sav set often has more than one row for each adolescent, so we will add the information from the demographics.sav file to more than one row—a one-to-many merge.\n\nWith the merged ef.sav file focused to be the active data set, we again access the requisite dialogue via Data &gt; Merge Files &gt; Add Variables… and now select demographics.sav from the appropriate source to merge into the current data set before clicking Continue. (First, though, note which DataSet demographics.sav is labeled as since this can help later when figuring out which is which; the exact number, e.g., DataSet3 or whatever, will depend on the order they’ve all been opened15.)\nIn the Add Variables from DataSetX dialogue, under the Merge Method tab, choose for this to be a One-to-many merge based on key values since we’ll want to load the respective demographic variables into each row for a given student, regardless of which which it is.\n\nUnder Select Lookup Table, select whichever DataSet is the demographics.sav file. This should be the DataSet noted in the dialogue title, Add Variables from DataSetX, and is the one that cold have been noted in step 1 of this merge. In addition, if you indeed had ef.sav focused to be the active data set, then demographics.sav will be the DataSet not asterisked to be the active one.\nAgain ensure to Sort files by key values before merging.\nSince we’ll want to demographic values to populate each row for a given student, we only want id alone to be in the Key Variables: field.\n\nUnder the Variables tab of the Add Variables from DataSetX dialogue, we can keep all variables again. Ensure here, though, that id is listed in the Key Variables: field.\n\nIf all went well, then the ef.sav file should now have all of the data from the three data sets in long format. Let’s also now click on File &gt; Save As… and save this as, e.g., all_data_long.sav (or even Data &gt; Copy Dataset).\nWith the data in this form, we can evaluate multilevel models of change fit to these data. First, though, let’s consider other ways we could analyze longitudinal data (in wide and/or long forms) and compare their respective advantages and disadvantages."
  },
  {
    "objectID": "mlm.html#sec-condudting_mlm_spss",
    "href": "mlm.html#sec-condudting_mlm_spss",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.11 Conducting a Multilevel Model of Change",
    "text": "10.11 Conducting a Multilevel Model of Change\nI hope I’ve convinced you that MLMs are worth giving a try. We can use SPSS’s GUI for most of this, but I also hope you’ll see the advantages of using its syntax. I wouldn’t expect you to use the syntax all of the time, but there are times when it’s useful—and times when it’s needed since many of SPSS’s vast number of commands and subcommands are not accessible through the GUI. So, if you are going to expand your repertoire of analyses to improve your opportunities (and if you’re going to use SPSS), then you’ll likely eventually want to venture into its syntax.\nThis also makes my explanations of how to conduct the analyses different since there’s little need to go step-by-step through them. Instead, I will present the syntax and explain what the different parts mean.\nBut first, let’s get some familiarity with using SPSS’s syntax to do anything.\n\n10.11.1 Introduction to SPSS’s Syntax\nAs noted in Section 15.2.3.2.3, as of version 28, the syntax SPSS used to generate output is no longer presented by default in the output, but Edit &gt; Options &gt; Viewer &gt; Display commands in the log will let it be subsequently presented there. With the syntax is presented in the output16, we can copy and paste that syntax right into the Syntax window to rerun or to modify and then run.\nLet’s briefly go over the pieces of SPSS syntax through an example doing just that.\n\nWe will be using the merged, long-form data set for the rest of this chapter, so close the other sets of data.\nWith all_data_long.sav open, click on Analyze &gt; Descriptive Statistics &gt; Descriptives…\nAdd stem_grades and all of the executive functioning scores to the Variables(s) field and click OK\nRight above the tables of Descriptives in the Output window is the syntax SPSS used to compute those descriptive stats. You can see in the thin navigation pane to the left of the output, the syntax is labeled as Log; clicking on the icon just next to Log () will take you to that syntax. We can double-left click into that box of syntax in the output window to copy the syntax; we could then click File &gt; New &gt; Syntax to open the Syntax window, and paste that syntax into it—but don’t do this.\nInstead, we can also skip all of that: Again click on Analyze &gt; Descriptive Statistics &gt; Descriptives…. The same set of variables should still be in the Variable(s) field (and all options, etc. the same, too). With the same variables, etc. chosen, instead of clicking on OK, click on the Paste button right beside of it. This will open up the Syntax Editor window with the syntax already pasted into it (or appended to any syntax we already had in that window).\nFor the Syntax Editor window, we can ran this command, but let’s first look it over\n\nSPSS syntax usually start with a procedure, which is essentially the verb of the syntax “sentence:” It tells SPSS what sort of analysis or command you will be executing. This procedure is presented in blue text in the Syntax Editor window. The syntax we pasted in has two procedures: a DATASET ACTIVATE17 procedure which tells SPSS which data set we wanted to run the subsequent procedure(s) on and a DESCRIPTIVES procedures that is followed by a bunch of stuff I’ll explain next.\nImmediately following the DESCRIPTIVES procedure name is VARIABLES in red. This is what SPSS calls an option keyword. As you can tell, in this case, it indicates which variables the DESCRIPTIVES procedure should be run on. Immediately after this is an equals sign followed by the variables in our data set that this VARIABLES option is defining.\nOn the next line, we see /STATISTICS in green. This is called a “statement,” or subcommand of DESCRIPTIVES. In any case, these subcommands are usually specific or additional analyses to be run, and they are always preceded by a forward slash. Here, it indicates which of types of descriptive stats to run—each statistic itself being an option.\nOne more thing to note is that SPSS procedures not only always start with a procedure subcommand (here, of course, that’s DESCRIPTIVES), but they also always end with a period.\n\nI’m sure that was very edifying, so let’s now build on that with more, interesting insights into the syntax:\n\nWe can, well, edit the syntax in the Syntax Editor. In fact, we could have typed it all out, or—as we will do later—copy and paste syntax from other files.\nLet us add another subcommand to the DESCRIPTIVES procedure. Left-click into the field in the Syntax Editor right before the forward slash in the /STATISTICS subcommand and hit the return key on your keyboard.\nIn the empty line above the /STATISTICS subcommand, type /SAVE18.\nThis is a rather cryptic subcommand, so let’s further modify the syntax. Right after the /SAVE subcommand (on the same line), type19:\n\n/*This subcommand generates standardized variables for all the variables included in the VARIABLES option.\n\nThis is a comment, a piece of text that SPSS ignores but that we can use to help us understand what something means (like we did here) or why we’re doing something so that we can reorient ourselves if we come back to it later. Note that this comment starts with a forward slash and then an asterisk—and that it ends with a period; for in-line comments like this, we need all three punctuation marks.\n\nWe could save this modified syntax if we wanted (Ctrl + S20, File &gt; Save, or the floppy disk icon ). SPSS syntax is saved with the .sps extension, but—unlike .sav files—they can be opened by other programs, like text editors (Notepad++, Atom, Kate, Vim, Emacs, etc.)\n\nOf course, we can also run it. To run all of the syntax in a window, click the green “play” button () or press Ctrl + R. This will run the procedure(s) that are highlighted or the procedure in which the cursor is currently placed. We could also click Run &gt; All to run all of the syntax in that window.\n\n\nBeing able to save/rerun analyses and to insert comments into syntax are perhaps the main reasons to use it for procedures that one could otherwise access from the GUI menus. As I’ve noted before, I recommend keeping a sort of journal21 of what you did. You can do this by adding text and notes directly into the Output produced, of course, but you may also want to clean up that output to be just the analyses that mattered—not, e.g., all the exploratory things you did. One way to create a simple and replicable history of your analyses is through a commented series of syntax-only procedures.\nAs I noted above, an other reason to use SPSS’s syntax is that not all procedures or options are available through it, including those to fully conduct MLMs. Now, some procedures herein are accessible via the GUI, but since many are not it seems awkward to switch back and forth; it also undermines the purpose of demonstrating using SPSS’s syntax interface.\nIn addition, you needn’t rely just on this chapter for help conducting MLMs in SPSS. All of the syntax I’m using based on that presented in the companion website to Singer and Willett’s book. Although I modified what they give there, this means you can use the excerpts from the book I provided along with that site (and this chapter) to have what I hope is a strong foundation in MLMs—analyses I am a clear advocate of.\n\n\n10.11.2 Overview of Analyses\nWe will investigate how well economic distress and other demographic variables predict the development of these teens’ executive functioning. For these analyses, let us assume that the variable of interest to us—say the one that addresses one of our research questions/hypotheses—is economic distress and that all of the other demographic variables are simply things we want to partial out. For example, our hypotheses could be that:\n\nAdolescents experiencing economic distress will self report fewer, initial executive-functioning-related behaviors in sixth grade than peers not experiencing this distress; those experiencing it will also self report relatively weaker subsequent gains in executive functioning throughout middle and high school.\n\n\nIn addition, economic distress will significantly improve predictions of executive functioning made by other, apposite demographic factors, namely gender, race/ethnicity, and whether the participant has an IEP.\n\nEarlier in our report, we could discuss how economic distress is often associated with having an IEP and race/ethnicity but that we believe that economic distress affects executive functioning in ways that are independent of these factors. We could also discuss how gender is often found to affect the development of executive functioning, and so it should be accounted for as well (nearly as a delimitation).\nNote that the above hypotheses do not posit any interactions between the factors. We proposed that the effects of economic distress on executive functioning will be at least partially independent of the effects of demographics—that the effect of economic distress on executive functioning is not only through demographics. However, we did not postulate that that one’s demographics will either amplify or reduce the effect of economic distress. We indeed could also test hypotheses about whether the factors do interact; however, we won’t until after we’ve gotten the basics straight.\nI will follow the general sequence of analyses recommended by Singer and Willett beginning on page 92. Therefore, first we will investigate whether there are differences in these teens’ executive functioning that warrants further investigation—that there are differences in their initial levels of executive functioning. Second, we will test whether there are significant changes in these initial levels—that there is indeed a reason to investigate changes over time. Third, if both these preconditions are met, we will compute a base model that contains only the demographic variables we wish to control for. Fourth and finally, we will add economic distress to that base model to see if this final model is a better predictor of executive functioning than the base model.\n\n\n10.11.3 Computing the Unconditional Means Model\nIn an ANOVA, we first run an omnibus test of a factor to see whether additional tests of it are warranted. Singer and Willett recommend taking a similar tack with MLMs; they suggest first conducting a pair of analyses to test whether we have sufficient reason to look further at the data. One analysis of the pair tests for differences in initial levels of the outcome; the other of the pair tests for changes over time. Remember that in MLMs, we retain two pieces of information about the time-varying Outcome: its intercept (or initial value) and its slope (or subsequent changes over time); this pair of tests thus test if we need to indeed include both pieces of information in our model or if instead a simpler model that excludes one of these is recommended.\nNote that I have never seen member of this pair reported in published article. This may be in part my limited exposure to their uses, but it still doesn’t make their reporting common. I agree that it’s good practice to compute them, but it’s not conventional to then also report them.\nWith that in mind, let’s run the model and use it to further our understanding of MLMs.\n\nIn SPSS, click File &gt; New &gt; Syntax\nPaste in the following syntax into the main field in the window that opens:\nDATASET ACTIVATE all_ef_long.\nTITLE \"Unconditional Means Model, p. 92\".\nMIXED global.exec.comp.sr\n/PRINT=SOLUTION\n/METHOD=ml\n/FIXED=intercept \n/RANDOM intercept | SUBJECT(id) COVTYPE(un).\nNote that this and all of the syntax below are available in the mlm_syntax.sps file in BlackBoard.\nSelect the entire set of syntax (e.g., with Ctrl + A or by highlighting it with your mouse) and then either type Ctrl + R or click the green “play” button ().\n\n\n\n10.11.4 Interpreting the Syntax\nThis will generate a set of output but let’s first go through this syntax to understand better what it’s doing:\n\nDATASET ACTIVATE all_ef_long.\n\nThis simply ensures that the all_ef_long.sav data set is the “active” one that will be used for any further work. It will remain the active data set until we either run another such command targeting another data set or we click on the Data Viewer window of another data set.\n\nTITLE \"Unconditional Means Model, p. 92\".\n\nThis simply outputs a title before the results echoing what’s given in the quotes. This just helps to keep the output straight.\n\nMIXED global.exec.comp.sr\n\nThe MIXED procedure declares that we are conducting a linear mixed model, which is another name for multilevel models. All linear mixed models / MLMs contain both fixed and random effects—this “mixing” of fixed and random effects is where they get their name.\nWe also declare here what our outcome (criterion) for this model is. (We’ll add more to this line later.) Here, we’re using the global executive composite score from the BRIEF-SR, the instrument competed by the adolescents about themselves. This score includes all of the various subscores for different, specific executive functions.\n\n/PRINT=SOLUTION\n\nThe /PRINT subcommand allows us to add yet more output to that which SPSS already spews. Here, we’re asking that the parameter estimates for the model be printed. Note that since this is a simple with just the intercept (more on that in a second), there isn’t much extra printed; this will be more helpful for later models.\n\n/METHOD=ml\n\nWith the ml option, the /METHOD subcommand requests that we use full maximum likelihood estimation. As Singer and Willett explain, full maximum likelihood estimation creates estimates for all of the parameters in the model—both the predictors (or “structural”) and error/residual (or “stochastic”) parts. This seems like a good idea—and is the choice one nearly always makes—but this method tends to over-estimate the certainty of our predictor estimates, making their standard errors a bit too small and thus our confidence in any effects we find for the predictors a bit too high. In other words, using full maximum likelihood estimation increases our chances of Type 1 (false positive) errors. This over-confidence in effects of predictors reduces as the sample size increases.\nThe subcommand could use the reml option instead of the ml option. reml tells SPSS to instead compute restricted maximum likelihood estimation. Restricted maximum likelihood attempts to minimize the unexplained (residual) variance—that which is consigned to what I’ve been calling the error terms, and that Singer and Willett (rightly) called the stochastic22 terms. This is appropriate for tests of the error terms, but we will not be doing those here—and you will likely rarely do them. Therefore, I recommend using the ml option while also striving to ensure that your sample sizes are “big enough” (say a couple hundred) and that you are cautious with p-values that are just on the significant side of .05.\n\nreml is also the default option for SPSS, so it will be used if we do not specify otherwise. Again, I suggest over-riding this default unless there is reason not to.\n\n\n/FIXED=intercept\n\nThe /FIXED subcommand is the first of two subcommands in which we specify the terms in our model (/RANDOM is the second). In statistical models, fixed terms are those in which all levels of that variable are present in the model. For example, if a treatment variable can only either be “experimental” or “control” (and we include both levels in our model), then that term is considered “fixed.” Binary gender, dummy variables, and most other nominal variables are fixed.\n\nThe are called “fixed” because we are only interested in the levels that are actually presented in our model—i.e., we are not trying to make assumptions about other values not presented in the model—and so our parameter estimates are unchanging, or “fixed,” to those particular values.\n\nFixed effects differ from random terms. Random terms are those in which not all levels of that variable are present. Sure, this could be, e.g., if you believe not all types of race/ethnicity are present, but in fact cases like that are typically relegated to being fixed terms anyway, the missing levels being ignored. Instead, random terms are essentially any variable that’s defensibly interval/ratio., i.e., a variable that has a continuum of values—like height or even time, both of which can be measured to an infinite number of decimals.\n\nThey are called “random” because we are assuming that the actual values we have in our data set were chosen at random from a huge pool of possible values (i.e., we sampled from a larger population of values).\nWhat about Likert-scaled variables? One usually assumes that they are random variables unless there are, say only three or four possible levels, in which case we may want to consider treating them as a series of nominal variables. (The lesson here is to try not to have Likert responses that only have a few levels.)\n\n\n/RANDOM intercept | SUBJECT(id) COVTYPE(un).\n\nFinally, the /RANDOM subcommand does double duty in MLMs of change. First, it includes—get this—random terms. However, there are two important options added to the /RANDOM subcommand after the bar (“|”): SUBJECT and the COVTYPE.\nThe SUBJECT option indicates which variable denotes the level of the participant. It also indicates what other variables are nested under participant by having that nested variable given right before the bar. Right now, this doesn’t make much sense (sorry), but it will in subsequent models we specify, so please just keep this in mind for later.\nCOVTYPE defines the covariance structure of the model. The covariance structure is nearly always the covariance matrix, so we can generally assume that when SPSS says “covariance structure,” it’s referring to the covariance matrix.\n\nI’ve not emphasized covariance matrices in class, but they hold a central role most much of what we’ve been talking about. You will remember that the covariance matrix of a list of variables is the unstandardized correlation matrix. When we compute linear regression models like we’re doing here, what we’re in fact doing is trying to maximize the chance that the values in a covariance matrix of the parameters are the correct values for those interrelationships.\nThere are several types of possible structures that SPSS can use; some of these are quite useful indeed, but all are beyond the pale of our needs here: Simply use the un (“unstructured”) type here.\n\n\n\n\n\n10.11.5 Interpreting the Results\nAfter some information about when and from where the analyses were run, SPSS first generates an error message:\n  ---------------------------------------------------------------------------------------------\n  The covariance structure for random effect with only one level will be changed to Identity.\n  ---------------------------------------------------------------------------------------------\nAn “identity” in mathematics is a term that does not change the value of an other term. For example, if I multiply a number by 1, then it stays the same number; in this case, 1 is an identity. A matrix is an identity matrix if it would not change the values in an other matrix when those two matrices are multiplied together; in practice, an identity matrix simply has ones in the diagonal and zeros in the places off the diagonal, like this:\n\\[\\text{A 3} \\times \\text{3 identity matrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1  \\end{bmatrix}\\]\nThere is only one intercept for each adolescent (id), and there are no other terms in the /RANDOM subcommand. If I created a covariance matrix of the values of the intercepts for the ids, then the diagonal would be ones—since the intercept for each adolescent shares all of its covariance with itself (just like an item is perfectly correlated with itself)–and the off-diagonal values would all be zeros—since we are assuming that the intercepts for the adolescents are all independent of each other. This would create an identity matrix.\nSo, all this error message is saying is that. That since we only have one /RANDOM subcommand term for each adolescent, we’re assuming that the intercepts are all independent of each other^[All of that just to say, yeah, you can ignore the error message..\nThe next table further exemplifies this. It shows that the only term in the model is the intercept, and that this creates an identity matrix (Covariance Structure) among the participants (the ids in the Subject Variables):\n\nYou may well be confused that the intercept term is being considered as both a fixed and an random effect—or simply that it should even be considered as a fixed effect since global.exec.comp.sr scores can clearly take on values not measured here (and thus is indeed a random variable in this sense). Indeed, we are forcing the model to do things it’s not really designed to do here^[And so starting our understanding of MLMs with an aberrant model isn’t perfect.. We will only use this model for one purpose, though, and it does meet that one goal. So, please otherwise ignore this odd structure.\n\nInformation Criteria\nThe two pieces of information worth noting are given in the next two tables. I will explain them in some detail now, but will not emphasize them in most of our analyses, instead concentrating on interpretations that should feel more familiar.\nThe first table presented after the Model Dimension table provides the Information Criteria for the model. Information criteria are important concepts and statistics in models. They represent the amount of information left in the data that is not well accounted for by the model. Yes, it’s the same concept as the error sum of squares in an ANOVA. Similarly, we are trying to minimize this residual—this unexplained variance—so a core goals here is to minimize the amount of information left in the data unexplained by the model.\nThe first of these information criteria is the -2 Log Likelihood (-2LL). O.K., so what is that? Let me give some background to lead into what it is and what it’s used for. The log likelihood itself is computed as part of the maximum likelihood estimation of how well the model fits the data. Again, given the model we propose, we seek to find the values for the parameters (i.e., the value of the intercept, the b- or β-weights for the factors, etc.) that are most likely given the data; we seek to maximize the chance that those parameters are correct. Without going far into the math of it, we could compute the parameter values by taking the least squares approach for each parameter and then compute the joint probability of getting those parameters (and doing this again and again, tweaking our values to try to make them better and better). However, computing that joint probability can be quite intensive—even for computers. We can loose no precision in our computations but make the math much easier if we instead try to compute the logarithm of those joint probabilities^[This is because we could have to multiply the joint probabilities but would only have to add the logs of those probabilities. Adding is less resource-intensive than multiplying.. So, we actually compute the log likelihood instead.\nAgain, we try to maximize the likelihood of getting those particular parameter values. Similarly, we try to maximize the log of the likelihood (of getting those values). Given how the math works, though, the log likelihood is (nearly always) a negative number. We maximize it by trying to get it out of negative territory—by having it get as close to zero as we can. So, the log likelihood is a negative number, and values closer to zero are better.\nHowever, we multiply the log likelihood by -2 because that transforms it into a value that follows a χ2 distribution (so for all of the values in this table, SPSS is right: The information criteria are displayed in smaller-is-better form). This is where the statistics here start to get useful. Remember that the difference between two χ2 values also follows a χ2 distribution. Therefore, we can take the difference between two -2LLs and conduct a χ2-test on that difference to see if that difference score is itself significant23.\nThis means that we can take the -2LL computed for one model, subtract it from the -2LL computed from an other model, and then test if those -2LLs are significantly different from each other. In other words, if one of those two models fits the data significantly better than the other.\nMany researchers, including Singer and Willett, call the -2LL the deviance statistic. This is really the same value (i.e., the difference between a true -2LL and a deviance statistics can be ignored). Although you will read about deviance statistics more often than “-2LLs,” I will continue to refer to them as -2LLs so it’s clearer what part of the SPSS output I’m referring to.\nNow, as useful as deviance statistics / -2LLs are, there are clear limits to when they can be used. The main limitation is that one can only meaningful compare two models that are computed from the same set of data. Even removing a few cases from a set of data (e.g., by subsetting the data or filter out certain cases), we disqualify tests between them. Yes, this is even a problem if a have missing data that change the sample size between models, so one should take care about that, either removing cases listwise or by imputing values for any missing data.\nSinger and Willett also discuss how one of the two models we compare should only contain a subset of the variables of the other model. In other words, one model contains, e.g., gender, race/ethnicity, and IEP status, then we can only compare that to models that either also contain those three variables plus other variables (e.g., gender, race/ethnicity, IEP status, and executive functioning) or compare it to another model that has a subset of those variables (e.g., gender and race/ethnicity). Confusingly, a model that contains a subset of variables from an other model is said to be nested in that larger model. So, succinctly, Singer and Willett argue that we can only use -2LLs to compare models if one is nested within the other.\n\nTo compare non-nested models, we use either Akaike's Information Criterion (AIC) or Schwarz's Bayesian Criterion (BIC). We implement either the AIC or BIC the same way we do a deviance statistics: We take the difference between two AICs or between two BICs and see if that difference is significant.\nBoth the AIC and BIC are based on the -2LL. The AIC penalizes (makes the absolute value greater) the -2LL for each factor in a model so that more complex models can’t just capitalize on chance. The BIC not only penalizes for the number of factors but also for sample size, so we can’t also get better models just by having more data.\nYou will find either statistic reported in various articles, and either is a fine choice—unless your sample size is so large the BIC becomes necessary. Nonetheless, I tend to use BIC since that is both appropriate even for smaller samples (it simply penalizes them less) and since it is indeed more conservative.\nLike the BIC, the Hurvich and Tsai’s Criterion (AICC)—more properly written AICC—also attempts to compensate for sample size. The AICC, however, is used for small sample sizes where models may not be able to fully adjust to fit the data.\nBozdogan's Criterion (CAIC) is similar to BIC. It has a stronger penalty for the number of parameters than both the BIC or AIC, but tends to converge on the same values as are computed by the BIC. Given that the BIC is more commonly reported, I think it’s fine to stick with the BIC.\nUsing information criteria to test models is flexible, but we will focus instead here on the more familiar F- and t-tests.\n\n\nFixed Effects\nThe next set of tables are a lot easier to understand. The Type III Test of Fixed Effects presents the F-score and dfs testing the significance of the fixed effects in the model. In this unconditional means model, the only term is the intercept, so all we’re testing here is whether this value differs significantly from zero.\n\nAnd with an F-score of 27634.2, I’d say it probably does.\n\nThe Estimate of Fixed Effects table reproduces this test (a t-score of 166.235 is equivalent to an F-score of 27634.19 since 166.2352 = 27634.19). These two tables are redundant because Intercept only has one degree of freedom here. Had we included an interaction term or a term with more than one degree of freedom (e.g., race/ethnicity as a nominal variable with teens coded as “Asian-American,” “African-American,” “European-American,” etc,), then the first Type III Tests table would give us more information about those categories.\nThis table also presents the Estimate of the global.exec.comp.sr term: 152.90; this is simply the mean score for the entire sample collapsed across waves.\n\n\nCovariance Parameters\nThe covariance parameters are not interesting in this model since they are redundant with the fixed effects.\n\n\nSummary\nAgain, one reason to compute the unconditional mean model are to test if there is an effect overall in the outcome, thus allowing ourselves to further test for effects of time and various predictors. This model also gives us a -2LL against which we can compare subsequent models, allowing us to see if predictors added to it can improve upon the prediction we would make with just knowing the overall mean for this sample of teens. You may remember we talked about the effects of predictors with just this interpretation of them.\n\n\n\n10.11.6 Computing the Unconditional Growth Model\nRemember that the unconditional growth model tests whether the outcome, here global.exec.comp.sr, changes over time, thus warranting further investigations into what other factor may predict there changes. Its goal is thus simple—and so can be our interpretation of it.\nTo compute this model:\n\nBack in the Syntax window, paste in the following syntax:\nTITLE \"Unconditional Growth Model, p. 97\".\nMIXED global.exec.comp.sr with wave\n/PRINT=SOLUTION\n/METHOD=ml\n/FIXED=wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\nNote that you can simply paste this in below the syntax for the unconditional mean model.\nHighlight this syntax and run this syntax. If you’re lazy (or just already fighting off the repetitive stress syndrome you’ll get from all the typing you’ll be doing) instead of highlighting it, you can simply press Ctrl + A and then Ctrl + R to run all of the commands in the mlm_syntax.sps file—another advantage of using syntax and generally learning to use keyboard shortcuts instead of a GUI. (Note you can also simply run a given command when the cursor is placed anywhere within the given command.)\n\n\n\n10.11.7 Interpreting the Syntax\nThere are only a few differences in the syntax, but these are profound.\n\nThe statement is not followed not only by the Outcome (global.exec.comp.sr) but then also by the with wave statement. The MIXED statement now includes an actual model. Models for SPSS syntax are written so that the outcome is predicted with a set of factors. Here, we are only including the wave term, so that alone is added to where the predictors are expected. I need to point out another peculiarity of SPSS’s syntax here. In the MIXED command, SPSS interprets any factors that follow with to be random (i.e., continuous) effects. As we will see later, fixed effects are indicated slightly differently.\nThe /FIXED subcommand lets SPSS know which of the predictors in the model given in the MIXED command are to be considered fixed factors. Yeah, wave is not a fixed factor—and we would normally not include it here—but we need to have at least one fixed factor, so we add that.\nIn the /RANDOM subcommand, we again add wave. It will stay here in subsequent models since it is a random factor—as most all measures of time should be.\n\n\n\n10.11.8 Interpreting the Results\nThe Model Dimension table reviews the terms in our model, showing that we now have a wave term added (temporarily as both a fixed and random effect). Note that SPSS indicates that the Number of Levels for the wave term is 1; this will come into play next.\n\nInformation Criteria\nThe current (unconditional growth) model differs from the previous (unconditional means) model only in that we added a term for wave. This means that the previous model is nested within the current one, and so we can use the -2LLs to compare the relative fits of these tow models to the data.\n\nThe -2LL for the unconditional means model was 28573.945; for the new, unconditional growth model, it is 28316.912. 28573.945 − 28316.912 = 257.033. Therefore, the χ2 score for a test between these two models is 257.03. The Number of Levels for the term that was added to this model (compared to the previous, unconditional means model) is 1; this is also the degrees of freedom for this χ2 score. So, our tests here is of χ2 = 257.03, df = 1. The critical value when df = 1 and a two-tailed α = .05 is χ2 = 5.02. We could justifiably use a one-tailed test since we could argue we’re testing if the χ2 is greater than zero (and where the critical χ2 = 3.84), but we’ll stick with the two-tailed test.\nWe can therefore conclude that adding a wave term to the model significant; improves the fit of our model to the data. In other words, we are justified to look further at the ways in which executive functions change here because their changes over time account for a significant portion of the variance in these data.\n\n\nFixed Effects\nWe do indeed get a similar set of information for the fixed effects tables, where the wave term is significant (F1, 654.7 = 211.33, p &lt; .001). On average—and across all participants—the global.exec.comp.sr scores change only a little bit: 0.01 points per wave, which is not much for scores that average at 126.6. Nonetheless, these changes are significant and account for a meaningful amount of information herein.\n\n\n\n\nCovariance Parameters\nThe covariance parameters table now includes some more information. The Residual in this table reports statistics related to the variance within each participant. The Estimate for the residual variance is quite high, indicating that a lot of variance is still unexplained. This is equivalent to Singer and Willett’s level 1 model residual. Frankly, this Residual Estimate isn’t commonly reported and is only marginally informative for one’s in-house analyses. It does convey the amount of unexplained variance left after the given model, but so do the information criteria, and those are presented in a format that is useful across models conducted on the same data in ways that are more amenable to further analyses.\n\nThe three rows within the Intercept + wave [subject = id] section provide parameter estimates (and standard errors for those estimates) for terms that are between participants. These are terms for random effects that are part of Singer and Willett’s level 2 model. The three terms reported herein are the variance of the intercept (UN (1.1)), covariance between the intercept and wave terms (UN (2.1)), and variance of the wave term (UN (2.2)). These terms are useful to investigate how much of the residual variance remains in the various areas, but—again—further understanding of the relationships are probably better studied through careful analyses of the predictors and perhaps graphs of the residuals.\nOne thing to point out from these level-2 variances is that the covariance between the intercepts and changes over waves is “borderline” significant (p = .059). This suggests that there may not be a significant relationship between adolescents’ initial levels of executive functioning and subsequent changes in it.\nWe can see from these residuals that, although intercept remains highly predictive, there is still much we don’t understand about what affects that values. This does nonetheless reflect a large body of research indicating that much of one’s executive functioning is determined during childhood.\n\n\nSummary\nThe unconditional change model is the second of two models that simply help establish the foundation upon which other models can be built and against which they can be compared. We do see through these two models that there are non-negligible inter-individual differences and that executive functioning does appreciably change over these administrations."
  },
  {
    "objectID": "mlm.html#univariate-mlms",
    "href": "mlm.html#univariate-mlms",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.12 Univariate MLMs",
    "text": "10.12 Univariate MLMs\nWe will next review a series—or “taxonomy”—of models each that contains one predictor (in addition to those in the unconditional models). Even with this rather limited set of variables, there are many comparisons we could make. However, we’ll keep it simple and look at the relative contributions of only three predictors on global.exec.comp.sr: gender, iep.status, and economic.distress. We will presume that the variable of especial interest here is economic.distress and whether it makes a unique contribution beyond that already made by gender and iep.status.\nTo investigate these three predictors, we will first look at the relationship of each one with global.exec.comp.sr alone, i.e., without the other two predictors in the model. We will also look at the relationship between each predictor and the adolescents’ initial levels of executive functioning as well as the relationship with subsequent changes in executive functioning. Each of the univariate models therefore looks at the contribution of the given predictor without consideration of that predictor’s relationship (correlation) with any of the other predictors.\nThese univariate models are also themselves “prefatory” ones that simply help lay the groundwork for the final few models that we would likely actually report in an manuscript. We are, of course, looking here at a few pieces of what is likely a very tangled web of influences on the lives and development of these teens. In order to understand any of it we are well-advised to first try to look at the pieces in relative isolation before considering how they may interact. We will do that now.\n\n10.12.1 Gender\n\nMain Effect\nWe will first look at whether boys’ and girls’ initial global.exec.comp.sr scores differ.\n\nVia SPSS’s GUI\nWe can—and will—do this using the syntax, but the model is complex enough to serve as an example for how to do this using the GUI:\nPlease note that we can run a similar model via the GUI:\n\nClick on Analyze &gt; Mixed Models &gt; ``Linear...\nAdd id to the Subjects: field, wave to the Repeated: field, and change the Repeated Covariance Type: to Unstructured\nClick Continue at the bottom\nIn the next dialogue, add global.exec.comp.sr to the Dependent Variable: field, gender to the Factor(s): field, and wave to the Covariates: field\n\nUnder Fixed..., change the middle button’s choice to Main Effects, add gender to the Model:, and make sure Include intercept is selected (and that the Sum of squares is Type III). Adding the intercept here lets each participant and each gender to have their own beginning value.\nUnder Random..., leave/make the middle button selection as after selecting Factorial and add wave to the Model: field.\nUnder Estimation..., chose Maximum Likelihood (ML) under Method and leave all other values at their defaults.\nUnder Statistics..., in Model Statistics, select Parameter estimates for fixed effects and Tests for covariance parameters.\nUnder EM Means..., add gender to the Display means for: field.\nFinally, click OK to run.\n\n\n\n\nVia Syntax\nWe do this by adding only a gender term to the model. To do this, run the following syntax:\nTITLE \"Gender Main Effect\".\nMIXED global.exec.comp.sr with wave by gender\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(gender)\n/METHOD=ml\n/FIXED=gender\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Syntax\nThe lines that differ are MIXED and /FIXED. We’ve also added a /EMMEANS=TABLES(gender) subcommand.\nThe MIXED command again contains the Outcome, but now followed by both a with and a by argument24. The with argument denotes the factors that should be considered as random effects (here, wave) while the by argument denotes factors that should be considered as fixed effects, like gender. Sure, gender could be seen as not measuring all possible levels of that domain, but we’ll treat is as fixed since that’s how it’s measured by the school.\nIn the /FIXED subcommand, we are also just adding a “main effect” of gender. This will become clearer in the next model, but for now know that we are only looking at the effect of gender on the initial (intercept) Outcome scores.\nThe /RANDOM subcommand has remained the same. We are again noting that wave is a random factor and that it is to be considered nested within id.\nThe /EMMEANS=TABLES(gender) subcommand requests that SPSS print a table of the estimated marginal means for the predictor(s) along with their standard errors and 95% confidence intervals.\nThe estimated marginal means are simply the means for the various levels of the given predictors that are predicted by the model (i.e., not their actual means), after partialing out the effects of any other terms. We only have one predictor in the current, so we are only partialling out the intercept here.\n\n\nInterpreting the Results\nAt last we are looking at theoretically-interesting results. The point of the current analysis is to investigate the relationship between the adolescents’ gender and their executive functioning.\nWe can look at it here in two ways: the significance of gender’s model term and at the change in model fit when gender is added to the previous model. We will focus on the former, looking at the significance of the gender term itself.\nThe Model Dimension table summarizes the model variables, covariance structure, degrees of freedom, and which variables are nested within which:\n\nIn the Number of Parameters column, we can see in that table that the nominal gender variable contains two levels (male and female). Note, though, that in the Number of Parameters column, there is only one “parameter”; this is the number of degrees of freedom used to add this term to the model. The number of parameters will always be one less than the number of levels for that term.\nNote, too, that both the Intercept and gender terms are listed under Fixed Effects. Again, SPSS considers all fixed effects to be between participants (in Singer and Willett’s level 2 model). wave remains under the Random Effects; SPSS considers all random effects to be within participants.\n\n\nFixed Effects\nIntentionally, we have only one fixed effect added to the model, allowing us to investigate that effect in isolation. The effect we’re testing is whether girls and boys began this study (in sixth grade) with different levels of executive functioning.\n\nThe Type III Tests of Fixed Effects table also tests the effect of gender. This test is tantamount that one would compute in an ANOVA—although here we are using maximum likelihood estimation25—so this F-score tests for mean difference between the gender groups. This F-score is not significant (F1,903.8 = 3.05, p = .081). There is insufficient evidence that boys and girls began this study in sixth grade self-reporting different levels of executive functioning.\n\nThe Estimates of Fixed Effects table that comes next reflects the same results as the estimated marginal means because the predictor has only two levels. Note that there is an oddity to this table: The gender term is divided into two rows, with values given for [gender=0] but not [gender=1]26. SPSS is assuming that the highest category for this predictor (when gender = 1) is the reference group—the level against which the other category is compared. So, the test here is whether those participants whose gender = 0 (boys) have significantly different mean executive functioning scores than the reference group (girls).\nThe Estimates of Fixed Effects table thus indicates that boys’ scores are 3.45 points lower than girls’ scores. This value of -3.45 is also the β-weight for the gender term (sigh, when girls are defined as the reference group; were I to report this, I would reverse the sign to meet readers’ expectations).\nOne more thing to say about comparisons like the levels of gender in the Estimates of Fixed Effects table: SPSS always assumes the highest level is the reference group. Or at least that I don’t know how to change it away from that. This is odd and inconvenient since dummy variables are designed to have the group that’s zeros be the reference group.\nThe Estimates of Covariance Parameters table still presents the within-participant residual variance in the first row and variances for the intercept, intercept \\(\\times\\) slope interaction, and slope per se, respectively. We needn’t reproduce that table here not consider it further in this context.\n\nA new table has appeared after that Covariance table, here called simply Gender. These are the estimated marginal means requested by the /EMMEANS=TABLES(gender) subcommand that we added to this analysis. Again, this provides the initial global.exec.comp.sr scores the model estimates boys and girls had at the time defined as crossing the intercept. We therefore estimate that boys’ initial scores were 150.558 and girls’ were 154.011. Note that 150.558 – 154.011 = -3.45, which is the Estimates value for boys ([gender=0]) in the Estimates of Fixed Effects table.\n\n\nSummary\nA multilevel model of change did not find that gender significantly predicted under-served adolescent students’ initial levels of self-reported executive functioning (F~1, 907.8~ = 3.05, p = .081) when that term was included with no other predictors except terms for the intercept and time. But we are not done with gender yet. We did not find that girls and boys started middle school with different levels of executive functioning, but this does not mean that they will not change as time goes by. Let us now look into that.\n\n\n\n\nGender × Time Interaction\nA strategy for investigating factors in this fashion includes one piece of information at a time to see what that adds to our understanding: We proceed in careful, precise steps to ensure accurate understanding before building those pieces together into a larger picture. Therefore, we will look now only at how gender may affect changes in executive functioning over time—independent of any effect gender (could have) had on the initial levels of executive functioning.\nTo do this, we simply add only a gender × wave interaction term to the “null” comparison model that contains only intercept and wave terms with the following syntax:\nTITLE \"Gender Interaction\".\nMIXED global.exec.comp.sr with wave by gender\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(gender*wave)\n/METHOD=ml\n/FIXED=gender*wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Syntax\nThe only difference between this model and the one we analyzed in the Gender Main Effect section is that the /FIXED=gender*wave line where we have a gender × wave interaction term added instead of a gender main effect term.\n\n\nInterpreting the Results\n\nInformation Criteria\nWe will not review the information criteria for this model. Neither it nor the previous model containing the gender main effect are nested within each other. We could use AIC (or BIC) here to compare whether gender predicting intercept accounts for more information in the data than gender predicting changes in executive functioning, but this is not of great interest here. In addition, the test of the term itself suffices to study its effects.\n\n\nFixed Effects\n\nThe gender × wave interaction is significant(F2, 649.6 = 104.93, p &lt; .001). Note that this is an interaction between a variable nested within participant (wave) and an other variable that is between participants (gender), but this still appears in the Fixed Effects tables. It is interpreted straight forwardly that the slopes of the executive functioning scores is significantly different for the boys and girls.\n\nThe Estimates of Fixed Effects table gives us insight into the nature of this interaction: On average, boys’ executive functioning scores increase 0.011 points per wave (here, that’s essentially per academic year) while girls’ scores increase slightly more, 0.013. Both of these increases—small as they are—are significant here. The standard errors help explain why: There is not much variance in the rates of change, so even small effects are detectable.\nNote that the Estimates of Fixed Effects table presents results for analyses of the individual levels of the gender \\(\\times\\) wave interaction. We are essentially conducting post hoc comparisons of the levels to see if both are significant. If the F-test given in the Type III Tests of Fixed Effects table was not significant, then these t-tests of the levels are not warranted. SPSS would report them anyway, though, so only interpret the level tests if the F-test is first significant.\n\n\nCovariance Parameters\n\nAn additional piece of information become interesting in the Estimates of Covariance Parameters table. Remember that the UN(2,1) row presents the covariance between the intercept and time terms. Using a Wald test, this relationship is significant (W = 4.28, SE &lt; 0.001, p &lt; .001) and positive (b = 0.00012), if rather small. This suggests that teens with larger executive functioning scores tended to have (slightly) more positive slopes than teens with smaller scores.\n\n\nSummary\nThe rather small variance in the rates of change in executive functioning allowed us to investigate their relationships with gender in some detail. We found that overall adolescents with grater executive functioning scores tended to have score that got even greater throughout their middle and high school grades while adolescents with lower scores tended to have scores that further decreased: To a small but significant degree, small initial differences in executive functioning tended to grow in difference.\nWe also found that girls’ executive functioning scores tended to become slightly greater over time. These executive functioning scores are coded so that lower scores denote stronger executive functioning, so—surprisingly—girls tended to show a greater reduction in their self-reported executive functioning relative to boys. Findings that disconfirm our expectations tend to garner ore attention (or, well, they should), so it may be worth, e.g., looking at comparable executive functioning scores reported by teachers about these same students; perhaps the effect here is not replicated in how some others view these teens’ executive-functioning-related behaviors. Perhaps instead the changes in self-reported scores are more related to changes in, e.g., one’s self confidence.\n\n\n\n\n\n10.12.2 Special Education\nWe next next review a similar pair of models to investigate the effects of IEP status on both initial executive functioning levels and subsequent changes thereof. Just as a reminder, an IEP is an “individualized education program” (or “plan”) designed to help address students with special needs that have been diagnosed to affect their academic performance.\n\nMain Effect\nOne advantage of using syntax is that it is sometimes easy to conduct a bevy of analyses while making only a few small changes much more quickly than one could do through a GUI. Here, we need only change the title (for future reference) and change instance of the word “gender” to “iep.status”.\nOr, simply paste the following syntax into the Syntax Editor:\nTITLE \"IEP Status Main Effect\".\nMIXED global.exec.comp.sr with wave by iep.status\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(iep.status)\n/METHOD=ml\n/FIXED=iep.status\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Syntax\nThis syntax will conduct the same “main effect” analysis testing whether students with and without IEPs differ significantly in their initial, sixth-grade levels of self-reported executive functioning. The only changes in it (outside of the title) are changing gender to iep.status. a change one could make by searching and replacing that phrase, but then also reviewing the code to ensure doing it automatically didn’t have any unintended consequences elsewhere in the code.\n\n\nInterpreting the Results\nSince we have already detailed the particulars of the results when investigating gender, we will be more brief in our coverage of iep.status. Hopefully this will help reinforce the main points and encourage practice remembering.\n\nInformation Criteria\nWe can test the change in fit of the entire model through the information criteria. We could use the -2LL (deviance) statistics to compare this to the unconditional means or growth model since both of those models are nested withing the current: They contain only subset of the terms in the current model.\nThe current model has one more term than the unconditional growth model—iep.status—which the Model Dimension table tells me adds only one to the Number of Parameters. Therefore, I could test for a change in model fit by subtracting the -2LL for the current model from the -2LL for the unconditional growth model and testing that difference against a critical χ2 value with on degree of freedom.\nThe current model has two more terms than the unconditional means model: the iep.status term and the wave term that was added in the unconditional growth model. The current model thus has two more degrees of freedom than the unconditional means model27, so the difference in -2LLs would be compared against the critical χ2 value for 2 dfs.\n\n\nFixed Effects\n\n\nThe Type III Tests of Fixed Effects and the Estimates of Fixed Effects tables indicate that the iep.status term is significant.\nSince iep.status scored as a dummy variable, the levels for it given in Estimates of Fixed Effects table are interpreted somewhat unconventionally: there are zeros in the row for [iep.status=1] (denoting that the student has an IEP). We can nonetheless use this table to interpret the magnitude of the effect of having an IEP since those without one ([iep.status=0]) begin the study with an average of 7.8 points less on the scale (the BRIEF-SR GEC) than those with an IEP; since lower scores denote stronger executive functioning, this suggests that those without an IEP already have stronger executives functions than those with an IEP.\n\n\nCovariance Parameters\n\nThe Estimates of Covariance Parameters table presents the residuals at both the between-participant level (level 2) as Residual and the variances and covariances of the within-participant (level 1) terms below that. This table shows again that there is significant variance yet unexplained and that the intercepts covary significantly with the changes—note, though, that this is not changes in outcomes related to IEP status since we have not included an iep.status × wave interaction term to the mode; this is simply whether intercept and slope are related in the overall set of data.\n\nThe Estimated Marginal Means table—entitled by the Label for the variable reported—indicates that the model estimates that the mean global.exec.comp.sr score for students without IEPs is 150.5, which is 7.8 points lower than the mean score estimated for those with IEPs.\n\n\nSummary\nWe found that IEP status is significantly related to initial levels of executive functioning. Indeed, it appears rather predictive of this initial level given the magnitude of the effect.\n\n\n\n\nIEP Status × Time Interaction\nWe now look at the effect of IEP status on changes in executive functioning over time. Looking at it in a separate model like this helps compare differences in model fit. It also lets us compare how much the effect of IEP on intercept is related to the effect of IEP status in subsequent changes in the outcome because we look at both effects in isolation and then can look at them together in the same model.\nOnce again, we simply change change gender to iep.status is the model containing the interaction term:\nTITLE \"IEP Status Interaction\".\nMIXED global.exec.comp.sr with wave by iep.status\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(iep.status)\n/METHOD=ml\n/FIXED=iep.status iep.status*wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Results\n\nInformation Criteria\nNote in the Model Dimension table that the iep.status \\(\\times\\) wave interaction term has 2 degrees of freedom—not 1. An addition degree of freedom is used to account for interactions, but only one for the first interaction; if we had added two interactions with the same within-participant term, we wouldn’t need to continue to add more than one degree of freedom per interaction. In other words, if we had included both an iep.status \\(\\times\\) wave and a gender \\(\\times\\) wave interaction, we would only need to add the additional degree of freedom (for the interaction with wave) once—not twice. This is confusing, can better seen by an example, which I when we compare more complex models, below.\n\n\nFixed Effects\n\n\nIEP status significantly predicts changes in executive functioning over the middle and high school years among these adolescents. The relationship is rather strong even though the effect is still not large: The global.exec.comp.sr scores for those with IEPs move toward levels indicating worse executive functioning at the mean rate of about 0.013 points per year, compared to those without IEPs whose scores more in the same direction a bit more slowly (at the rate of about 0.011 points per year).\n\n\nCovariance Parameters\nThe pattern of significances among the random effect terms when iep.status \\(\\times\\) wave is added to the model reflects that found when iep.status was added. The values differ, e.g., for the intercept × change term (UN(2,1)) because we are estimating the effect with somewhat different information.\n\n\nSummary\nNot only do those students with IEPs already come into the range of grades measured here with significantly worse self-reported executive functioning, but they also tend to show significantly greater depreciation in those faculties throughout their secondary grades.\nWe chose to investigate executive functioning because we anticipated nurturing its development could help those who are struggling to overcome challenges both within and without. Seeing executive functioning weaken most among those who may benefit most from it seems alarming.\n\n\n\n\n\n10.12.3 Economic Distress\nAnd yet among all of the challenges faced by these teens, their economic ones may be most pervasive. Like race/ethnicity, the effect of poverty is often misattributed. Having relatively little money is among the least of the challenges faced by the poor; more salient is living in less safe, more stressful, and more dangerous conditions; limited access to healthy food and lifestyle choices; fewer opportunities for success and many voices taking their failure for granted; and several other factors that could affect not only the development of their executive functioning, but also whether they have disabilities warranting IEPs and whether they are correctly diagnosed with those needs. All of this may also be filtered through the increasing different experiences of boys and girls in these environments.\nIn short, economic.distress may affect initial levels of executive functioning, subsequent rates of development. It may also interact with one’s special needs—and this all may be filtered through the lens of being a boy or girl.\n\nMain Effect\nWe are presuming here that our main interest is indeed on economic distress and whether any relationship it has with executive functioning is moderated by IEP status and gender—or the extent to which its effects are independent of those other factors.\nWe begin this phase of our analysis by first looking at the relationships of economic distress alone, without consideration of the effects of IEP status or gender. And first, the relationship with initial levels:\nTITLE \"Economic Distress Main Effect\".\nMIXED global.exec.comp.sr with wave by economic.distress\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(economic.distress)\n/METHOD=ml\n/FIXED=economic.distress\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Results\n\n\nUnexpectedly (after my harangue at least), economic.distress does not have a significant effect on the initial levels of executive functioning.\n\n\n\nIEP Status × Time Interaction\nWe now investigate the effects of economic distress on subsequent changes in executive functioning with the following syntax:\nTITLE \"Economic Distress Interaction\".\nMIXED global.exec.comp.sr with wave by economic.distress\n/PRINT=SOLUTION TESTCOV\n/EMMEANS=TABLES(economic.distress)\n/METHOD=ml\n/FIXED=economic.distress*wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\n\nInterpreting the Results\n\n\nSkipping to the variable-level tests, we see that—taken in isolation—economic distress has a clearly significant effect on the development of executive functioning throughout middle and high school. The difference in slopes for those above and below the criterion for being considered “economically distressed” show only slightly different trajectories, but the standard errors again indicate why this is significant.\n\n\n\n\n10.12.4 Summary of Univariate Model Analyses\nThe adolescents’ initial levels of executive functioning were significantly predicted by whether the teen had an IEP, but not by the teen’s gender nor whether they were experiencing economic distress. All of the predictors were significantly related to changes in executive functioning over time.\nThese results, however, were found when looking at each relationship separately. It is entirely plausible that they will interact. For example, the effect of economic distress may be intermixed with the effect of having an IEP, and gender may moderate either effect.\nEven among this limited set of variables, there are many further analyses we can conduct to better understand what is happening in these teens’ lives. We will look only at one specific scenario, though: How the effects that we found significant in isolation interact when included together.\nWe will look at the significant relationships in two steps. First, we will include the gender and IEP status effects together in a base model. We will then add in economic distress’s interaction to this model to see whether this adds new information to our understanding."
  },
  {
    "objectID": "mlm.html#multivariate-mlm",
    "href": "mlm.html#multivariate-mlm",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.13 Multivariate MLM",
    "text": "10.13 Multivariate MLM\nWe will now demonstrate using multiple predictors, thus creating a multivariate MLM.\nSince we are now using more than one predictor, there is the chance of multicollinearity affecting the value (and stability) of the model parameters—especially for those predictors that are highly correlated with each other. Although there are analyses that can detect multicollinearity in multi-level (hierarchical) models (q.v., Yu et al., 2015), reviewing them is outside of the pale of this course. Note that in general, multicollinearity is usually less of a problem than it’s sometimes conceived to be, and that it matters in MLMs more when it is between nested groups than within one.\n\n10.13.1 Base Model\nWe create our base, comparison model with:\nTITLE \"Gender and IEP Status\".\nMIXED global.exec.comp.sr with wave by iep.status gender\n/PRINT=SOLUTION TESTCOV\n/METHOD=ml\n/FIXED=iep.status iep.status*wave gender*wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\nThis includes main effect and interaction terms for iep.status and the interaction term for gender.\n\nInterpreting the Results\n\nInformation Criteria\n\nWe will return to using the information criteria (viz., the -2LL—or deviance—statistic) in addition to reviewing the tests of the individual parameters. First, note that this model is using 9 dfs to estimate the values for the parameters.\nWe can see from this Model Dimension table that a degree of freedom (a parameter) was only added once—here listed in the iep.status * wave row, but it would have been listed in the gender * wave row if we had added gender*wave first in the syntax. This is because we only need to add that parameter once for all interactions with that within-participant term (wave). When we add the economic.distress*wave term, below, we will thus only be adding one additional parameter—not two.\n\nAlso note that the -2LL for this base model is 25594.418.\n\n\nFixed Effects\n\n\nWhen all three terms are added to the model, the iep.status * wave interaction is no longer significant. Both of those with and without IEPs show changes in executive functioning, but since the omnibus F-test did not find that term to be significant, those changes over time are not significantly different.\n\n\nCovariance Parameters\nWe are not analyzing the covariance terms here. Since the model is more complex, the weights among the residuals and within-participant variances and covariances are harder to interpret (without direct comparisons to other models to disentangle these effects).\n\n\nSummary\nThis is the base model, against which we wish to compare the effects of economic distress. Therefore, the significances of the terms are not of primary interest here. Nonetheless, it is interesting to see that changes over time due to IEP status are no longer significant when we also account for changes due to gender.\n\n\n\n\n10.13.2 Final Model\nTo this base model, we now add one addition term: the economic.distress * wave interaction. We are therefore testing here whether the economic distress contributes significantly to our understanding of adolescent development of executive functioning beyond that already made by the other terms. If economic distress is significant, then the effect it has on development is at least partly due to factors independent of gender and IEP status. If economic distress is not significant, then the relationship we found earlier between it and executive functioning development may be sufficiently accountable by gender and IEP status.\nThe syntax for this final model is:\nTITLE \"Adding Eco Dis Interaction to Model w/ Gender & IEP Status\".\nMIXED global.exec.comp.sr with wave by economic.distress iep.status gender\n/PRINT=SOLUTION\n/METHOD=ml\n/FIXED=economic.distress*wave iep.status iep.status*wave gender*wave\n/RANDOM intercept wave | SUBJECT(id) COVTYPE(un).\nIn this syntax, we have added economic.distress to the MIXED command and the economic.distress * wave interaction to the list of /FIXED parameters.\n\nInterpreting the Results\n\nInformation Criteria\n\nNote that we only added one additional parameter: The base model (without the economic.distress * wave interaction) had 9 Total parameters whereas the current model now has 10.\nWe can see in the Information Criteria table just below that the -2LL for this extended model is 25592.703. The 2LL for the base model was 25594.418; the difference between these two models is 25594.418 – 25592.703 = 1.75. The critical χ2 for 1 df is 5.02 for a two-tailed test, or 3.84 for a one-tailed test; in either case, the economic distress term did not make a significant co0ntribution to the fit of the model to the data.\n\n\n\nFixed Effects\nThis non-significance is reflected in the variable-level tests:\n\nThe omnibus F-test did not find the economic.distress * wave term to be significant. Therefore, the additional tests of the levels of the economic.distress * wave interaction are not warranted (even though they are still given):\n\n\n\nSummary\nWe know from our univariate analysis that economic distress—when taken alone—significantly predicts changes in executive functioning. However, we now see from this analysis that the effect of economic distress here is arguably explainable through the effects of IEP status and/or gender.\nOf course, this oughtn’t be the final word on the issue. We do not know yet if the covariance between economic distress and changes in executive functioning overlaps that of IEP status, gender, or both. We could look at this by next taking out those terms systematically. We could also, e.g., add an economic.distress * gender * wave interaction to test the effects of more complex relationships.\nWholly beyond the pale of the current chapter would be to see if perhaps iep.status (or maybe even gender) mediate the relationship between economic.distress and wave. That will require another course to discuss—perhaps one on measurement and factor analysis….\nMonsalves et al. (2020) provides good advice on writing up models like these. A couple other guides are given in the Resources section of Chapter 3: Writing Results Sections."
  },
  {
    "objectID": "mlm.html#additional-resources-topics",
    "href": "mlm.html#additional-resources-topics",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "10.14 Additional Resources & Topics",
    "text": "10.14 Additional Resources & Topics\n\n10.14.1 Some Other Ways to Analyze Longitudinal Data\n\nStatistical Process Control\nStatistical process control is a more graphical approach to longitudinal analyses. It focuses on visually scanning for changes in some outcomes up to and then after some discrete event, like when an intervention or new procedure was implemented.\nSome resources about it are:\n\n[Statistical Process Control], a supplement to Polit & Beck (2017)\n\n\n\n\n\n\n\n\n\n\nHausman, C., & Rapson, D. S. (2017). Regression discontinuity in time: Considerations for empirical applications. National Bureau of Economic Research Working Paper Series, No. 23602. https://doi.org/10.3386/w23602\n\n\nHausman, C., & Rapson, D. S. (2018). Regression discontinuity in time: Considerations for empirical applications. Annual Review of Resource Economics, 10(1), 533–552. https://doi.org/10.1146/annurev-resource-121517-033306\n\n\nMonsalves, M. J., Bangdiwala, A. S., Thabane, A., & Bangdiwala, S. I. (2020). LEVEL (Logical Explanations & Visualizations of Estimates in Linear mixed models): Recommendations for reporting multilevel data and analyses. BMC Medical Research Methodology, 20, 1–9. https://doi.org/10.1186/s12874-019-0876-8\n\n\nPolit, D. F., & Beck, C. E., Cheryl T. (2017). Nursing Research: Generating and Assessing Evidence for Nursing Practice (Tenth). Wolters Kluwer.\n\n\nYu, H., Jiang, S., & Land, K. C. (2015). Multicollinearity in hierarchical linear models. Social Science Research, 53, 118–136. https://doi.org/10.1016/j.ssresearch.2015.04.008"
  },
  {
    "objectID": "mlm.html#footnotes",
    "href": "mlm.html#footnotes",
    "title": "10  Longitudinal Analyses: Why and How to Conduct Multilevel Linear Modeling",
    "section": "",
    "text": "Although I’ll be talking about “participants” that we’re studying over time, we could just as easily be studying something else that has different waves of data attached to it, such as the number of falls in a given unit over the course of a week.↩︎\nVariables like this—that can have meaningful values outside of the that our instruments can measure—are called “censored.”↩︎\nSinger and Willett explain this well in the context of obtaining more precise (defined as an efficient measure that quickly hones in on the true, underlying score). There, they note that simply increasing the variability of when (in time) measures are taken helps determine the rate (e.g., slope) of change. An example of why this is true: Imagine I want to draw a line on a wall the is parallel to the floor. I can do this by using a ruler to measure the same distance from the floor, make marks on the wall at each measurement, and then lay the rule flat along those marks on the wall to draw my line. I will have the most luck in actually making a parallel line of I make my marks at different places along the wall—and especially if those places I mark are far apart.↩︎\nSure, maybe because of a ceiling or floor effect. But even more generally, people who, e.g., start with lower scores will also be more likely to end with lower scores.↩︎\nIt’s also called a “two-way (or two-factor) ANOVA with repeated measures.” If there are more factors in the model, it would be called a three-way, four-way, etc. ANOVA with repeated measures.↩︎\nI’ve avoided mentioning this pretty much every time I mention post hoc comparisons. Even within the realm of level comparisons, there are alternatives, most notably planned comparisons, which are conducted instead of an omnibus ANOVA followed by post hoc analyses. Planned comparisons are preferred if one knows ahead of time (I.e., they’re planned) which specific subset of comparisons between levels one wants to conduct. One then uses, e.g,. t-tests to compute those specific tests and no other inferential tests for those data. Planned comparisons tend to be more powerful, but you need some discipline (and, of course, specific questions you want answered) to use them. They do tend to get short shrift, though.↩︎\nThis website has a pretty simple and clear explanation of the ways the sums of squares are computed.↩︎\nIf you’ve been reading these footnotes and thinking really deeply about ll of this, well, first, thanks. Second, if you have, then you may have reflected that if it really is simply whether the experimental-group participants showed greater pre-post differences and did the control-group participants, then we could simply conduct a planned comparison of this—you’re right! If you also reflected how this kinda looks like a t-test of pre-post differences scores like I first listed in this section…yeah, you’re right.↩︎\nIt’s called sphericity (it’s also called circularity, and a type of compound symmetry) because it posits that—with three waves—the variance in wave 1 equals the variance in wave 2 which equals the variance in wave 3, or \\(\\sigma^2_{wave 1} = \\sigma^2_{wave 2} = \\sigma^2_{wave 3}\\). They create a little circle—or I guess sphere—of the variances all being equal to each other.↩︎\nThere are other ways to handle time—and even ones more flexible than MLMs. Latent growth curve (LGC) modeling is one example that allows even finer tests of change over time, e.g., that different participants show different growth curve (say, some showing linear growth while others show exponential.)↩︎\nAs well as for estimating the intercept if the intercept is set to be something other than one of the waves, e.g., if you don’t/can’t assume the first wave of data is the intercept for the participants.↩︎\nIt is the number of days since January 1, 4713 BCE, presuming use of the modern. Western calendar to count back that far. It was developed by an historian, Joseph Scaliger (whence the date range), and named after his father, whence the important-sounding name. Note, however, that the dates here are not computed along the lines set out by Scaliger, but a derivative method used by a standard package in the R programming language.↩︎\nTo finally create a footnote of actual value: In the early stages of this ling of research, I didn’t limit myself to using Julian age, but also, e.g., simply used the calendar year or—at most—semester. However, those measures of time proved not to be precise enough to track the rather subtle effects we investigate here. Only when I used Julian dates—i.e., only when I measured time as precisely as I could—did I reliably find interesting insights. I guess I shouldn’t have been surprised that analyses of times-varying effects benefited from good measures of time.↩︎\n“STEM” is the rather aspirational acronym for courses related to science, technology, engineering, and math.↩︎\nFor match merges like this, it’s arguably easier to have only the data set to which you want to add files open and to access the closed data set from An external SPSS Statistics data file field in the Add Variables to X dialogue since you needn’t worry about which open set is which, but I wanted to show you the more involved way so you know how to do that. The other way can be sufficiently covered in a simple footnote. ↩︎\nWe can also get the syntax from the journal file. The location of the journal file can be found by going to Edit &gt; Options &gt; File Locations and looking under the Session Journal section for the Journal file field. Note that Record syntax in Journal must be selected, but it is by default.↩︎\nSPSS pus certain elements in upper or lower case to help distinguish things, but case doesn’t. You can write SPSS syntax in what case you want.↩︎\nYou may notice that SPSS proffers suggestions for syntax as you type. You can select the appropriate command from the proffered list by hitting Enter. In addition to this as a guide to possible options and how to type them, you can access a syntax reference sheet via Help &gt; Command Syntax Reference.↩︎\nOr don’t actually type all of that; just know that you can add comments like this, and that that is what this subcommand is doing.↩︎\nCommand + S on a Mac.↩︎\nIndeed, SPSS can create just that via Edit &gt; Options &gt; General where you can select Record syntax in journal to have any syntax—including that accessed via the GUI—recorded to a .jnl file.↩︎\n“Stochastic” in statistics denotes randomness that is indeed truly random, i.e., that indeed have no bias.↩︎\nIf that χ2 difference score is significantly different from zero, to be exact—but that’s a lot of “differences” to digest in one sentence.↩︎\nThis is one of the main ways that my syntax differs from that posted in the companion website to Singer and Willett’s book: They don’t separate out effects into by statements.↩︎\nThe ordinary least squares estimation one uses in an ANOVA arrives at the same outcome as maximum likelihood estimation—when the assumptions of the ANOVA are met (viz., when residuals are truly normally distributed).↩︎\nIt is my own convention to always set dichotomized gender so that female = 1 (and male = 0) whenever it is dichotomized as such, but always following the same routine makes it easier to remember and interpret. This also seems to me to give a bit more prominence to women, which is never a bad thing. Being dichotomized, though, we know nothing about the other vast and varied facets of sexual identity.↩︎\nIt is not always the case that a new term has only one degree of freedom. Interaction terms rarely will, and the ethnicity nominal variable has several. Check the Model Dimensions table to see how many degrees of freedom—Number of Parameters—each term has.↩︎"
  },
  {
    "objectID": "925.html#sec-foundations_measurement",
    "href": "925.html#sec-foundations_measurement",
    "title": "11  NURS 925: Psychometrics Course",
    "section": "11.1 Foundations of Modern Research Measurement",
    "text": "11.1 Foundations of Modern Research Measurement\n\nSlides \nThis presentation covers:\n\nTheories and domain sampling\nTypes of measurement\nA brief history of measurement\nGeneral concepts of good measurement\nLikert response formats"
  },
  {
    "objectID": "925.html#sec-validity_and_reliability",
    "href": "925.html#sec-validity_and_reliability",
    "title": "11  NURS 925: Psychometrics Course",
    "section": "11.2 Validity and Reliability",
    "text": "11.2 Validity and Reliability\n\nSlides \nNote: This presentation is a PDF document\nThis presentation covers:\n\nHistorical conceptions of validity\nModern conception of validity\nReliability as measurement of true score\nTypes and measurments of reliability"
  },
  {
    "objectID": "925.html#introduction-to-factor-analysis-exploratory-factor-analysis",
    "href": "925.html#introduction-to-factor-analysis-exploratory-factor-analysis",
    "title": "11  NURS 925: Psychometrics Course",
    "section": "11.3 Introduction to Factor Analysis & Exploratory Factor Analysis",
    "text": "11.3 Introduction to Factor Analysis & Exploratory Factor Analysis\n\nSlides \nThis presentation covers:\n\nIntroduction to Factor Analysis\n\nAn Aside About Principal Component Analysis\n\nExploratory Factor Analysis\n\nConcept of EFA\nGeneral Steps to Conducting EFAs"
  },
  {
    "objectID": "925.html#confirmatory-factor-analysis",
    "href": "925.html#confirmatory-factor-analysis",
    "title": "11  NURS 925: Psychometrics Course",
    "section": "11.4 Confirmatory Factor Analysis",
    "text": "11.4 Confirmatory Factor Analysis\n\nSlides \nThis presentation covers:\n\nReview of Factor Analysis\n\nEFA vs. CFA\n\nOverall & Particulars of Conducting CFAs\nTesting CFA Model Fit\nA Few Words About Sample Size"
  },
  {
    "objectID": "efa.html#the-concept-of-factor-analysis",
    "href": "efa.html#the-concept-of-factor-analysis",
    "title": "12  Exploratory Factor Analysis",
    "section": "12.1 The Concept of Factor Analysis",
    "text": "12.1 The Concept of Factor Analysis\nFactor analysis is a member of a rather large family of analyses that—among other things—uses ostensible variables to measure and analyze non-ostensible constructs, domains, or what are generally called factors. In this sense, a “factor” is that non-ostensible thing that determines whatever value ostensible variables take on. Factor is intentionally a very general term here that can encompass true constructs/domains, but is also simply whatever underlying thing is driving what we actually see.\nIt is not, however, intended to imply a factor in a statistical model, nor a “factor” in math (a multiplicative product or common divisor). So, please forget whatever definition you already have for the word “factor” and understand it here as simply that non-ostensible thing that drives one or more ostensible variables. (Indeed, one of the two main types of factor analysis, “exploratory factor analysis,” revolves around trying to figure out just what the underlying factors really are. I won’t discuss here the other main type of factor analysis, confirmatory factor analysis.)\n\n12.1.1 Role of Correlation/Covariance Matrix in Factor Analysis\nAll factor analyses per se begin with the assumption that the more that ostensible variables correlate with each other, the more likely they are to measure the same, underlying (non-ostensible) factor. This assumption may not be true, but factor analyses1 assume it is. And what factor analysis does—in essence—is group variables together based on how well they correlate2; in fact, most statistical software can conduct a factor analysis on a correlation matrix of data alone: we don’t need to have access to the raw data (except to know the sample size). Given this, it may help to look at factor analysis first through the lens of a sample correlation matrix:\n\n\n\n\nItem 1\nItem 2\nItem 3\nItem 4\n\n\n\n\nItem 1\n1\n.80\n.10\n.20\n\n\nItem 2\n.80\n1\n.05\n.15\n\n\nItem 3\n.10\n.05\n1\n.90\n\n\nItem 4\n.20\n.15\n.90\n1\n\n\n\nIn this correlation matrix, Items 1 and 2 are strongly correlated with each other, but neither correlates well with Items 3 or 4. Alternatively, Items 3 and 4 correlated well with each other (but not with Items 1 & 2). In this example, we wouldn’t be surprised if Items 1 and 2 measured the same thing and if this “thing” was largely unrelated to whatever Items 3 and 4 measured. In this example, then, we would expect that Items 1 and 2 are both ostensible manifestations of the same non-ostensible factor, and that Items 3 and 4 are the manifestations of an other non-ostensible factor.\nWith such a simple and clear example, we wouldn’t need to conduct a factor analysis: Our eyes can be trusted well enough here. Often, however, the picture is not as clear or simple, and we may yearn for some objective process firmly grounded in common, sensible assumptions and well-tested by research. For these occasions, we may turn to factor analysis.\n\n\n12.1.2 Factor Analysis Is Similar to the Linear Regression Analyses You Already (Should) Know\nAs I noted above, factor analysis itself relies on the correlation matrix (or, similarly, the variance/covariance matrix3). There are different ways to analyze this matrix (or derive it and related statistics from the raw data), but in general, factor analysis conducts analyses conceptually similar to multivariate multiple regressions4 to determine how the items “load” onto the factors very similarly to how we estimate the \\(beta\\)- or b-weights for parameters in a linear regression. And like, e.g., an ANOVA, we not only get stats for how well our terms explain the data, we get stats (like Mean Square Error) for the extent to which our terms don’t fit the data. (More on this much later.)"
  },
  {
    "objectID": "efa.html#steps-to-conducting-an-exploratory-factor-analysis",
    "href": "efa.html#steps-to-conducting-an-exploratory-factor-analysis",
    "title": "12  Exploratory Factor Analysis",
    "section": "12.2 Steps to Conducting an Exploratory Factor Analysis",
    "text": "12.2 Steps to Conducting an Exploratory Factor Analysis\n(Please note that I broke out the steps here a bit differently than I did in the presentation. The steps are the same in both, I simply divided the same procedure into a different number of steps based on how well each worked for the two media. Costello and Osborne (2005) offer more good guidance.)\n\n12.2.1 1. Estimate the Number of Factors\nIn multivariate analyses like a MANOVA, we know ahead of time how many DVs there are. Similarly, to get measures of how well the ostensible variables (e.g., items5) load onto the factor(s), we need first to get a sense of how may factors there may be. Therefore, the first step to an exploratory factor analysis (EFA) is to estimate the number of factors to use for further analysis.\nThe number of factors we choose could range from 1 to the number of items we have.\nIf we were to choose that the number of factors equaled the number of items (e.g., saying that there are 10 factors that underlie a 10-item instrument), we would essentially be saying that every item measures something different. This may be so, and would mean that there really isn’t any reason to conduct a factor analysis since it wouldn’t help us understand the data any better than item-level scores.\nOften, though, when we conduct item-level analyses means we can’t easily (and unbiasedly) tease apart interesting from uninteresting variance—or we may find all variance “interesting” and never see the forest for for the trees. There are certainly times to scrutinize each item, but our ultimate goal is hopefully bigger—more profound—things than that.\nTherefore, we strive to choose a smaller number of factors than items. But how much smaller? The general strategy is to find a number of factors that explains “enough” of the data. And so, a lot of thought has gone into what we mean by “enough.” One common criterion is to choose any factor that accounts for more of the data than an individual item does.\nAnother common criterion is to choose those factors that seem to “stand apart” from all of the other factors6.\n\n1.1. Drop-offs in Scree Plots\nOne of the earliest and biggest contributors to factor analysis was Raymond Cattell. An astoundingly productive and brilliant man, Cattell was also not above controversy for what were either his political beliefs or misunderstandings of them{^He may or may not have believed—as Galton certianly did—in eugenics and other right-wing supremacist positions throughout much of his life]. Whoever he was as a person, he was a gifted researcher, able indeed to see the “forest” and even to devise ways for us to do so better ourselves.\nPerhaps his most lasting strategy was to use what he called “scree” plots to help decode how many factors to choose. “Scree” is the collection of rubble that gathers at the bottom of a cliff or plateau. The idea is that we’re interested in the solid, prominent cliff and not the rubble. We can choose whichever factors “stand out” above the scree of other factors. In the following plot, we may thus decide that this 12-item instrument contains two factors that account for most of the variance since the first two dots make what nearly looks like a ledge with all of the other ten dots fat below it—like rubble at the base of a cliff7:\n\n\n\nExample Scree Plot of a 12-Item Instrument\n\n\n\n\n1.1. Eigenvalues Greater than 1\n\nMeanings of “Eigenvalue”\n\nMeaning 1: Proportion of Total Variance\nThe y-axis in the scree plot measures each factor’s eigenvalue. Eigenvalues, commonly used in mathematics and engineering, represent the amount of total variance explained by a given factor in the context of factor analysis.\nHere’s how it works:\n\nThe sum of all eigenvalues equals the total number of variables (or items) analyzed. For example, in a 12-item analysis, the eigenvalues in the scree plot will sum to 12.\n\nThe eigenvalue for the first factor is 3. This means: \\[\n\\frac{3}{12} = 0.25\n\\] Thus, the first factor accounts for 25% of the total variance.\n\nIf we compare this to correlations, the square root of this proportion (\\(\\sqrt{0.25} = 0.5\\)) suggests an average correlation (\\(r\\)) of 0.5 between the items and the factor. This aligns with the principle that variance is the square of correlation.\nFor the first two factors (eigenvalues of 3 and 2.7), their combined contribution is: \\[\n\\frac{3 + 2.7}{12} = \\frac{5.7}{12} = 0.475\n\\] or nearly half of the total variance.\n\n\nMeaning 2: Sum of Items’ Squared Loadings on a Factor\nAn eigenvalue can also be understood as the sum of the squared loadings of all items on a factor. To illustrate:\nSuppose a 4-item instrument has the following factor loadings:\n\\[\n\\text{Factor}_1 = .7x_{1} + .5x_{2} + .1x_{3} + .2x_{4}\n\\]\nHere: - \\(x_{1}\\) through \\(x_{4}\\) are scores on the items in the instrument. - Factor loadings (0.7, 0.5, etc.): The correlations between the items and the factor.\nThe eigenvalue for the factor is calculated as the sum of the squared loadings:\n\\[\n.7^{2} + .5^{2} + .1^{2} + .2^{2} = .49 + .25 + .01 + .04 = .79\n\\]\nThus, the eigenvalue is 0.79. This represents the total variance accounted for by the factor across these four items.\nFor the 12-item scree plot example, the eigenvalue for the first factor is 3. This implies that the sum of the squared loadings of the 12 items onto the first factor equals 3.\nWhy squared loadings?\n\nFactor loadings are essentially correlations, and squaring them converts correlations into proportions of explained variance. Since an eigenvalue represents the proportion of total variance accounted for by a factor, it must be based on the squared loadings.\n\n\n\nMeaning 3: Relative Contributions of Individual Items\nAs noted earlier, the sum of all eigenvalues equals the total number of items. In the 12-item example, the eigenvalues sum to 12. This is because, by definition, the eigenvalue for any single item (if treated as a factor) equals 1.\n\nImplication: A factor with an eigenvalue greater than 1 accounts for more variance than any single item.\n\nConversely, a factor with an eigenvalue less than 1 contributes less variance than a single item.\n\nThis is why a common criterion for factor retention is Kaiser’s Criterion8, which retains only factors with eigenvalues greater than 1. Looking at the scree plot:\n\nFactors with eigenvalues less than 1 (e.g., Factors 4 through 12) do not contribute enough variance to justify their inclusion.\nRetaining such factors would mean adding complexity without meaningful additional information.\n\nTakeaway: If a factor has an eigenvalue less than 1, it is generally better to rely on the individual items rather than attempting to interpret that factor.\n\n\n\n\n1.3. Theory (or Practicality)\nI purposely made Factor 3 in that scree plot above just a little greater than 1 (it’s 1.3). Since it’s greater than 1, we may want to keep it. Since it’s much lower than the next-larger factor (Factor 2), we may want to exclude it. We could use either criterion to justify our decision, so how do we decide?\nFactor analysis—especially exploratory factor analysis—is not entirely an objective task. We should thus not simply swallow whole the results of factor analysis; we should chew on it and see if it tastes like something real. In this case, we could look at the results we get from choosing a 2-factor solution with those of a 3-factor solution and decide ourselves which appears to make more theoretical or practical sense.\n\n\n\n12.2.2 2. Evaluate the Results\nThe second conceptual step in exploratory factor analysis is to evaluate the results we obtain given the number of factors we chose. And yes, it’s often worth playing around here, choosing different numbers of factors, and maybe even selecting subsets of items (as I did with when I removed the items from the APT that compared animals to humans).\nIn exploratory factor analysis, the main way we evaluate the results is by reviewing how well various items load onto the various factors. We review them to see if the factors make sense, if they provide any good insights, etc.\nOften when we review how well the items load onto the factors, we will find that items load pretty well onto more than one factor. To turn again to the 4-item example I gave in the presentation, the items had the following loadings onto the two factors:\nFactor1 = .7x1 + .5x2 + .1x3 + .2x4\nFactor2 = .1x1 + .1x2 + .6x3 + .8x4\nSure, the loadings of .1 are really small. But what about that item 4 that loaded a bit onto both factors?\nOne criterion is to simply choose the highest loading (putting item 4 onto Factor 2). Another is to place items onto any and all factors on which its loading is &gt;.3.\nNote that an item may not load well onto any factor. (Again, commonly this means that its loadings are all less than .3 on all factors.) Such items deserve especial attention—and may well be removed from all factors (and possibly even instrument scores and other analyses).\n\n\n12.2.3 3. Review Different “Rotations” of the Factors\nAs I wrote just above, the four items load a bit onto both factors. Since all four item scores would be part of both factors’ scores, these two factors themselves would be slightly correlated. When factors are allowed to be correlated like this, we say they are “oblique” to each other; they are called oblique because if I plotted them as two axes, those axes would be at right angles to each; the term for two lines (axes) that are neither perpendicular nor parallel is “oblique.”\nWhy go through all the trouble of calling them oblique when we could just say they’re correlated? Because we can do further mathemagic on the factors and force them to be uncorrelated—and this involves “rotating” the axes so that they are indeed perpendicular to each other. Of course, even that is not eldritch enough, so instead of saying the factors are now perpendicular, we say they’re “orthogonal,” which means the same thing.\nTo make matter even worse, there are different ways to rotate factors into orthogonality—and even different ways to rotate them to be variously oblique. The relative merits of the growing number of rotation techniques is a vibrant area of research, with no clear winner found.\n\nChoosing Which Rotations to Use\nIn general, it’s good advise to try out at least one orthogonal and one oblique rotation and see how this helps us interpret the data.\n\nOrthogonal Rotations\nOrthogonal rotations force all factors to be unrelated to each other. These rotations therefore use various procedures to maximize the differences between factors.\n\nVarimax is by far the most popular orthogonal rotation method, and it is often the default choice when there is no strong preference for another method. Its primary goal is to simplify the interpretation of factors by redistributing the variance of loadings to make high loadings higher and low loadings lower. Specifically, it maximizes the variance of the squared loadings for each factor across all items. In practice, this results in a factor structure where each factor has a few strongly loading items and many items with near-zero loadings. This simplification helps make the factors more interpretable. Varimax does not inherently make factor eigenvalues more similar or the proportion of explained variance more equal among factors, though it can result in a more balanced distribution in some cases.\nQuartimax is another common orthogonal rotation method. While varimax aims to simplify the interpretation of factors, quartimax focuses on simplifying the interpretation of items. It does this by maximizing the variance of squared loadings for each item across all factors. In practice, quartimax tends to produce a factor structure where items load strongly on a single factor while having minimal loadings on others. This can make it easier to identify which items belong to which factor. However, quartimax often results in a factor structure where the first factor dominates, accounting for the largest proportion of the total variance. This makes the factors less balanced in terms of variance explained. For example, applying quartimax to the first two factors in a scree plot would likely accentuate the difference in eigenvalues between those factors, making the first factor seem more dominant.\n\nThere are plenty of other orthogonal rotations, but both of these have stood the test of time (and even some formal scientific tests) and should work well enough for most data.\n\n\nOblique Rotations\nThere are even more oblique rotations. They also tend to require a bit more hands-on involvement from the researcher. Luckily, though, for the two I describe next, this just means deciding the maximum possible value for how strongly the factors can correlated with each other. In other words, I could determine ahead of time that I want my factors to have correlation coefficients with each other that are no larger than .4.\n\nPromax is a common oblique rotation that in fact begins with an orthogonal rotation before “relaxing” this orthogonality requirement enough to let factors correlate with each other (either to a least-squares sense or to the maximum level you set ahead of time.) It tends to give rather easily-interpreted loadings onto the factors.\nDirect Oblimin doesn’t begin with an orthogonal rotation like promax but instead attempts to find the best axis for each factor and then modifies this slightly to try to reduce how much items load onto each factor.\n\nPromax will tend to produce factors that are more orthogonal than direct oblimin.\n\n\n\n\n12.2.4 3. Interpretation\nThis is arguably not a single, discrete step. Instead, one should be considering what is happening at each of the other steps, reflecting upon how it relates to apposite theories, and using one’s own judgment to guide analyses.\nNonetheless, researchers do tend to step back at the end of one round of factor analysis to consider how things look. I, too, recommend making sure there is at least this one deliberate pause to reflect on what the results imply about data and their larger meanings.\nSimilarly, you may now want to stop to think about all that I’ve written here—and let me know what questions you have."
  },
  {
    "objectID": "efa.html#footnotes",
    "href": "efa.html#footnotes",
    "title": "12  Exploratory Factor Analysis",
    "section": "",
    "text": "Other analyses related to factor analysis don’t necessarily make this assumption, and this assumption can be relaxed with, e.g., confirmatory factor analysis. Nonetheless, it may help to understand the basics by going with this otherwise common assumption.↩︎\nRemind you of the classical measurement theory’s concept of reliability?↩︎\nA variance/covariance matrix—also simply called a covariance matrix for short—is simply a correlation matrix before the values in it are standardized. (Remember, correlations are measures of shared variance—that are standardized to values between 0 and 1 (and designated as positive or negative depending on the direction of the values).)↩︎\n“Multivariate” here means that there are more than one criteria (“DVs”) and “multiple” means there’s more than one predictors (“IVs”); the criteria here are the non-ostensible factors we’re estimating, and the predictors are the ostensible variables (e.g., items).↩︎\nSo far, I’ve talked about the items that are analyzed for their inter-correlations. However, we could include any ostensible variable—not just the items of an instrument. We could include demographic variables, scores on other measures, or even predictors (IVs) and criteria (DVs). When we include predictors and criteria in our “factor analyses,” we are simply starting to turn them into structural equation models.↩︎\nThere are other criteria one can use to determine how many factors to keep, including computing a \\(\\chi^{2}\\) based on values from the matrix of residuals (i.e., what’s left over in the variance/covariance matrix after running the factor analysis), the number of items, and the sample size. But, whatever criteria we use, it’s never seen as meaningful to retain any factor whose eigenvalue is less than 1.↩︎\nI simply made that figure in an spreadsheet program, so I just made up the item loadings. If you’re curious, their actual values are: 3.0, 2.7, 1.3, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, and 0.35 (which do indeed add up to 12).↩︎\nIt’s called “Kaiser’s criterion” after the research who first espoused it, Henry Kaiser, in Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39, 31 – 36; a clarifying review of that seminal article is here.↩︎"
  },
  {
    "objectID": "Using_Templates_and_a_Reference_Manager.html#overview",
    "href": "Using_Templates_and_a_Reference_Manager.html#overview",
    "title": "13  Using Templates and a Reference Manager",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nThis guide covers how to install and use both a style template and reference manager with Microsoft (MS) Word (after a brief description of their use with LibreOffice Writer)."
  },
  {
    "objectID": "Using_Templates_and_a_Reference_Manager.html#style",
    "href": "Using_Templates_and_a_Reference_Manager.html#style",
    "title": "13  Using Templates and a Reference Manager",
    "section": "13.2 Style Template",
    "text": "13.2 Style Template\nThe idea behind style templates is that you separate out the plain-text content of a file from the way is is formatted, including the typeface/font, colors, how phrases are emphasized (e.g., with italics), etc. Separating out these two things greatly helps documents move from one platform or program to another. It also helps if, e.g., you submit a manuscript to one journal that uses APA format, but it gets rejected and you want to then submit it to another journal that uses, say, MLA. With templates, you simply choose a new template; without templates, you spend your afternoon plodding through your manuscript re-formatting headers, redoing every citation, etc. Using style templates thus help write because it lets you not worry about how to even put it into, say, APA format in the first place.\n\n13.2.1 Loading Templates in MS Word\nYou can create your own template1, but it’s nearly always easier to use an existing one. There are several APA templates available online, including an “official” one from Microsoft2, ones from MS proponents, from coders, and from other schools. The one we’ll use is available here.\n\nLoading a Template for the First Time\n\nDownload the APA_7th_Ed_Template.dotx Word template to your computer.\nOpen this file in MS Word.\nChoose to Save As the file\n\nNear the bottom of the dialogue box that opens, under Save as type, select to save it as a Word Template.\nWord should automatically save it in the Documents &gt; Custome Office Templates folder. This will allow you to more easily access this template in the future (as we’ll discuss just below)\n\n\nNote that you can now use the template to write your paper. Now, though, when you save it, save is as a normal .docx file—not a .dotx template. (You can save it as a .dotx file and use that, but Word treats them differently and this will create a bunch of un-needed templates.)\n\n\nLoading an Existing Template\nNow that you’ve loaded the template into Word, you can more easily reuse it. Let’s start from the top to do this, so first close that template file and Word. Now, re-open Word.\n\nFrom the Opening Dialogue\n\nWord, of course, opens to a dialogue that lets you choose recent files from the left-hand menu or to open a new file, using any of the mostly-useless templates that it gives in the main window on the right.\nYou most likely only ever chose to open a Blank document, but now you should see the APA template you saved listed among all of the other templates.\n\n\n\n\nOpening the Template You Added to Word\n\n\n\nIf you don’t immediately see it, click on the word Personal next to the currently-highlighted Featured word.\nYou should see your APA template there—perhaps even as the only Personal template.\n\nThere is more to adding, opening, and modifying templates. TechRepublic has a good coverage of that, so we’ll jump now to using them.\n\n\n\n\n13.2.2 Loading Templates in LO Writer \nLibreOffice Writer—a free alternative to MS Word—works well with style templates.\nAlthough there is an extension you can use to apply a style template, this is easily done by:\n\nOpen a template like you would any other file in LO Writer. A template for the 7th edition of the APA Publication Manual is here.\nClick on Insert &gt; Document3.\nSelect the existing document you want to apply the style to.\nSave As a new version of the document that’s either an ODF Text Document (.odt) or Word 2007--365 (.docs) file. I.e., that’s not a template (.ott) file.\n\nYou can most easily change the style template being used with the Template Changer extension:\n\nDownload & load the extension\n\nDownload the Template Changer extension\nIn LO Writer, click on Tools &gt; Extension Manger... (Cntl + Alt + E)\nIn the dialogue that opens, choose to Add, and then navigate to where you downloaeed the extension\nSelect and Open the downloaded extension (or, of course, just double-click on it)\nAccept the license agreement (if you indeed do)\nRestart LO Writer\n\nClick File &gt; Templates &gt; Change template (current document)...\n\n\n\n13.2.3 Using a Template\n\nMS Word\nThe template we’re using already has the outline for a typical article manuscript (and a few other things we’ll discuss later in this guide). You really simply need to start typing, and then Save as a normal .docx file to use it. There are a few things to point out and practice, though.\nAgain, you can use a template to do things like italicize words, but the main use will be for the section headings and sub-headings. To use the style template:\n\nUnder the Home ribbon, notice the Styles section\nIn the lower, right corner of the Styles section, click on the tiny box-with-arrow button to expand that section…\n\n\n\n\nOpening the Styles Menu\n\n\ninto a menu that defaults onto the right of the Word window:\n\n\n\nThe Styles Menu\n\n\nYou can now more easily use the styles.\nI labelled the headings levels in the template4, but you can also tell what formatting is being applied to a given section by left-clicking on a piece of text; the formatting that’s being applied will be highlighted in that right-hand menu.\nClicking, e.g., on the document’s Title shows that it’s being formatted as a Level 1 heading. If you click on, say, Heading 2 in that menu, you will see that the section that’s currently chosen will change its formatting to be like the other Level 2 headings (like Participants is formatted under the Methods section).\nNormal text also has its own style. Clicking on the normal text under Descriptive Statistics in the Results section shows that it’s formatted as Body Text. We could have used the Normal formatting just above Body Text in the menu; it really makes no difference—as long as both of those style elements are formatted the same (as they are here).\nYou can easily switch between style elements by clicking on different elements in the styles menu for a given piece of text. Note that headings and title elements in the menu will be applied to an entire paragraph5 while most other elements can be applied to single words (or other parts of a paragraph); there is a small pilcrow (¶) next to paragraphs styles in that menu and a small letter a next to non-paragraph styles.\n(The Figure 1 title in the template is styled as Strong to show you how you could use styles to even make bold and italics text, even though you’ll likely just use Control/Command + B and Control/Command + I instead.)\nNote that you can change the styling of the style elements (e.g., change how Level 1 headers look). To do this, simple right-click on an element in that styles menu and select Modify.... You can change the font elements, or—from the Format button at the bottom of the dialogue that opens—change the styling of the whole paragraph (or other things like the page border). When you open this Modify Style dialogue, note that you can apply this modification to just this file or to the template itself for use in other files:\n\n\n\nApplying style element modifications to either just this file or to the template itself\n\n\nAmong the changes you may want to make to this template are taking out the sub-headings under Participants. I put them there simply to show all of the heading levels that APA allows, what they look like, and where they’re located in the styles menu.\n\n\n\n\n\n\nNote\n\n\n\nWhen using a template to write a manuscript, remember to Save as a normal .docx file once you’ve started using. This will keep the styling but not screw up your template file.\nIf you want to make changes to the template, then Save as a .dotx file instead.\n\n\n\nApplying a Template to an Existing Document in Word \nIt is relatively easy to apply a new template to an existing document—even if Word doesn’t make the steps to do so intuitive.\nThere are indeed times when you may want to apply a new template to an existing document. Many journals, for example, require you to format your submitted manuscript into their own style. So, if you submit a manuscript for publication to one journal, have it rejected, and then apply to an other journal, you will need to reformat the entire document for the second journal. This is a lot easier to do by simply applying a new style template to it.\nPlease note that this is more useful if the existing document you’re applying the new template already used styles to format the headings, etc. I covered how to do this above in Section 13.2.3.1.\nThe steps to doing so for MS Word are here. To summarize those steps (while only stealing a few of their nice graphics):\n\nOpen your existing document in MS Word\nClick on File &gt; Options.\nIn the dialogue box that opens, click on Add-ins near the bottom of the menu on the left. This is step 1 in the image below.\nIn the main part of that dialogue box, there is a Manage section near the bottom. From that, choose Templates.\nClick Go right next to where you selected Templates. \nIn the new dialogue box that opens, make sure to select Automatically update documet styles. \nClick Attach and then OK in that same dialogue.\nNavigate to the new style template (i.e., the .dotx file) you wish to use. Select it, and click Open at the bottom.\nClick OK when you return to the previous dialogue (shown in Templates and Add-ins dialogue).\nYou can now (re)save your document with the new style applied.\n\n\n\n\nLO Writer\n\nOpen a style template file in Writer. Templates’ extension is .ott; an example of one is this APA 7th Edition template.\nSave the file as an ODF Text Document (.odt) file; this is the default extension for all open document text files, like those created by LO Writer.\nThere are three ways to change styles of parts of the document:\n\nClick on the Styles drop-down menu\nClick on the Styles drop-down menu that may appear in the the toolbar.\n\nThis option may not be added by default, depending on the version of Writer, but can be easily added via Tools &gt; Customize... &gt; Toolbars where it can be added to one of the toolbars.\n\nTapping F11 to open aside menu with all of the various types of format-able fields.\nUsing the keyboard directly where, e.g.:\n\nCntl + 1 (Command on a Mac) formats the paragraph as a level 1 header\nCntl/Command + 2 formats as a level 2 header, etc.\nCntl/Command + 0 formats as text body\n\n\n\nI find using Cntl/Command + works best since I can simply type my manuscript and just hit, e.g., Cntl/Command + 1 and keep typing, say, “Methods,” hit Enter to start a new paragraph, hit Cntl/Command + 2 and type “Participants,” hit enter again, and then start writing that section. That’s it, with just a few key combinations to remember you can not worry about formatting.\nAnd you may have noticed in what I wrote, most styles are applied to paragraphs, not—say—individual words or phrases inside a paragraph. You can use styles to do this (and sometimes I do), but just Cntl/Command + I to italics, Cntl/Command + B for bold, etc. work just as well; using those common key combinations will apply whatever style has been set for italics, bold-face, etc."
  },
  {
    "objectID": "Using_Templates_and_a_Reference_Manager.html#ref-manager",
    "href": "Using_Templates_and_a_Reference_Manager.html#ref-manager",
    "title": "13  Using Templates and a Reference Manager",
    "section": "13.3 Reference Manager",
    "text": "13.3 Reference Manager\nStyling documents is one thing that computers ought to do well and relieve us from having to do ourselves. Managing our citations is certainly another. A reference manager allows you to collect and organize your citations. More importantly here, it lets you add them to your manuscript easily and to create your References section automatically. The only issue is if you collaborate with others on the same manuscript, you’ve got to use the same citation file format (.bib vs. .ris etc.) \nI will cover two reference managers, RefWorks (Section 13.3.1) and Zotero (Section 13.3.2). RefWorks is useful for MS Word on Windows & Apple OSs; Zotero is good for GNU/Linus OSs. Please note that other popular reference managers include Mendeley, Endnote, and RefWorks. I used to use JabRef and still kinda miss it.\n\n13.3.1 RefWorks\nRefWorks is a web-based reference management tool that helps researchers organize citations, generate bibliographies, and integrate seamlessly with word processors. This tutorial provides step-by-step instructions for:\n\nDownloading and setting up RefWorks (Section 13.3.1.1).\nInstalling and using RefWorks Citation Manager (RCM) in Microsoft Word (Section 13.3.1.2).\nUsing RefWorks with Google Docs (Section 13.3.1.3)\nUsing RefWorks with LibreOffice Writer (Section 13.3.1.4).\n\n\nDownloading and Setting Up RefWorks\nRefWorks is a web-based citation management tool, meaning it does not require installation on your computer. However, it does have browser plugins, word processor add-ons, and import/export tools that need setup.\n\nStep 1: Creating a RefWorks Account\n\nGo to the RefWorks website.\nClick on Create Account and go through the steps listed.\n\nRefWorks asks for your institutional email; using this should let you use RefWorks for free. Hopefully you still can after you graduate.\n\nFollow the registration prompts to set up your account.\n\n\n\nStep 2: Installing Browser Extensions (Optional)\nRefWorks offers browser extensions for importing citations from websites.\n\nChromium / Google Chrome:\n\nInstall the Save to RefWorks extension from the Chrome Web Store.\nClick on the extension icon and log into your RefWorks account.\n\nFirefox:\n\nInstall the Save to RefWorks add-on from the Firefox Add-ons Store.\nClick on the extension and sign in.\n\n\n\n\n\nUsing RefWorks with Microsoft Word\nRefWorks integrates with Microsoft Word via RefWorks Citation Manager (RCM), available as an add-in.\n\nInstalling RefWorks Citation Manager (RCM) in Microsoft Word\n\nWindows and macOS:\n\nOpen Microsoft Word.\nGo to Insert &gt; Get Add-ins.\nSearch for RefWorks Citation Manager.\nClick Add to install.\nOpen the RefWorks Citation Manager from the References tab.\nLog into your RefWorks account.\n\n\n\n\nUsing RefWorks in Microsoft Word\n\nOpen RefWorks Citation Manager in Word.\nSelect a citation from your RefWorks library.\nClick Insert Citation to add it to your document.\nTo create a bibliography:\n\nClick Bibliography Options &gt; Create Bibliography.\nSelect a citation style (e.g., APA, MLA, Chicago).\n\n\n\n\n\nUsing RefWorks with Google Docs\n\nInstalling and Using RefWorks in Google Docs\n\nOpen Google Docs.\nClick on Extensions &gt; Add-ons &gt; Get add-ons.\nSearch for RefWorks and install the RefWorks Citation Manager add-on.\nOpen the add-on by navigating to Extensions &gt; RefWorks Citation Manager &gt; Start.\nLog into your RefWorks account.\n\n\n\nUsing RefWorks in Google Docs\n\nOpen RefWorks Citation Manager in Google Docs.\nSelect a citation from your RefWorks library.\nClick Insert Citation to add it to your document.\nTo create a bibliography:\n\nClick Bibliography Options &gt; Create Bibliography.\nSelect a citation style (e.g., APA, MLA, Chicago).\n\n\n\n\n\nUsing RefWorks with LibreOffice Writer\nRefWorks does not have a direct plugin for LibreOffice Writer, but you can still use it effectively.\n\nMethod 1: Manually Exporting Citations\n\nIn RefWorks, select the citations you want.\nClick Export &gt; Bibliographic Software.\nChoose RIS Format and download the file.\nOpen LibreOffice Writer, insert references manually.\n\n\n\nMethod 2: Using the Write-N-Cite Tool\nWrite-N-Cite is available for Windows and macOS, but not for GNU/Linux. It allows inserting citations directly in LibreOffice.\n\nDownload Write-N-Cite from RefWorks (link).\nInstall the tool and log in.\nIn LibreOffice, open Write-N-Cite and insert citations.\n\n\n\nMethod 3: Using the Quick Cite Feature\n\nIn RefWorks, go to Cite &gt; Quick Cite.\nSelect citations and copy-paste them into LibreOffice Writer.\n\n\n\nTroubleshooting and Additional Tips\nCommon Issues and Solutions\n\n\n\n\n\n\n\nIssue\nSolution\n\n\n\n\nRefWorks add-in not appearing in Word\nRestart Word and reinstall from the Add-ins store.\n\n\nSave to RefWorks not working in browser\nClear cache and reinstall the extension.\n\n\nBibliography formatting issues\nEnsure correct citation style is selected in RCM.\n\n\nLinux users can’t use RCM\nUse Quick Cite or export citations manually.\n\n\n\nFor more information, visit the RefWorks Help Center.\n\n\n\n\n13.3.2 Zotero\n\nZotero has a rather user-friendly set of guides in their Documentation section. I’ll cover only the most common features now.\n\nGo to the Zotero Download page where there should be shown options to install it into your given operating system.\n\nNote that you can also install it into your browser from there. More about this later.\n\nWhen you run the file you just downloaded, it will install Zotero on your computer.\n\nIt should also automatically integrate itself into Word. If you need to, though, you can manually install that Word functionality.\n\n\n\nAdding Citations to Zotero\n\nI’ve added a few sources into a file in BlackBoard as Sources to Add to Zotero, but you can also download that same file here. Do please save that file to your computer.\nIn Zotero, click on File &gt; Import... (or Control/Command + Shift + I)\nIn the dialogue box that opens, leave selected A file (BibTeX, RIS, Zotero RDF, etc.) and click Next\nNavigate to the file you downloaded (it’s named zotero_sources.bib) and Open it\nIn the Options dialogue, I suggest unselecting Place imported collections and items into a new collection6 before clicking Next then Finish\n\nTa-da! You have your first set of citations.\nYou can also get citations from, e.g., the Hunter library:\n\nGo to the main page and enter something into the OneSearch field.\nClick on a source you like to open the page about it.\nUnder the Send to section, click on the Export RIS \nFor Encoding, choose UTF-8 and then click Download and then Save it\nYou can now navigate to this file and import it just like you did that one I created for you.\n\nNote that a good number of publishers will allow you to download articles and citations directly to your library. So, sometimes it’s worth clicking to View Online in the Hunter library page for a source, go to the article’s site under the publisher, and download it directly to Zotero from there.\nIf you’ve added the browser extension, you can also use that both to access citations (through your Zotero account if you’ve set on up) or to download sources through that. This works rather well (in FireFox at least; I rarely use Chrome/Chromium or Safari).\n\n\nSyncing Library\nIf you want, you can to Register for an account at Zotero. Doing this allows you to upload your library to their server and to use that to sync your library of sources across machines. There are options to pay for more storage, but I doubt you’ll need to do that; I accidentally have over 13,000 sources in my library7 and still haven’t run out of room.\nOnce (if) you’ve set up an account, you can click on the Web Library link to access your citations online. You can also synchronize them with your local instance:\n\nIn Zotero on your computer, click on Edit &gt; Preferences\nClick on the Sync tab\nClick to Link your account\n\nI suggest selecting to Sync automatically and perhaps to select all other options to syn full text, attachments, notes, etc.\n\nZotero will ask you to log in; with larger libraries, it can take several minutes to sync the first time, but subsequent syncs are as quick as any such operations with, e.g., Dropbox.\n\n\n\nUsing Zotero to Cite Sources in Word\n\nSetting up Preferences\n\nMake sure Zotero is open\nIn Word, open/create the file you’d like to import citations into\nThere should be a Zotero tab near the right end of your list of ribbons; if not, can manually install it.\nGo to the place in the text where you want to insert the citation; leave the cursor there\nClick on the Zotero tab to access that ribbon\nClick on Document Preferences in that ribbon\nIn the dialogue that opens, choose the citation style you want to use from the Citation Style list\n\nUnless you work with those like me who use LibreOffice, leave it to Store Citations as Fields and make sure Automatically update ciations is also selected.\n\n\n\n\nTo Add a Citation in Text\n\nMake sure Zotero is open\nIn Word, open/create the file you’d like to import citations into\nThere should be a Zotero tab near the right end of your list of ribbons; if not, can manually install it.\nGo to the place in the text where you want to insert the citation; leave the cursor there\nClick on the Zotero tab to access that ribbon\nClick on the Add/Edit Citation button\nA very slim dialogue opens where you can search for the citation(s) to add: \n\nNote that you can search by author, title, journal, keyword, etc.\n\nSelect the citation you want from the drop-down list\n\nIf you want to add more citations, simple search for and choose them as well from that same dialogue\n\nYou now have a few options for how to include that citation:\n\nIf you leave it as it is and simply hit Enter, it will add a parenthetical citation, e.g., “(Cohen, 1988)”\nIf you instead left-click on the citation, an other dialogue will open:\n\nSelecting to Omit Author will add just the data in parentheses, e.g., “(1988)”; you would simply type in, e.g., “Cohen” before that to note the authors\nSelecting Page will allow you to add page numbers, e.g., “(Cohen, 1988, p. 200 – 201)”\nPrefix allows you to add text before the citation in the parentheses, e.g., “(see Cohen, 1988)”\nSuffix allows you to add text after it, e.g., “(Cohen, 1988, and others)”\n\n\n\n\n\nTo Create a Reference Section\nSimply click on the Refresh button in the Zotero ribbon. This will create and update your reference section with any citations you’ve added or removed. In fact, having selected Automatically update ciations under Document Preferences should suffice.\n\n\n\nCreating Figure and Table Fields\nOne more thing to automate: the numbering of figures and tables. This is a bit kludgy in Word, but still worth it.\n\nAdding and Captioning a Figure or Table\n\nAdd your figure or table\n\nFigures are usually inserted as simple image files (Insert &gt; Pictures)\nTables are best done via:L\n\nInsert &gt; Table\nUse your mouse to drag your cursor to select the number of rows and columns you want (or choose Insert Table there to type in the number of columns & rows along with formatting of them)\n\n\nTables should automatically gain a caption, but figures won’t. To add a caption for a figure:\n\nRight-click on the image\nClick on Insert Caption\nUnder Options, choose Figure\nUnder Caption, maybe type in a title for your figure. Note that this will put it on the same line as “Figure 1” which is not strictly APA; APA dictates adding the title on the line below “Figure 1,” as I did in the template\nUnder Position, choose Above selected item since that’s what APA wants\nSelect to Exclude label from caption\nClick OK\n\n\n\n\nInserting Updatable References to Figures & Tables\n\nNow, go to an other place in the manuscript where you want to reference that figure or table.\n\nUnder the References tab, click on the Cross-reference button: \nUnder Reference type, choose Figure or Table as applies\nSet Insert reference to Entire caption (unless you didn’t select to Exclude label from caption from the Caption dialogue or, of course, if you want to include it)\nDoing all of this should produce a list of the figures or tables in the For which caption field; simply choose the one you want.\n\nWord will update the numbering of figures and tables . . . if you ask it to; it won’t do it automatically. To update them:\n\nAs per these instructions, you can choose to update them by selecting text with similar formatting\nOr simply select all text (Control/Command + A)\nRight-click on the reference to a figure/table in the text and choose Update field from the drop-down menu that appears\nIf a dialogue opes asking what to update (“Update Table or Figures?”), then choose Update entire table\nAnd click OK\n\n\nThat should do it. Again, it is a bit kludgy in Word. I included an example of such a cross reference in that template—yet another thing you may wish to delete before making that template your own."
  },
  {
    "objectID": "Using_Templates_and_a_Reference_Manager.html#additional-resources",
    "href": "Using_Templates_and_a_Reference_Manager.html#additional-resources",
    "title": "13  Using Templates and a Reference Manager",
    "section": "13.4 Additional Resources",
    "text": "13.4 Additional Resources\n\nWorking with tables\n\nPutting tables in APA format\nMore manipulations of descritpive tables including removing columns and rows and pivoting tables"
  },
  {
    "objectID": "Using_Templates_and_a_Reference_Manager.html#footnotes",
    "href": "Using_Templates_and_a_Reference_Manager.html#footnotes",
    "title": "13  Using Templates and a Reference Manager",
    "section": "",
    "text": "And the one we’ll use is one that I created.↩︎\nAlthough MS offers none for the current edition of the APA Manual, and they have not put much effort into making such templates complete or ensuring their accuracy. After all, it’s hard to support both another billionaire CEO and quality products!↩︎\nIn the Ubuntu version of Writer, the menu selections are Insert &gt; Text from File....↩︎\nOnce you’re more familiar with doing this, you may want to delete those extra cues—and the extra text—and re-save this as a cleaner .dotx template file.↩︎\nThis actually presents a problem for APA style. APA dictates that level 4 and 5 headings show not be on their own line, but simply at the beginning of a paragraph, with the rest of the paragraph formatted like normal text. I don’t know how to get around that without some kludgy elements that, say, have negative margins on the bottom.↩︎\nYou can create several libraries, e.g., one for each line of research you pursue. However, I find that gets complicated and doesn’t really help. So, I put all of my citations into one library.↩︎\nMistaken duplications↩︎"
  },
  {
    "objectID": "Intro_to_Excel.html#overview",
    "href": "Intro_to_Excel.html#overview",
    "title": "14  Introduction to Excel",
    "section": "14.1 Overview",
    "text": "14.1 Overview\nThis activity is intended to review some of the main ways that you may use a spreadsheet program (viz., MS Excel) to manipulate, review, and even analyze data."
  },
  {
    "objectID": "Intro_to_Excel.html#filling-in-series",
    "href": "Intro_to_Excel.html#filling-in-series",
    "title": "14  Introduction to Excel",
    "section": "14.2 Filling in Series",
    "text": "14.2 Filling in Series\n\nEnter the first few values of a series in a row or column\nHighlight those cells\nMove cursor to bottom right corner of the last cell\n\nThe cursor should change shape (e.g., from a fat, white cursor to a skinny, black one)\n\nHold down the (left) mouse button\nDrag the cursor right/down to fill in more values of that sequence\n\nExcel (and LO Calc, Gnumeric, etc.) are pretty good at recognizing what the series should be. E.g., counting by even numbers, counting by tens."
  },
  {
    "objectID": "Intro_to_Excel.html#pasting-special-transposing",
    "href": "Intro_to_Excel.html#pasting-special-transposing",
    "title": "14  Introduction to Excel",
    "section": "14.3 Pasting Special & Transposing",
    "text": "14.3 Pasting Special & Transposing\n\nData can be pasted with various levels of formatting retained or removed\nThis is especially useful in two situations:\n\n1. Converting formulas to raw numbers\n\n(After creating some range of cells that contains formulas), select the range of cells you’d like to convert from formulas to raw numbers\nCopy that range (Control + C)\nMove to the range of cells you’d like to paste it to\n\nYou can paste right back into the same range if you’d just like to convert those formulas to values there\n\nOpen the Paste Special dialogue either by typing Control2 + Alt3 + V. Or:\n\nClick on the File tab, and then the little down-pointing arrowhead under the Paste button\nEither choose the type of Special pasting you’d like to do, or clicking on the Paste Special button\n\nSelect to paste Values (or just type V)\n\n2. Transposing Data \nTransposing means to flip down from, e.g., going from top to bottom instead to be going from left to right. To do this:\n\nSelect & copy the range of cells to transpose\nMove to where you’d like them transposed\nAccess the Paste Special dialogue (e.g, Control + Alt + V)\nType T to select transpose, or select that option from the dialogue box.\n\n\nMore at, e.g., ablebits.com or extendoffice.com"
  },
  {
    "objectID": "Intro_to_Excel.html#navigation",
    "href": "Intro_to_Excel.html#navigation",
    "title": "14  Introduction to Excel",
    "section": "14.4 Navigation",
    "text": "14.4 Navigation\n\n14.4.1 Navigating & Selecting Within a Sheet \n\n\nTable 14.1: Key Commands to Navigate Within a Sheet in Excel\n\n\n\n\n\n\nKey(s)\nFunction\n\n\n\n\nControl\nGo to the beginning/end of a string (column or row) of cells with values in them\n\n\nShift\nSelect cells\n\n\nControl + Shift\nSelect all cells in a string\n\n\nHome\nBeginning of row\n\n\nControl + Home\nTop left of sheet\n\n\nEnd\nEnd of row\n\n\nControl + Spacebar\nSelect entire column\n\n\nShift + Spacebar\nSelect entire row\n\n\nAlt + Page Down\nMove one screen to the right in a worksheet\n\n\nAlt + Page Up\nMove one screen to the left in a worksheet\n\n\n\n\n\n\n14.4.2 Navigating Between Sheets\n\n\nTable 14.2: Key Commands to Navigate Between Sheets in Excel\n\n\nKey Combination\nFunction\n\n\n\n\nShift + F11\nInsert new sheet (to the right)\n\n\nControl + Page Down\nSwitch to next sheet to the right\n\n\nControl + Page Up\nSwitch to next sheet to the left\n\n\n\n\nMore key shortcuts are at:\n\nThis site with a few of the most common and\nThis MS site"
  },
  {
    "objectID": "Intro_to_Excel.html#formulas",
    "href": "Intro_to_Excel.html#formulas",
    "title": "14  Introduction to Excel",
    "section": "14.5 Formulas ",
    "text": "14.5 Formulas \nFormulas in Excel and Calc allow you to quickly conduct many. useful operations on your data. It’s even sometimes easier to use these here than to do themin SPSS, R, or an other stats program. Using formulas in a spreadsheet program helps you get up close and personal with your data before analyzing them in those stats programs, too.\nI will cover a few that I use most often in this section. Among the additional resources to consider are:\n\nExcelGPT (aka ExcelBrew), which uses AI to create formulas based on your description of the action(s) you’d like to do\n\n\n14.5.1 General Steps for Entering Formulas (and Common Formulas to Use)\nTo begin to enter a formula in a cell:\n\nType = within a cell to enter a formula\nAlt, M: Go to formula tab\nCommon Formulas:\n\n\n\nTable 14.3: Common Formulas in Excel\n\n\nFunction\nExcel Formula\n\n\n\n\nSum of range\n=sum(cell range)\n\n\nMinimum value of range\n=min(cell range)\n\n\nMaximum value of range\n=max(cell range)\n\n\nNumber of cells with numbers in a range\n=count(cell range)\n\n\nNumber of cells with words in a range\n=counta(cell range)\n\n\nMean of range\n=average(cell range)\n\n\nStandard deviation of range4\n=stdev.s(cell range)\n\n\nGenerate a random number5 from 0 – 1\n=rand()\n\n\nGenerate a random whole number between a range\n=randbetween(lower value,upper value)\n\n\n\n\n\n\n14.5.2 if\nThe if function evaluates a cell and returns different values if the contents of that cell do or do not match some criterion.\nFor example, I could have gender coded in one cell as a 1 if that person identifies as female or a 0 if they identify otherwise6. I may then want to have an other cell that actually spells out what that 1 and 0 mean in case I forget. I could use if to do this.\nThe general format for an if function is:\n=if(Some statement,What to output it the statement is true,What to output it the statement is false)\nTo continue with that example, let’s imagine that the cell with the dummy-code for female is in cell A2 and I want to put the phrase Female or Not Female in cell B2, like this in Excel:\n\n\n\nRow/Column\nA\nB\n\n\n\n\n1\nDummy Variable\nCategory Label\n\n\n2\n1\nFemale\n\n\n\nTo do this, I would type the following into cell B2, i.e., the cell where I want the result to go:\n\n=if(A2 = 1, \"Female\", \"Not Female\")\n\nTo break down the parts of an if statement doing this:\n\n=if(\n\n\nThis first part starts with an equal sign (=) letting Excel know that you’ll be entering a function\nThe if( lets it know what the function is you’ll be using\n\n\nA2 = 1,\n\n\nThis tells Excel where to look to evaluate an argument (we’re telling it to look in cell A2)\nIt then tells Excel what the formula is to evaluate. Here, we’re asking whether the value in cell A2 is equal to 1 or not (= 1)\n\n\n\"Female\",\n\n\nThis is the value we’re asking Excel to return if indeed the value in cell A2 equals 1. If A2 equals one, we want Excel to print out the word Female.\n\nSince we’re asking Excel to print out a word, we have to put it in quotes.\nIf we’d asked Excel to print out a number, we would not put it in quotes7.\n\n\n\n\"Not Female\")\n\n\nThis is what we want Excel to print out if our formula (A2 = 1) is incorrect, i.e., if the value in cell A2 is anything besides a 1.\n\nIf we had more cells in that first column—other gender identities for other participants, it could look like the following table—noting that I’ve added in a column to show what the function would look like right before what Excel would produce in that column:\n\n\n\n\n\n\n\n\n\nRow/Column\nA\nB\n\n\n\n\n\n1\nDummy Variable\nFunction Typed into Cell in Column B\nExcel Output in Column B\n\n\n2\n1\n=if(A2 = 1, \"Female\", \"Not Female\")\nFemale\n\n\n3\n0\n=if(A3 = 1, \"Female\", \"Not Female\")\nNot Female\n\n\n4\n3\n=if(A4 = 1, \"Female\", \"Not Female\")\nNot Female\n\n\n5\nNA\n=if(A5 = 1, \"Female\", \"Not Female\")\nNot Female\n\n\n6\n\n=if(A6 = 1, \"Female\", \"Not Female\")\nNot Female\n\n\n\n\nNote that cell A6 is empty, i.e., row 6 has an empty cell in column A. Excel interprets an empty cell as not meeting the evaluated condition (here that A6 = 1).\n\nif statements are very useful. They can evaluate several operations:\n\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\nResult\n\n\n\n\n=\nEqual to\n=if(A2 = B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is the same as the value in B2\n\n\n&lt;&gt;\nNot equal to\n=if(A2 &lt;&gt; B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is different than the value in B2\n\n\n&gt;\nGreater than\n=if(A2 &gt; B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is greater than the value in B2, e.g., if A2 = 2 and B2 = 1\n\n\n&gt;=\nGreater than or equal to\n=if(A2 &gt;= B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is greater than or equal to the value in B2, e.g., if A2 = 2 and B2 = 1 or of B2 = 2\n\n\n&lt;\nLess than\n=if(A2 &lt; B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is less than the value in B2, e.g., if A2 = 5 and B2 = 10\n\n\n&lt;=\nLess than or equal to\n=if(A2 &lt;= B2, \"TRUE\", \"FALSE\")\nReturns TRUE the value in A2 is less than the value in B2, e.g., if A2 = 5 and B2 = 10 or if B2 = 5\n\n\n\nIn addition to recoding, you can use it to test if cells are blank (=if(A2 = \"\", \"Blank\", \"Not Blank\")), compute different formulas based on different values (e.g., return the absolute value if a cell is negative: =if(A2 &lt; 0, abs(A2), A2)), etc.\n\nNesting if Statements\nif statements can be nested. For example, I use the following formula to compute letter grades from a percent grade given in cell A1. I can just drop this formula into, say, cell B1 and it will automatically give me the letter grade using CUNY’s conversion standards8:\n\n=IF(A1&gt;=97.5,\"A+\", IF(A1&gt;=92.5,\"A\", IF(A1&gt;=90,\"A-\", IF(A1&gt;=87.5,\"B+\", IF(A1&gt;=82.5,\"B\", IF(A1&gt;=80,\"B-\", IF(A1&gt;=77.5,\"C+\", IF(A1&gt;=70,\"C\", IF(A1&gt;=60,\"D\", IF(A1&lt;60,\"F\"))))))))))\n\nNote that there is an other Excel function, ifs that makes the syntax for nesting if statements a bit cleaner, but I personally prefer seeing it all spelled out—even if it means having a bunch of parentheses at the end.\nThe ifs statement for that same coding of percents into letter grades is:\n\n=IFS(A1&gt;=97.5,\"A+\", A1&gt;=92.5,\"A\", A1&gt;=90,\"A-\", A1&gt;=87.5,\"B+\", A1&gt;=82.5,\"B\", A1&gt;=80,\"B-\", A1&gt;=77.5,\"C+\", A1&gt;=70,\"C\", A1&gt;=60,\"D\", A1&lt;60,\"F\")\n\n\n\n\n14.5.3 vlookup\nI love vlookup. It’s such a powerful way to recode variables based on some criterion. For example, I could recode all males to 0 and females to 1 in just a few steps.\nvlookup is a vertical lookup that you use to fill in values down a column. You can use hlookup to look up values to a row (not a column) of data with values also looked up in rows of data. (Apparently xlookup is a new Excel command that knows which way the data are being read and looked up, so can be used instead of both vlookup and hlookup, but I’ve not gotten it to work.)\n\nFormula:\n\n=vlookup(cell to look up, range of cells to find replacement value, column in range for value to return, match mode)\n\nMatch modes:\n\n\n\nTable 14.4: vlookup Formula Match Modes in Excel\n\n\nValue to Enter\nDescription\n\n\n\n\n0\nExact match. If none found, returns #N/A. This is the default\n\n\n-1\nExact match. If none found, return the next smaller item\n\n\n1\nExact match. If none found, return the next larger item\n\n\n2\nA wildcard match where *, ?, and ~ are “wildcards”, MS’s pathetic attempt at regular expressions\n\n\n\n\n\nTo “lock” part of a cell reference, add $ right before it\nE.g.:\n\nTo keep the reference for replacement values “locked” to cells A1 through B12 (written in the formula as A1:B12) when filling in cells below with that formula,\nAdd a $ before the 1 and the 12, making it A$1:B$12 \n\n\n\nIndex and Match\nYou can get similar results by instead using the index and match commands. Since I prefer vlookup, for now, I will simply link to this ExcelJet page that covers them well."
  },
  {
    "objectID": "Intro_to_Excel.html#pivot-tables-charts",
    "href": "Intro_to_Excel.html#pivot-tables-charts",
    "title": "14  Introduction to Excel",
    "section": "14.6 Pivot Tables & Charts",
    "text": "14.6 Pivot Tables & Charts\nPivot tables are a great way to quickly generate descriptive statistics for categories. They are also flexible so that you can look at subgroups or “cross tables” that show, e.g., the stats for two variables that are crossed with each other (like looking at, say, the hip-waist ratios of genders crossed with ethnicities/races).\n\n14.6.1 Inserting/Creating a Pivot Table or Chart\nTable:\n\nInsert tables using:\n\nAlt, N, V\n\nGUI:\n\nIn the Tables group, click on the Insert tab\nClick on PivotTable\nClick OK\n\n\nMore here\n\n\nChart:\n\nInsert charts using:\n\nAlt, N, S, C\n\nGUI:\n\nIn the Tables group, click on the Insert tab\nClick on PivotChart\nClick OK\n\n\n\nMore guides on pivot tables:\n\nhttps://magoosh.com/excel/excel-pivot-chart/\nhttps://blog.hubspot.com/marketing/how-to-create-pivot-table-tutorial-ht\nhttps://www.guru99.com/pivot-tables-in-excel-beginner-s-guide.html"
  },
  {
    "objectID": "Intro_to_Excel.html#basic-statistics",
    "href": "Intro_to_Excel.html#basic-statistics",
    "title": "14  Introduction to Excel",
    "section": "14.7 Basic Statistics",
    "text": "14.7 Basic Statistics\n\nMost of the statistical analyses are within the Data tab under the Data Analysis button\n\n\n14.7.1 Correlations\n\n\n\n\n\n\n\nFunction\nFormula\n\n\n\n\nReturn a correlation matrix\n=correl(cell range 1,cell range 2)\n\n\n\nCreating correlations (and many other descriptive & inferential stats) is also accessible via a GUI in the Analysis ToolPak described under ANOVAs, below.\n\n\n14.7.2 t-Test \nThis formula returns the p-value for the t-test. It does not return the actual t-score.\nFormula:\n=t.test(group 1 column range,group 1 column range,tails,type)\n\nTails:\n\n\n\nTable 14.5: t-Test Tails in Excel\n\n\n\n\n\n\nValue to Enter\nDescription\n\n\n\n\n1\nOne-tailed testI.e., that Group 1 values are larger than Group 2 values\n\n\n2\nTwo-tailed testI.e., that Group 1’s values are either larger or smaller than Group 2’s values\n\n\n\n\n\nType:\n\n\n\nTable 14.6: t-Test Types in Excel\n\n\nValue to Enter\nDescription\n\n\n\n\n1\nPaired t-testI.e., that the values in a given row are from the same participant.E.g., Group 1 is a person’s pretest score and Group 2 is that same person’s posttest score\n\n\n2\nUnpaired t-test assuming homoscedasticityI.e., that the variance in the populations from which Group 1 is sample from is the same as the variance in the population from which Group 2 is sampled\n\n\n3\nUnpaired t-test assuming heteroscedasticityI.e., that the variances in the populations from which Groups 1 and 2 are sampled are different\n\n\n\n\nN.b., OLS tests (e.g., t-tests & ANOVAs) are rather robust to heteroscedasticity, so choosing 2 for unpaired t-tests is generally fine. If variances are very different, then standardizing the variables usually suffices to address the issue.\n\nSteps to Conduct a t-Test on the China Posttest Data\nAs an example for computing the p-value for a t-test in Excel, let us use the posttest data from the study of elementary students in China.\n\nOpen the CFL_Posttest_Data.csv file in Excel\nSort all of the data by Population. To do this:\n\nType Control + Home to go to the top-left cell\nShift + Control + End to select all of the data\nType Alt (Option on a Mac), then D, and then S to open the Sort dialogue box (it’s not easy to see how to get the combination, but it’s one I use often enough to have just memorized, If it helps to know, that’s D for the Data menu and then S for Sort.\nSort by Population in A to Z order\n\nNow go to somewhere outside of the data to create the formula. Me, I created a new sheet and went to that instead of doing things on the sheet with the data—don’t want to accidentally overwrite data and not realize it. Believe me.\nIn essence, the t.test formula has you type =t.test(9 and then to follow that with a selection of the first set of data and then follow that with a selection of the second set of data. Plus some other, almost-random stuff at the end of the command.\n\nYou can do all of this by typing =t.test( into a cell and then going back to the sheet with all of the data left-mouse-clicking in the first cell with toca.pro data (cell M2), and then holding the mouse button down while you scroll down until you see in the Population column that it’s changed from Migrant to Non-Migrant to select the first group of data. Then repeating that for selecting the non-Migrant data in below that in the same column.\nOr you can paste this into a cell: =T.TEST(CFL_Posttest_Data!M2:M849,CFL_Posttest_Data!M850:M1130,2,2) What that does is say:\n\nThe first set of data are in the CFL_Posttest_Data sheet in cells M2 through M849; these are the Migrant students’ raw prosociality scores\nThe second set of data are also in the CFL_Posttest_Data sheet, these in cells M850 down through cell M1130; these are the Non-Migrant students’ raw prosociality scores\nThat first 2 after indicating the second set of data tells Excel that it’s a two-tailed test (meaning we’re testing whether the Migrant scores are higher or lower than the non-Migrant scores; a one-tailed test would test only if they were higher but not lower; one-tailed tests are more powerful but only answer that one question—as is common in stats, there’s a trade off between power and precision)\nThat second 2 tells Excel it’s an unpaired t-test (meaning the people in one group are not the same people as in the second group; they could be the same people, e.g., if we were following the same people in a pre-post design). It also tells Excel to assume that the variance of prosociality scores is the same in both groups; again, this is an assumption that’s (a) usually wrong to some degree and (b) O.K. to make as long as they’re not that different.\n\nAfter doing all this mountain of work, you should be gifted with a rather anticlimactic result of a single number appearing in that cell. That modest number should be 0.196848 (depending on how many digits you see). That is the p-value for the t-test; if this number were less than, say, .05, we would say that there is a significant difference between the two groups. However, it is not, so we would not say there is a significant difference.\n\n\n\n\n\n14.7.3 ANOVAs \n\nInstalling the Anlaysis Add-in\nThe functionality to conduct most of the stats that Excel can are not loaded by default. (These are, however, available by default in LO Calc). Fortunately, they are (at least so far) available through an easy installation. To do this:\n\nClick on File\nIn the menu that opens, click on Options then Add-ins\nSelect the Analysis ToolPak option near the top\nAt bottom of dialogue box that opens, under Manage, select Exel Add-ins and click Go\nSelect Analysis ToolPak (and whichever other ones you want)\n\n(VBA is for using MS’s visual basic functionality—a holdover from when Windows & Office were still figuring themselves out)\n\nAn Anlaysis tab will now appear in under the Data tab\n\n\n\nConducting an ANOVA\nE.g., a “Single Factor” ANOVA, which is otherwise known (confusingly) as a one-way ANOVA. This is simply an ANOVA that has only one predictor (or independent variable, IV) and one outcomes variable (or dependent variable, DV).\n\nUnder the Data tab and in the Analysis group, click on the Data Analysis button\nIn the dialogue box that opens, click on Anova: Single factor\nClick on the icon next to the Input Range field to minimize the next dialogue box that opens\nSelect the range of cells (here, two columns, one for the IV & one for the DV), and then click on the icon to maximize that dialogue box\nChoose whether data are grouped by columns of rows—most likley by columns, with one column for each variable\nChoose where the results will be posted; I tend to choose New Workseet Ply: which I give an apposite name to, e.g., “ANOVA Source Table”\nClick OK"
  },
  {
    "objectID": "Intro_to_Excel.html#footnotes",
    "href": "Intro_to_Excel.html#footnotes",
    "title": "14  Introduction to Excel",
    "section": "",
    "text": "I miss Quattro Pro nearly as much as Word Perfect. Then again, I also miss making mixed tapes on my boom box and decorating the cassette cases, so take it for what it is.↩︎\nThis is usually the Command key on a Mac.↩︎\nAlt usually translates to Option on Macs↩︎\nThe .s at the end of it denotes that it is to generate the standard deviation of a sample—not the population. (That’s =stdev.p although you’ll likely never use it: We rarely have all of the data for a population. That’s for qualitative research to say that their participants are that entire population.)↩︎\nRemember that computers can’t do random things. (So. there’s no need for a Voight-Kampff test; all you need is to detect non-randomness.) So, the values generated by Excel cannot be considered random for, e.g., randomizing within a study. True randomization can be achieved best through old-fashioned ways: throwing dice, pulling pieces of paper from a bag, flipping coins, etc.↩︎\nThis is “dummy coding” gender into a yes/no variable about whether the person identifies as female. If the person identified as female, we code that as a 1; any other response (except missing data) we code as a 0; missing data are coded as missing data. We could also have an other variable that dummy-codes whether they identify as male (there, 1 for male and 0 for anything else.). And yes, this allows us to have times when someone identifies as both female and male by letting that person have a 1 for both variables. One of the advantages of dummy coding then is that it allows for multiple responses/categories.↩︎\nUnless we instead wanted Excel to treat the output not as a number but as a word, i.e., treating some number as a word and not as a number.↩︎\nNotice that the IF statements are read by Excel from left to right, so, if a percent is not ≥ 97.5 then Excel goes to the next IF statement to see if it’s ≥ 92.5, and if not then goes to the next IF, etc.↩︎\nTyping the left parenthesis let you then fill in the values with mouse selections and/or further typing↩︎"
  },
  {
    "objectID": "Intro_to_SPSS.html#overview",
    "href": "Intro_to_SPSS.html#overview",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "15.1 Overview",
    "text": "15.1 Overview\nThis document is intended to introduce you to a couple of things. First, we will review SPSS’s GUI1 and how data are prepared and handled therein. We will import a set of rather clean data that we will use to demonstrate ways to further prepare and manipulate data.\nSecond, we will then introduce some common data exploration functions to understand these data better. We will use this opportunity to further consider some of the concepts we’re covering through our other class activities, including normality and outliers."
  },
  {
    "objectID": "Intro_to_SPSS.html#orientation-to-spss",
    "href": "Intro_to_SPSS.html#orientation-to-spss",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "15.2 Orientation to SPSS",
    "text": "15.2 Orientation to SPSS\n\n15.2.1 Accessing SPSS\n\n\n\n\n\n\n\n\nSPSS can be accessed online with your CUNY ID through Apporto—as long as “your browser” is Chrome.\nTo access SPSS through Apporto:\n\nGo to CUNY’s Apporto login page: https://cuny.apporto.com/\nEnter your CUNY login credentials (your @login.cuny.edu “email” address)\nIf you don’t already see an icon for SPSS, in the Apporto home page, click on the App Store button in the top, left corner, just below the hamburger icon that opens up that left-hand menu.\nClick to Launch SPSS and follow any steps to “optimize2” and reconnect.\n\nTo open data in Apporto, there are two ways:\n1, Uploading files via dialogue\n\nLocate the menu bar immediately above the Apporto window:\n\nClick on the File upload button ()\nFollow the dialogue therein\n\n2, Dragging files into the Apporto window\n\nOpen up a file manager outside of the Apporto environment (i.e., in a normal window outside the browser in which Apporto is running)\nLeft click to grab and drag a data file from your file manager into the Apporto window. Apporto will open a notification window letting you know that the file has indeed been imported; it should also now appear in the Apporto window\nYou can then drag the file from Apporto window into the SPSS window that is itself inside Apporto3\n\nFiles you save in Apporto will (at least eventually) appear in either the This PC &gt; Desktop folder (accessible from the Desktop folder under Quick Access in Windows’ native file manager) or in the This PC &gt; Documents folder (Documents under Quick Access).\nClicking on the Settings gear to the right of the Apporto menu bar gives the option to access USBs, although this proved to not always be reliable for all OSs for me.\nTo export files from Apporto:\n\nIn that menu bar immediately above the Apporto window:\n\nClick on the File download button ()\nYou get the idea\n\nAlternatively, you can open your email from within Apporto and send it to yourself as an attachment.\n\n\n15.2.2 Editing Global Options\nBefore we dive into the windows and workings of SPSS, I’d like to note that there are a few useful options to consider modifying given your needs. There are, in fact, many options for tailor SPSS’s functioning, output, and performance given throughout its dialogues and within its rather large list of syntax commands. Here, however, we will simply note a few “global” options that can be set to adjust how SPSS acts in general.\nTo access these, select Edit &gt; Options from the menu (in any window). When that dialogue opens, you will see many choices, including the Variable Lists section in the top right oft the General tab. In that section, you can choose to either have SPSS default to Display names or to Display labels of variables. As discussed further below, a given variable can be identified by either the shorter, more-restricted name or by the longer label used to describe it. By choosing one of these option you can either show smaller, less-intuitive names or longer, more explanatory labels in (nearly) all of the output SPSS generates. Of course, you can also switch between these as needed.\nSome of the other options under the General tab are worth considering (such as whether you want to have SPSS display No scientific notation for small numbers in tables; I mean, we’re doing research here, not science). The Language, Viewer, Data, Currency, Charts, Scipts, and Syntax Editor tabs are less useful for most users, but the items in the Output tab’s Outline Labeling section may also be worth considering. Either of those options can let you choose whether to show only the variable names, labels, or both; I suggest using Labels for output you share with others, but you may want to use Names for your own analyses since it will make for simpler output.\nUnder the Pivot Tables tab, you may want to consider changing the TagbleLook to APA_TimesRoma_12pt when you’re ready to produce pivot tables for your dissertation or publishable manuscripts.\nFile Locations can be nice to change if you store your data and analyses in dedicated folders.\nFinally, you may (or may not) wish to change settings in the Privacy tab.\nThere is more one can do to customize SPSS output and set defaults that allow for automatic APA styling. Including:\n\nUsing an APA-formatted table to serve as the style for subsequent tables\nStyling a given table using Format &gt;TableLooks\nStyling figures\n\n\n\n15.2.3 SPSS Windows\nSPSS is inherently a syntax-driven program, but its popularity is arguably due in large part to its useful GUI. The GUI has three main windows:\n\nThe Data Editor which is comprised of the Data View and Variable View tabs\nThe Output window\nThe Syntax Editor window\n\n\nThe Data Editor Window\nThe Data Editor window is the one most commonly used to interface with SPSS. I think one reason for this is that is can help to be looking at one’s data while working with it—if nothing else to remember what variables there are and what their names are.\nAnother reason, though, is because you will have one Data Editor window for each data set you have open; when you access the drop-down menu at the top to, e.g., Analyze your data, SPSS will assume you want to work with the data in whatever window is either currently raised or that was last raised. So, if you have more than one data set open, simply cycle through to the one you want to work with and then choose what you want to be from the drop-down menu—from either the Data Editor, Output, or even Syntax window.\nRelatedly, you will notice that the drop-down menu at the top is the same4 for all of the windows. This indeed means that you don’t have to cycle back to the Data Edtior window before you do anything. In fact, it can be sometimes easier top use the menu from the Output window so you can look at the results of one command to know what to do with the next. (Anyway, you can see the whole list of variables accessible to a given command in that command’s dialogue boxes.)\n\nThe Data View Tab\nThe Data View tab5 presents a spreadsheet of the data. Just like other spreadsheet programs, you can enter, edit, and scroll through your data here. You can use the Page up and Page Down or the arrow keys to scroll. Holding down the Control/Command button while tapping arrow keys will go to the ends of the data; e.g., Control/Command + \\(\\Downarrow\\) will go to the bottom of the data set; Control/Command + \\(\\Rightarrow\\) will go to far right of it, etc. One way this works differently from, e.g., Excel though is that SPSS will skip over empty cells whereas Excel will stop right before each empty cell instead of going all the way to the end.\nRight-clicking on things in the Data View tab lets you do some useful things.\n\nRight-clicking on a column header (i.e., the part at the top that list the variable name) lets you:\n\nSort the entire data set by that variable\nCopy the variable name or label (more about those things under Variable View)\nClear the data set of that variable. This is the command to delete something in SPSS. Right-clicking and then choosing Clear will delete the selected cell, row, or column in either the Data View or Variable view tab.\nGet Variable Information including the variable’s name, label, type6, any codes for missing values, and the measurement scale for that or any other variable.\nSend a command to give a nice set of descriptive statistics to the Output window (and go there automatically to see those results)\n\nRight-clicking on a row number lets you:\n\nCut or Copy that row\nClear (i.e., delete) that row\nInsert Cases to manually enter a new row of data (or paste one that you said to cut or copy)\n\nRight-clicking on a cell lets you:\n\nCut or Copy the values in that cell\nPaste values selected from cutting or copying\n\nYou can also Paste with Variable Names, useful (or confusing) for pasting into a different column\n\nCopy the variable name or label\nAccess Variable Information or Descriptive Statistics for that entire variable\nClear (i.e., delete) the information in that cell\nCheck the spelling against SPSS’s dictionary\nChange the font slightly\n\n\n\n\nThe Variable View Tab\nThe Variable View presents what is essentially, a codebook, a list of the variables and information about them, including:\n\nName,\n\nthe name that SPSS uses to access that variable. These are best kept short so that you can see the whole thing in some of SPSS’s unnecessarily-small dialogues. They also can only contain letters, numbers, periods, and underscores.\n\nType\n\nindicates whether the variable is a String (alphanumeric), Numeric (numbers not specially formatted), or a number with various types of special formatting, such as dates, currency, etc. The Comma and Dot types are for numbers with thousands etc. indicated by commas or dots, respectively7. Scientific notation is for numbers formatted like 1 \\(\\times\\) 103 to denote 1,000. Clicking on the button with an ellipsis opens a dialogue where you can change the number type (as well as change the length of the variable—how many characters long it can be).\n\nWidth,\n\nwhich simply indicates how many characters long or how many digits a variable has left of a decimal. No big deal\n\nDecimals\n\npresents how many decimal places a (numeric) has been assigned.\n\nLabel\n\nis very useful. In this field you can write a rather long description of what a given variable measures. You can use nearly any characters here to explain it well. To create or change a label, simply left-click inside that field and start typing.\n\nValues\n\nis also quite useful; for variables that are encoded with numbers, you can use this field to indicate what each level of the variable actually denotes. For example, if you have a Likert-style response encoded a number from 1 to 5, you can click on the ellipsis button to denote that 1 = Strongly Disagree, etc. When you explore the variable with descriptives, etc. SPSS will use these value labels instead, making output considerably easier to read. We will show an example of doing this below.\n\nMissing\n\nis yet another useful field. Sometimes a certain character or value will be used to denote a missing value. For example, 99 or NA may be used a place-holders to signify that that datum is actually missing. By clicking on the ellipsis button, you can denote this. We do this below.\n\nColumns\n\nsimply notes how many characters wide a column is. You can change the value here or, under the Data View tab, left-click the space between two rows to change this.\n\nAlign\n\njust indicates the left, right, or center alignment of a column.\n\nMeasure\n\nis an unexpectedly important attribute of a variable. SPSS is quite finicky about the “measure” type of a variable: You can only perform actions on a variable that match that variable type. For example, you can only run correlations on continuous variables. The measurement types that SPSS allows are:\nScale denotes a “scalar” variable, which corresponds to either of Steven’s “interval” or “ratio” levels. It is indicated by a little ruler ().\nOrdinal denotes a, well, ordinal variable and is indicated by a little histogram ().\nNominal denotes a nominal variable is is indicated by a cute little Venn diagram ().\n\nRole\n\nis a rather under-utilized field. It can be used to indicate whether a variable is a predictor / independent variable (Input), a outcome / dependent variable (Target), Both, or whether it is used to Partition or Split the data set. We will create a variable that indeed partitions when we subset the data to only include migrant students.\n\n\n\n\n\nThe Output Window\nAnother reason I think SPSS is so widely used is because, with just a few mouse clicks, it delivers copious amounts of output. As I noted in class, personally I’ve found that some researchers use this output to determine their analyses, assuming that if some stat program spits it out, it must be good. Nonetheless, it can be good—and certainly makes it worth annotating the output.\n\nAnnotating Output\nThe Output window is comprised of two sections, an outline and a main window. The information in either can be changed or added to manually. This can be a good idea. First, of course, because SPSS does return a lot of results and sifting through even a few sets of analyses can be tedious.\nSecond, I strongly recommend taking notes on what you are doing in your analyses and what your thoughts on them are. With data and analyses of any real size and complexity, it can be difficult to jump back in to your analyses even a week or so later; steps that seemed obvious and important at the time can quickly become obscure and lost.\nWays of annotating your output:\n\nInsert a heading in the outline by clicking Insert &gt; New Heading. This will create a new heading at the cursor; double-click on this heading to type in a phrase that will remind you of what you are doing in that section of the output.\n\nAlternatively, you can simply double-click on an existing heading to change it. For example, if you conduct more than one t-test output, you can double click on the first to change it to t-test of toca.pro by group and the second to t-test of toca.dis by group. You can left-click and drag the spacer between the windows to make the outline section wider, but you’ll still not want to make the headings too long since they’ll quickly become longer than a useful outline window.\n\nInsert notes into the output itself by clicking Insert &gt; New Text. This will create a text box in the output section into which you can write pretty much whatever you want. Unlike a heading, this can be as long as you want to give yourself and your colleagues as much information about what you are doing and what it means.\nYou can use the Insert menu to insert other things, too, including whole titles for the output, images, etc.\n\nNote that you can also double-click on any element in the main output section to manipulate that element. This way, you can modify the colors, fonts, or even the text within tables, figures, etc.\nOf course, you can then save your output (to a .spv file) as notes on your analyses.\n\n\nExporting Output\nRight-clicking on an element lets you copy it to then paste it into, e.g., your manuscript (as we will do in Chapter 3: Writing Results).\nAlternatively, you can Export an element. When you right-click on an element and choose to do that, you will be able to export it as a .html, .pdf, .ppt, .doc, etc. For importing into, e.g., Word, I suggest exporting as an .html file.\n\n\nSyntax in Output\n\nSPSS is a powerful stats program, but I personally think that its GUI is a big reason for its success. Nonetheless, SPSS’s GUI is in fact just an “overlay” that just lets us access its most common commands more intuitively; SPSS is in fact running the syntax that those mouse clicks created.\nSPSS versions 27 and earlier return the syntax it used to generate results in the Output window by default right above the given results8. As of version 28, it does not. We can set SPSS to automatically return the syntax used in the output by going to Edit &gt; Options &gt; Viewer and then checking the Display commands in the log box in the lower-left of that Viewer window9.\nWhy do this? Because there are several ways in which the syntax that SPSS posts can be quite useful. First, you can copy that syntax into the Syntax Editor (as noted below) to rerun any analyses. This is useful when you are returning to analyses later on and, e.g., want to generate a smaller set of analyses.\nSecond, as you learn what SPSS can do, you can use the syntax to learn better how to do it—and how to tweak your analyses to get exactly the output you want. Reviewing existing syntax is a lot easier than learning it from scratch.\nThird, once you’ve gained some facility using SPSS, you will find that there are things you want to do that you can’t through the GUI. Instead, you will need to do things directly withe the syntax. Although you certainly can type syntax directly into the Syntax Editor, it’s often easier to paste in existing syntax and edit it as needed. In fact, in the long run, that’s also faster.\nFourth, you can annotate syntax a bit like you can annotate output. This way, you can create and save a syntax file (saved as a .sps file) that’s a lot smaller and easier to navigate through than some massive output file—and still be able to generate that mountain of results with a few quick keystrokes10.\n\n\n\nThe Sytnax Window\nSPSS doesn’t open a Syntax window automatically, like it does a Date Editor or Output window, but simply clicking File &gt; New &gt; Syntax opens one. We will demonstrate using it below, but the general way to use it is to either paste in or type some syntax command and, with the cursor in some part of that syntax, either click on the big, green play button11 or type Control/Command + R.\nSPSS syntax itself follows a set grammar. Some command is given first; often this is immediately followed by a “statement” that just tells SPSS what variables, etc. to run that command on. This is followed by one or more options, for example whether to print out both figures and tables based on the command. Critically, each command must end with a period.\nAs you might expect, SPSS has many commands to choose from; more are available if you pay them more (and have your own copy of SPSS; this won’t work with the version we have access to through CUNY)."
  },
  {
    "objectID": "Intro_to_SPSS.html#data-preparation-cleaning",
    "href": "Intro_to_SPSS.html#data-preparation-cleaning",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "15.3 Data Preparation & Cleaning",
    "text": "15.3 Data Preparation & Cleaning\nIn this section, we will now build on this to further clean and prepare the data for analyses. Most of these tasks are here to demonstrate how to clean or improve data in ways that are commonly needed.\n\n15.3.1 Change id to nominal\nThe id variable is right now a Scale variable. That’s natural since it is a number after all. And it’s not uncommon for SPSS to import IDs as numbers since replacing names with numbers is a very typical way to anonymize participants. Frankly, leaving it as a number (a Scale level Measure) won’t likely create any problems in SPSS12, but it still presents a good opportunity to demonstrate changing the Measure of a variable. To do this:\n\nGo to the Variable View tab of the Data Editor window.\nLeft-click on the Measure cell in the id variable’s row. When you do, a drop-down menu will appear listing the three measure levels.\nSelect to make id a Nominal variable.\n\nNow, SPSS will “understand” that this is in fact a name that signifies each participants and should be treated as such in all analyses.\n\n\n15.3.2 Setting Values labels for pollution\nThis next task is also, frankly, not necessary here. But it is a very useful one to know since—like giving good variable labels—it can really help make things clearer in output. We will give labels to each of the response values for the CHEAKS Pollution subscale. For a bit of background, all of the CHEAKS subscale scores—including that for pollution—are created from responses to two items. If a respondent indicates in a given item that they have done something to help, e.g., reduce pollution, then that item response is coded as a 1. If they do not report having done that, e.g., pollution-related tasks, then their response to that item is coded as a 0. Since there are two items for each subscale, the scores on a subscale can range from 0 (i.e., didn’t do either task), through 1 (they did one of the two tasks), to 2 (they did both tasks). Let’s code the pollution score to reflect this:\n\nAlso in the Variable View of the Data Editor, click on the Values cell in the the pollution row.\nClick on the ellipsis button that appears.\nIn the dialogue box that opens, enter a 0 in the Values field.\nType Engaged in neither task in the Label field.\nClick the Add button. 0 = \"Engaged in neither task\" will now appear in the field next to the Add button.\nNow, type a 1 in the Values field.\nType Engaged in one task in the Label field.\nAgain click the Add button to add this association as well.\nDo this once more to add 2 = \"Engaged in both tasks\" to the list.\nClick OK\n\nNow when you click on the values cell for the pollution row, you will see these value labels added. Right-clicking on the pollution row and choosing to look at the Variable Information will show these in addition to the other information there.\nNote that we have not actually changed the data. They are still numbers (Scale level measures). Right-click again on that variable (in either the Data View or Variable View tabs) and select Descriptive Statistics. You will see in the output that SPSS generates means, etc. just as it would for any interval/ratio variable.\nHowever, now in the drop-down menu click on Analyze &gt; Descriptive Statistics &gt; Frequencies and you will see that the level values are replaced with the more explanatory value labels, helping us (and out colleagues and readers) more easily see what the responses really meant.\n\n\n15.3.3 Setting missing values for wave\nAs mentioned briefly above, we can set certain values to be recognized as representing missing values. Most of the variables in this set were imported with blank cells denoting missing values, but wave has a few values that are listed as NA. Right now, SPSS doesn’t recognize these as missing; to show this, simply right-click on that variable in the Data Editor and select Descriptive Statistics13, which shows that there are 1129 Valid cases and 0 Missing:\n\nwhile is also treating NA as a factor level:\n\nWe can easily fix this, though:\n\nIn the Variable View tab of the Data Editor window, click on the the ellipsis button in the Missing cell of the wave row\nClick on the radio button next to Discrete missing values\nIn the first field under that, type in NA\nClick OK\n\nHad we numerical ranges we wanted to indicate denoted missing values, we would have instead entered them into the Range plus optional discrete missing value fields.\nNow when we look at the descriptives for wave, we see that SPSS indeed sees NA as a missing value:\n\n\n\n\n15.3.4 Transform city into a numeric variable called city.n\nThere are pretty often times when it’s useful to create a new variable based on the values of one or more existing variables. One occasion to do this is to make a nominal/ordinal variable into a numeric one14. Let’s create a new variable based on the city values15\nTo do this:\n\nIn any window, click on Transform &gt; Recode into Different Variables16\nIn the dialogue box that opens, click on city in the list of variables in the left-hand field.\nClick on the arrow button () next to that list of variables. This adds city to the Input Variable -&gt; Output Variable field.\nIn the Output Variable section, enter city.n into the Name field and Numeric version of City (Or something like that) into the Label field.\nClick on the Old and New Values button\nA new dialogue that opens; in the Old Value section, type in Kunshan in the Value field\nIn the New Value field, type in 0\nClick the Add button to add that to the Old -&gt; New field\nRepeat this for transforming Shanghai to 1. Note that since this is the only remaining option, we could instead select All othe values at the bottom of the Old Value section while still adding 1 to the New Value field.\nClick Continue ro return to the previous dialogue and then click OK\n\n\n\n15.3.5 Create a dummy variable for Population called population.n\nI am a pretty strong advocate for using dummy variables. They can make it easier to interpret the effects of each level of a nominal variable without needing to resort to, e.g., post hoc analyses. \n\nAlso under the Transform menu, select Create Dummy Variables\nSelect Population under Variables and then add that to the Create Cummy Variables for: field by again clicking on the arrow ()\nWe are going to create a simple dummy variable—not, e.g., one derived from a combination of other variables—so leave Create main-effect dummies selected\nWe didn’t create any value labels, but it’s fine to leave selected Use value labels under Dummy Variable Labels since neither choice matters for a simple “main effect” dummies.\nUnder Macros, select to Omit first dummy category from maro definitions. We can nearly always select to do this because we usually need one fewer dummy variables than there are values in the original variable. The Population variables has two values (Migrant and Non-Migrant), so we only need one dummy variable (i.e., 2 - 1 = 1) to fully encode the information in the Population variable17\nIn the Root Names field, type population\nClick OK\n\nDummy variables can only take on the values of 0 or 1. For some reason, SPSS gives dummies it creates two decimal places. We clearly don’t need these, so:\n\nIn the Variable View tab, click into the Decimals cell of the population_1 variable18\nChange the value to 0\n\nNote that we could also change to Width to 1 since we only need one digit to the left of the decimal.\n\nFrequencies & Crosstabs\nSo far, we’ve been mainly cleaning and prepping the data. I want to break this pattern to preface the step after this one in which we subset our data.\nA common way to examine nominal (and ordinal) variables is through frequencies and cross tabulations (“crosstabs”) of those frequencies across pairs of variables. Let us do this with some of our Population and Group19:\n\nClick on Anlayze &gt; Descriptive Statistics &gt; Frequencies\nIn the dialogue that opens, select Population and Group. You can do this by clicking on them separately or, e.g., holding down the Control/Command key to select them both\nClick on the arrow button to move them to the Variable(s) field\nSPSS provides many options for displaying variables. Some of these are listed under the Statistics button, but since these are both nominal variables, we can only meaning select Median under Central Tencency (and perhaps Quantiles)\nUnder Charts we can select to add, e.g,. Bar charts for Frequencies. Note that pie charts are often misleading and rarely the best option for displaying data.\nFor now, there’s not much of interest under Format, Style, or Bootstrap\nBut do notice on the main dialogue box that we can select whether we want to Create APA style tables, a nice option for preparing pieces for dissertations and manuscripts\nClick OK\n\nWe can see from both the tables and bar charts that we have many more migrant than non-migrant students and many more students who participated in the Caring for Life program (CFL) than those who didn’t (No-CFL). Given that these were chosen variables—part of the study’s manipulation—this is curious. Let’s investigate further\n\nClick on Anlayze &gt; Descriptive Statistics &gt; Crosstabs\nFrom the variable list, select Population and use the arrow to move it to the Row(s) field\nAnd move Group to the Columns(s) field\nSPSS again offers many choices for options. For example, under Statistics we can (but won’t) choose Chi-square to test whether the distribution of counts differs from expected values and, under Cells, whether to present not only the Observed cells counts but also the Expected. Now, though, simply click OK\n\nThe table this produces:\n\nreveals why the counts for both Population and Group were so uneven: There are no non-migrant students who participated in the CFL program20\n\n\n\n15.3.6 Subset the data to only include migrant students\nGiven that there are no non-migrant students in the no-CFL group, it produces bias to include them as part of the CFL group. This is thus one of the few times when it is justified to subset one’s data: Although subsetting literally removes information from your analyses21, the information that they would include would make CFL – no-CFL comparisons like comparing apples to oranges.\nLet us thus subset our data to include only migrant students.\nUnder the Data menu option, there are a few options for subsetting data:\n\nSplit File\n\nThis keeps the data together, but produces separate output for the groups they are split into.\nFor example, we would split these data by population and then easily compare descriptives between the migrant and non-migrant.\nFor those who believe in “segregating” their groups, this is a good option22\n\nSplit into Files\n\nIn which we can create two or more sets of data separated by values on one or more variables.\nFor example, we could separate these data into files that have first- and second-grade students that are either migrant or non-migrant students. Many do this.\n\nSelect Cases\n\nThis selects cases based on some criterion. The unselected cases can then be “hidden” from analyses, deleted, or moved to an other file.\n\n\nTo do this:\n\nClick on Data &gt; Select Cases\nIn the dialogue box that opens, select Population\nMake sure If condition is satisfied is selected23\nClick on the If... button24\nThe the new dialogue, again select Population and move it to the field at the top right.\nEither type or click on the = button\nType into that field \"Migrant\". Do include the quotes so that that field now shows: Population = \"Migrant\"\nClick Continue\nUnder Output, make sure Filter out unselected cases is checked. This retains the unselected data, but removes it from any analyses. Copy seleted cases to a new dataset is tantamount to Split into Files\nClick OK\n\nIn the Data View of the Data Edtiro window, you can no see that the rows for non-migrant students have a line crossed through their row number:\n Under the Variable View tab, you’ll also see that a new variable called filter_$ has been created. The label for this variable, at least, is informative: Population = \"Migrant\" (FILTER)\nAnd now all subsequent analyses will disclude the non-migrants students25. We can stop filtering out the non-migrant students simply by deleting (Clearing) that filter variable. Being able to now subset our data is why I wanted to do this task a bit out of step with the rest of this guide.\n\n\n15.3.7 Compute CHEAKS Total from Other Variables\nThis data set doesn’t contain the responses to individual items (or other things), but it’s obviously much more common for data to have things like that—and for us to need to compute scores based on these individual items. Intentionally, though, we can still do that here since the CHEAKS total score is itself comprised of the scores on the various CHEAKS subscales.\n\nClick on Transform &gt; Compute Variable\nIn the dialogue that opens, type in cheaks.total in the Target Variable field\nJust to do it, click on the Type &* Label button\nTo make the label CHEAKS Total Score (although selecting Use expression as label isn’t a bad idea to make a clear history of what you’ve done)\nAnd leave the Type as Numeric\nIn the list of variables, scroll down to pollution and click on that\nClick on pollution from the list of variables, and then the blue arrow to move it to the Numeric Expression field\nIn that numeric expression field, type + and then move general over before following that with another +.\nContinue to do this until the Numeric Expression field shows: pollution + general + water + energy + animals + recycling26\nClick on OK\n\nNote that if you are regularly importing data into SPSS (e.g., if you are “peeking” at data as they come in27), then this would be a great occasion to save the syntax created to rerun upon each data importation.\n\n\n15.3.8 Standardize Variables\nI’m also a big fan of standardized data. It doesn’t change the distribution of scores at all but makes values on one variable directly comparable to values on an other—even if they’re measured on very different scales28\nSPSS makes it very easy to standardize variables:\n\nClick on Anlayze &gt; Descriptive Statistics &gt; Descriptives\nClick on toca.pro\nNow, holding down the Shift key either single-(left-)click on recyclilng or tap the down-arrow key until you have selected all of the variables from toca.pro to recycing\nNow click on the blue arrow to move all of those variables to the varialbe(s) field\nUnder the Options dialogue, we might as well check to review, e.g., the Mean, Std. Deviation, Minimum, Maximum, and S. E. Mean (i.e., the standard error of the mean that we covered in our first lecture)\nBut our real goal here is to check the Save standardized values as variables before clicking on OK\n\nAnd that’s all it takes to create standardized variables."
  },
  {
    "objectID": "Intro_to_SPSS.html#export-a-table-in-apa-format-to-word",
    "href": "Intro_to_SPSS.html#export-a-table-in-apa-format-to-word",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "15.4 Export a Table in APA Format to Word",
    "text": "15.4 Export a Table in APA Format to Word\nTo export a table in APA format in SPSS, you can use the following steps. This process involves generating the table, modifying it to meet APA style guidelines, and then exporting it.\n\n15.4.1 Steps to Export a Table in APA Format in SPSS:\n\nGenerate the Table:\n\nFirst, create the table you need by running the appropriate analysis.\nFor example, to create a descriptive statistics table, go to Analyze &gt; Descriptive Statistics &gt; Descriptives..., select the variables, and run the analysis.\n\nModify the Table:\n\nOnce the table is generated, it will appear in the Output Viewer.\nTo modify the table, double-click on it to open it in the Pivot Table Editor.\nAdjust the table’s appearance to match APA style as closely as possible. This might include:\n\nEnsuring that the table uses a simple grid with minimal lines.\nAligning text correctly (typically left-aligned for text, right-aligned for numbers).\nUsing appropriate font and size (Times New Roman, 12-point is common for APA).\nIncluding relevant statistics (e.g., means, standard deviations).\n\n\nExport the Table:\n\nAfter making the necessary adjustments, you can export the table.\nClick on File &gt; Export... in the Output Viewer.\nIn the Export Output dialog box, choose the desired file format. For APA tables, Microsoft Word (.doc or .docx) is typically a good choice.\nSpecify the file name and location where you want to save the file.\nUnder Objects to Export, select All Visible Objects or choose the specific table you modified.\nClick OK to export the table.\n\n\n\n\n15.4.2 Example Export in Word:\n\nOpen the Exported File:\n\nOpen the exported Word document.\nReview the table to ensure it adheres to APA formatting guidelines.\n\nAdjust in Word if Necessary:\n\nIf further adjustments are needed, you can make them directly in Word.\nEnsure the table is labeled correctly with a table number and title (e.g., Table 1).\nInclude any notes below the table, formatted according to APA guidelines.\n\n\n\n\n15.4.3 Summary\nTo export a table in APA format from SPSS, generate the table through the appropriate analysis, modify it using the Pivot Table Editor to adhere to APA style, and then export it to a Word document. Make any final adjustments in Word to ensure the table fully complies with APA formatting standards."
  },
  {
    "objectID": "Intro_to_SPSS.html#additiolnal-resources",
    "href": "Intro_to_SPSS.html#additiolnal-resources",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "15.5 Additiolnal Resources",
    "text": "15.5 Additiolnal Resources\n\nSupplemental materials from Polit & Beck (2017)\n\nSPSS Analysis of Descriptive Statistics\nSPSS Analysis of Inferential Statistics\nSPSS Analysis and Multivariate Statistics\n\n\n\n\n\n\n\n\n\n\n\nPolit, D. F., & Beck, C. E., Cheryl T. (2017). Nursing Research: Generating and Assessing Evidence for Nursing Practice (Tenth). Wolters Kluwer."
  },
  {
    "objectID": "Intro_to_SPSS.html#footnotes",
    "href": "Intro_to_SPSS.html#footnotes",
    "title": "15  Introduction to SPSS & Data Preparation",
    "section": "",
    "text": "“Graphical user interface”↩︎\nBecause following all of the steps they already laid out for you could not be optimal.↩︎\nYou can load it from the Apporto file system via, e.g., This PC &gt; Desktop, but files don’t immediately appear there (needing connection refreshes?), so simply dragging it into the SPSS Data Editor window seems most reliable to me.↩︎\nWell, actually the Syntax window has a few extra menu items related to running syntax and accessing additional extensions.↩︎\nThe tabs are at the bottom left of the window.↩︎\nThis “type” is given as either the letter (A or F) or word (DATE, TIME, PCT (for percent), DOLLAR, etc.) followed by a number. An A means that it is a string variable (i.e., Alphanumeric), and an F means it’s a number (an “F” is used for esoteric reasons). The number presents the number of digits possible before and after the decimal point; if the value has no decimal (e.g., F4), then that variable has no decimals.↩︎\nI.e., Comma is for numbers formatted like 1,000.00 and Dot is for numbers formatted like 1.000,00↩︎\nThe syntax is posted under Log headings in the outline. This is useful for finding it, but the log is also used by SPSS to report errors and warnings, so it can be a little confusing to find to the syntax or even know that errors/warnings were generated.↩︎\nWe can also turn on outputting syntax with syntax: SET PRINTBACK LISTING. turns it on, and SET PRINTBACK NONE. turns it off.↩︎\nControl/Command + A to select all of the syntax in the window, and then Control/Command + R to run it all.↩︎\nI.e., this button: ↩︎\nAs we’ll discuss briefly in the measurement class, interval and ratio variables—those that SPSS calls Scale variables—can be analyses in more ways than ordinal variables; ordinal, in turn, can be analyzed in more ways than nominal.↩︎\nOr go to Analyze &gt; Descriptive Statistics &gt; Descriptives in the drop-down menu.↩︎\nOf course, we can simply add value labels to existing (numeric) values in a Scale variable, but here we’re not just adding labels but instead creating a whole new variable with different values.↩︎\nI’m doing this as a way of anonymizing city, but since there are only two cities in these data, I’m really creating a dummy variable. There is a better way to create dummy variables that we’ll cover below, but I just wanted to point this out now.↩︎\nAs you can see, we could also recode into the same variable and indeed anonymize city in one action. Personally, I tend to create new things instead of overwriting existing in case I make a mistake and can’t easily recover what I overwrote.↩︎\nNote that SPSS may create two variables anyway. I’m not sure why it does this, but we can simply delete (Clear) the one with the Population=Non-Migrant label since we’ll only work with the migrant students.↩︎\nOr whichever is the dummy with the Population=Migrant label that we’ll be keeping.↩︎\nWe could also do this with these variable’s numeric equivalents—especially if we had given value labels to their numbers.↩︎\nThis was a consequence of working in the field—i.e., with data collected outside controlled, laboratory-like settings. Schools who participated in the control (i.e., no-CFL) group were essentially asked to give their time and resources to the study without getting anything in return since (that year), they were not getting the CFL program offered at their school. It can be hard to find busy schools with many demands that can afford to do this. To compensate for this, I was forced to use a non-migrant control group from a previous study. Hardly ideal, but necessary.↩︎\nWhich turns out to be a bad idea.↩︎\nAs you may guess, I prefer not doing this. Better is to create a factor in one’s analyses that measures the effect of the variable (e.g., Migrant) on outcomes than in essentially doubling the number of analyses conducted and thus nearly doubling the chances of errors.↩︎\nRandom sampe of cases woudl be used, e.g., to split the data into a “training” and “test” set of data to create and test, e.g., the generalizability of a model. Splitting Based on a time or case range would be done if we feel that, e.g., participants differ qualitatively over time or within ranges of cases. Use filter variable is essentially what we’re doing, but we’re first needing to create that filer variable.↩︎\nAlways a good thing to do in life.↩︎\nDon’t worry, they’re relatively privileged otherwise in life compared to the migrant students.↩︎\nYeah, or just copying that line into the Numeric Expression field to save some typing. Note too that we could also have chosen either All or Statistical from the Function group field and then Sum from the Functions and Special Variables field and made the sum of those variables.↩︎\nWhich, of course, you should never do, and that no one ever does.↩︎\nIn analytic models—if all variables are either standardized or dummy-coded—it also lets us remove the intercept term, making our analyses a bit more powerful … but more on that in Stat 2.↩︎"
  },
  {
    "objectID": "Intro_to_GPower.html#the-relationship-between-α-power-effect-size-and-sample-size",
    "href": "Intro_to_GPower.html#the-relationship-between-α-power-effect-size-and-sample-size",
    "title": "16  Introduction to Power and Sample Size Estimation Using Either G*Power or R",
    "section": "16.1 The Relationship Between α, Power, Effect Size, and Sample Size",
    "text": "16.1 The Relationship Between α, Power, Effect Size, and Sample Size\nPower is one of four, inter-related values used (implicitly or explicitly) in hypothesis testing:\n\nα, the probability of a false positive—seeing an effect that isn’t there. (Also called a Type 1 error.)\nβ, the probability of a false negative—missing a real effect. (Also called a Type 2 error.)\n\nNote that power is 1 – β\n\nEffect size, the magnitude of a measured effect. More about this is at Chapter 6\nN, the sample size.\n\nChanges in any of these four values affects the chances of obtaining a significant effect:\n\nSample size doesn’t appear directly in this figure, but it affects the width of the distributions—when those distributions represent uncertainty around an estimate (such as estimates of a population mean1). We are using the sample values to estimate these population values, so these distributions represent the probabilities of what the population values are given the sample values we found. The vertical red and blue lines denote the sample values we got for each group, and the curves represent the probabilities of where the actual population values are expected to be. The larger our sample, the more confident we are that our sample’s values are darned close to the population values—and so the distributions become thinner.\nAs the samples grow—and the distribution of estimates of the population means become thinner—the chances of false positives (α) and false negatives (β) become smaller:\n\nGenerally, if we know any three of those values—α, β, effect size, or N—we can compute the fourth2. This most often means that we can estimate the sample size (N) that we would need to detect a given effect size, while assuming particular values for α and β."
  },
  {
    "objectID": "Intro_to_GPower.html#using-gpower-or-r-to-estimate-a-priori-sample-size-estimates",
    "href": "Intro_to_GPower.html#using-gpower-or-r-to-estimate-a-priori-sample-size-estimates",
    "title": "16  Introduction to Power and Sample Size Estimation Using Either G*Power or R",
    "section": "16.2 Using G*Power or R to Estimate a Priori Sample Size Estimates",
    "text": "16.2 Using G*Power or R to Estimate a Priori Sample Size Estimates\nG*Power (and R) are perhaps the best, current, one-stop applications3 to estimate sample sizes needed to expect significance of many, common analyses.\n\nG*PowerR\n\n\nG*Power is a free (as in “free beer””) software follows the “Unix” philosophy to do one thing and do it well. What it does well is estimate how large a sample one will need to be for various analyses. As the name implies, it’s in fact designed to conduct analyses related to statistical power, but it most often used to computes sample sizes well (power is only occasionally worth computing anyway).\n\n16.2.1 Installing G*Power\nFinally, software that’s easy to install:\n\nClick the Download button on the right of the G*Power site\nDownload the latest version for either Mac or Windows4\nThe downloaded file is zipped, so extract it and install.\n\n\n\n16.2.2 Citing G*Power\nThe creators of G*Power request that one uses one or both of the following citations when using it:\nFaul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39, 175–191.\nFaul, F., Erdfelder, E., Buchner, A., & Lang, A.-G. (2009). Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses. Behavior Research Methods, 41, 1149–1160.\n\n\n16.2.3 Orientation to G*Power\nG*Power has a rather simple interface. Below the menu bar at the top (and a window that will fill with power cures for your estimation), it presents a drop-down menu for Test family and Statistical test; most of the options below these drop-down menus will change depending on the options chosen there.\n\nUsually, the first step to use G*Power is to select Test family and then the Statistical test. After that, one fills in the values in the fields below in the Input Parameters section.\n\nType of power analysis\nThe drop-down menu under immediately under Test family and Statistical test allows one to compute either sample size or power, α, or effect size. The only option we will use is A priori: Compute required smaple size - given α, power, and effect size. However, it’s worth briefly discussing the other options.\nThe Type of power analysis offers more options than the A priori: Compute required sample size - given α, power, and effect size. Most of the options pertain to computing the other variables in the α, power, effect size, sample size group. There is one worth explaining, however: Compromise : Compute implied α & power - given β/α ratio, sample size, and effect size\n\nA priori: Compute required sample size and a Note on Post hoc Power Analyses\nComputing effect size can be informative. Computing required α may be interesting if rarely practical. Computing post hoc “achieved power,” however, is rarely either [hoenig2001]. Post hoc power analyses depend very heavily on the particulars of a given set of data; i.e., they do not generalize—neither to other samples nor even to the actual power of the test (Yuan & Maxwell, 2005; Zhang et al., 2019).\nPost hoc power analyses are also based on a tenuous interpretation of power: Achieved power is computed assuming that there is an effect, but hypothesis tests actually assume that there is no effect. Remember that hypothesis tests actually test the probability of finding the results we did if the null hypothesis is true. If we conclude that there is a significant effect, we are saying that there is insufficient evidence to conclude that the null is true. We are thus also stating that there is insufficient evidence that a post hoc power analysis is justified.\n\n\nCompromise: Compute implied α & power\nThe Compromise: Compute implied α & power - given β/α ratio, sample size, and effect size option is an interesting one—even if it’s rarely useful. We begin with a sample size—typically the largest sample we know we would be able to attain within practical constraints—and an effect size that we either know or expect to have. Within those real constraints we can explore what levels of α and β we could achieve. We could see—for example—how badly power would be affected if we try to maintain a significance level of .05, or what level of significance we would have if we tried to keep the chances of both types of error the same.\nIt is an idea that harkens back to the original intents of those (like Fisher) who originally thought up the idea of a significance test. The significance level wasn’t always frozen at .05; the original idea was to use whatever level one felt was appropriate—be it .05, .01, .10, .25 or any level that conveyed how important one felt false positives were in a particular situation.\nBut messing with α has become verboten. A wall doing its best to keep out p-hackers and other all-too-human threats to the integrity of science. There is now little practical point to play with the proportion of false positives. It may satisfy an idle curiosity to see how strongly it is affected versus false negatives, but it isn’t going to get you any closer to published or funded. Let’s instead explore what does.\n\n\n\nA Note About the Default Level of β in G*Power\nIt is common in the health and social sciences to also assume power is .80, i.e., that we have an 80% of detecting a real effect. Otherwise said, we assume we have a 20% to miss detecting a real effect. The convention is to prefer to miss seeing something important over mistakenly thinking we found something; we accept more false negatives (β = .20) than false positives (α = .05). This convention is not rigid, however, and we certainly can and should change those values—such as making α and β the same—if justified.\nHowever, the default value for β in G*Power is .10. G*Power is used in many areas of science, and that .80 convention for power isn’t followed everywhere. For example, pharmaceutical researchers often use that higher value for power (1 – β = .9) so they have a better chance to detect real—even rare—side effects in clinical trials.\n\n\n\n16.2.4 Estimating Required Sample Sizes\nFor all of these exercises, we will be estimating how large a sample we would expect to need in various analyses and assuming common standards for α and β. Before we do, however, I want to point out that the key words in that sentence are “estimating” and “expect”: Sample size estimation is not an exact science and the actual study we conduct will surely have actual rates of significance different from what we expect. After all, if we already knew what we would find, we probably wouldn’t be conducting the experiment.\nIt is also worth pointing out that we pretty often actually need a larger sample than we estimate. There is a tendency to be overly optimistic about our abilities to realize certain effects or overcome real-world challenges for recruiting a conscientious group of well-delimited participants. Nonetheless, our a priori estimates can at least put us in the general vicinity of where we need to be.\n\nCorrelations\nLet us begin with estimating samples for correlations. Although it’s not evident from most stat programs, we use different sorts of tests for different sorts of correlations. Therefore, tests of correlations are under different Test family options in G*Power.\n\nPearson’s Product-Moment Correlation (r)\nWe begin with the most commonly-used type of correlation, that which is most formally called Pearson’s product-moment correlation coefficient. This is the one symbolized with a simple r that measures the degree of association between two continuous variables. This sample size estimate is for tests of whether the correlations differ between two groups.\nFor example, if Hepatitis B/C is more strongly correlated with incidents of liver cancer in males than in non-males. Or whether the correlation between income and longevity is different among Blacks versus that correlation among whites. Let’s use that latter example and assume that we expect that the correlation will be .5 among Blacks but only .3 among whites (or vice versa). We’re thus interested in seeing how large a sample we would need to reliably detect a significant difference between these correlation coefficients.\n\nIt is found under the z tests option in the Test family menu.\nAfter selecting that option, under Statistical test, choose Correlations: Two independent Pearons's r's5:\n\nAs we will for all of these exercises, under Type of power analysis, choose A priori: Compute required sample size - given α, power, and effect size\nIn the Input Parameters section, let’s first choose One under Tails. (We will next see how things change when we choose two.)\nHandling effect size\n\nBy computing the difference between two anticipated correlations.\n\nThe effect size statistic for a difference between correlations is Cohen’s q (Cohen, 1988, p. 109), as noted in the next row, Effect size q. Most effect size statistics are easy to compute, but unfortunately q is not6. Fortunately, G*Power can easily compute it for us:\nClick on the Determine =&gt; button to the left of Effect size q. A new window will open to the right of the main window:\n\nBy sheer coincidence, the default values given by G*Power for the first (Correlation coefficient ρ1) and second (Correlation coefficient ρ2) correlations are the ones we want7, so leave them as they are.\nStill inside the side window, click on the Calculate and transfer to main window button. The computed value for q will be populated into both the side window’s and the main window’s Effect size q fields.\nClick Close in that side window.\nNote that most power analyses in G*Power will have a Determine =&gt; button, but they will do different things for different tests. Often, it is used to convert one effect size measure to an other one that is more appropriate for the given test (unlike we did here).\n\nBy using Cohen’s (1988) recommendations.\n\nBack in the Input Parameters section of the main window, mouse over the field where we will enter our values:\n\nThe bubble that appears lists the values that Cohen (1988) recommends for—in this case—Pearson’s rs. A bit more about his recommendations for these is given on page 129 of his book. Looking at those suggested levels, we see that the effect size for a difference between r1 = .3 and r2 = .5 is a bit less than a “medium”-sized effect.\nAlthough Cohen’s recommendations are viewed by most as more canonical than Cohen intended, it is still useful practice to use them as guides for anticipating effects, especially if we don’t know ahead of time what size of effects we can expect. Or here, if we didn’t have any reason to expect certain correlations for each group beforehand.\nIn such cases, researchers often assume a priori that they will achieve a “medium” effect. (Although I tend to recommend assuming something half way between a “small” and “medium.”) G*Power also uses a “medium” effect as well for the default value, so feel free to use that here instead of the \\(\\approx\\)|0.23| that we computed from a priori expected correlations.\nIt is quite worth noting, however, that it is greatly preferable to instead use prior research—even if only tangentially related—to estimate what levels of effects one should expect.\n\n\n\nFor Effect size q, please enter either ~0.23 or .3. I will continue assuming that you entered in the latter option.\n\nThe α-error prob option can be left at the 0.05 default both now and probably always.\nThe default value in G*Power for Power (1 - β err prob) is 0.95. Since 1 – .05 = .95, that is implying that the probability of a false negative (β) should be equal to the probability of a false positive (α). I commend the creators of G*Power8 for advocating for this parity. However, convention is to deprecate power relative to α and set 1 – β to .8 (thus setting β = .2, four times larger than α).\n\ntl;dr: Enter .8 into the Power (1 - β err prob) field\n\nThe Allocation ratio N2/N1 is asking if the size of the two groups (i.e., the groups whose different correlations we’re testing, e.g., if Blacks versus whites have a different correlation between income and longevity) is the same.\n\nIf we assume that there will be equal numbers in both groups, then enter 1.\nIf we assume, e.g., that we will have twice as many whites as Blacks, then enter 2 (or.5 to change which group is consider which race).\nOf course, any other ratio can be used, and different ratios tried to obtain tolerance ranges.\n\nClick on the Calculate button in the lower right.\n\nAssuming you used these values—including 1 for Allocation ratio N2/N1—your output should look like this:\n\n\nUnder the Output Parameters section, we can see that the Critical z value is \\(\\approx\\) 1.64. This is also presented in the figure near the top of the G*Power window. This is simply the value of the test statistic (here z) computed and used to test the significance of the difference between correlation coefficients.\nOf greater interest are the next three rows.\n\nSample size group 1 and Sample size group 2 present the required samples sizes for the two groups. Note that these would be different if we had chosen a value other than 1 for the Allocation ratio N2/N1.\nTotal sample size is just the sum of those two samples sizes. We would thus need a total N of 282 to detect a difference between the two correlations (or 438 if you used -.23 for the effect size).\n\nThe Actual power field simply presents what the power is for these two sample sizes. It differs slightly from what we entered into the Power (1 - β err prob) field due to rounding error from the sample sizes needing to be whole numbers.\n\nIf we change the tab at the top from Central and noncentral distribution to Protocol of power analysis, instead of the two curves at the top, we see both the values we entered to estimate the sample size and the output. This alone is of little use, but of slightly more use is that, under this tab, we can select File &gt; Save Protocol and save these input and output values as a text file. (Under the Central and noncentral distribution tab, we can instead save that image of the two distributions.)\nClick now on the X-Y plot for a range of values button next to the Calculate button. The following new window will appear:\n\nThis figure presents what the power would expected to be were we to use different total sample sizes. You will see that the y-axis (Total sample size) is about at 280 (or 440 if you used q = -.23) when the x-axis is at 0.8. Had we used G*Power’s default of .95 for power, the estimated total sample size would have been a little less than 500.\nThe fields below this figure present the values we entered in the other window as well as options for changing the figure, including changing the range of power values presented and the “steps” between each dot in the figure. The Table tab at the top lets us look at these values as a table instead of a figure for more precision:\nI think this X-Y plot for a range of values window is under appreciated. We can change values in this figure (and the corresponding table) to see not only the estimated sample size needed, but how smallish changes to the values—especially to power—would change with different assumptions.\nPlease return to G*Power’s main window and change the Tails to Two and recalculate the sample size estimation. Under the Protocol of power analysis tab, you can save that output as, e.g., corr_2-tailed.rtf.\nMore about this particular analysis is on page 65 of the G*Power Manual.\n\n\nPoint Biserial Correlation (rpb)\nPoint biserial correlations measure the association between a dichotomous variable and a continuous variable (e.g., the association between pregnancy and blood pressure). In significance tests of point biserial correlations, we’re seeing how closely matched some continuous outcome score is for two group. Two nominal groups and a continuous outcome, that sounds like a t-test.\n\nUnder Test family, choose t tests.\nUnder Statistical test choose Correlation: Point biserial model\nAs always, under Type of power analysis, choose A priori: Compute required sample size - given α, power, and effect size\nSince we usually use two-tailed significance tests, please change Tails to Two. (We will next see how things change when we choose two.)\nThe effect size statistic has changed to Effect size |p| since a different measure is used to compute it for rpb (and t-tests in general). Mousing over that field, though, shows that the same values pertain for “small,” “medium,” and “large” effects. (This isn’t always the case.) Again, please enter .3 for a “medium” effect.\nThe only other options here are for α-error prob and Power (1 - β err prob) for which we will use .05 and .8, respectively.\nClick on the Calculate button in the lower right. The following should appear:\n\n\nThe Ouput Parameters area contains somewhat different values. Instead of z, the test statistic is the Noncentrality parameter δ along with Critical t, which is tested—in this particular case—against a Df of 62. That is two df less than the Total sample size estimated to be needed to find an effect under these conditions.\n\n\n\nt-Tests\nAlong with a few other tests (like for rpb), all t-tests are under Test family &gt; t tests. Two of the most common t-tests are:\n\nMeans: Differences between two dependent means (matched groups)\n\nThis is used when the values used to compute the two means we’re testing come from the same participants, e.g., when we have pretest and posttest measures for the same patients.\n\nMeans: Differences between two independent means (two groups)\n\nThis is used whenever the means come from different participants. This is more often the case—and would apply even if we had pretest and posttest measures but from different patients (e.g., if we measured HCAHPS satisfaction scores from an outpatient unit before and after an intervention).\n\n\n\nLet us use the latter of those two, Means: Differences between two independent means (two groups)\nThe options for Input Parameters is similar to what we were presented with for Pearson correlations (in Section 16.2.4.1.1). However, when we mouse over the Effect size d field, we see different recommendations for sizes:\n\n\nCohen’s d, the measure of effect size now used, is computed differently than his q used to measure the effect size of difference between correlation coefficients. As I describe in Chapter 6, this is simply the standardized difference between the two means; it’s also one of the most commonly-used measures of effect size.\nLet’s stick with the default given, a “medium” effect of 0.5.\n\nα err prob can of course stay at 0.05\nBut please change Power (1-β prob) to .8\nWe could again stipulate a different ratio for the number of participants in the two groups, but let’s again assume we will have equal numbers and enter 1 in Allocation ratio N2/N1\n\nLike with rpb, in the Output Parameters section, we have Noncentrality parameter δ, Critical t, and Df. However, we now have two groups, so we instead have Sample size group 1, Sample size group 1, and Total sample size. That last field reports expecting to need 102 total participants, so the Df for the Critical t is 100.\n\n\nF-Tests\nF-tests are primarily (nearly only) used to test effects in ANOVAs and their ilk (ANCOVAs, MANOVAs, etc.). These include the most complex sample size analyses available within G*Power, and the most complex ones researchers typically try to do9 The reason for this is because the variance associated with effects differ depending on a couple of parameters, such as the number of variables and whether those variables are “fixed” or “random” effects10.\n\nANOVA: Fixed effects, omnibus, one-way\nA one-way ANOVA is simplest form of ANOVA: It contains only one input variable11 (and one output variable). This one-way ANOVA is really just a t-test that is used when the input variable has more than two levels to it. We would use a t-test to test the difference (in some continuous outcome) between two groups, say between those diagnosed or not diagnosed with a certain cancer. If there are more than two groups—if, e.g., we were instead looking at the stage of the cancer—we would use a one-way ANOVA.\nSample size estimates for one-way ANOVAs thus closely resemble those for t-tests. The difference is that, for one-way ANOVAs, we must indicate how many levels the input variable has. To conduct sample size estimates for them:\n\nUnder Test family, choose F tests\nUnder Statistical test, choose ANOVA: Fixed effects, omnibus, one-way\nUnder Type of power anlaysis choose A priori: Compute required sample size - given α, power, and effect size\nIn the Input Parameters section, when you mouse over the Effect size f field, you will once again see that the values Cohen (1988) suggests for “small” through “large” effects are different than for z or t tests:\n\nAs noted in Chapter 6, Cohen’s f denotes a the effect of a variable after partialing out the effects of other variables. In a one-way ANOVA, there are no other effects—no other variables—but the criteria for effect sizes is still based on this other standard.\n\nLet’s again leave this value at the default—a “medium” effect of 0.25\n\nWe can also again leave α-error prob to be 0.05 but change Power (1 - β err prob) to be .8.\nThe Number of groups field is slightly misleading. This is in fact asking for the number of levels of the input variable. So, e.g., if we are studying the effects of cancer stage and using the TNM staging system, we would have three groups—one for each of the three stages.\n\nGoing with that, please enter 3 in the Number of groups field.\n\nClicking Calculate generates the following output:\n\n\nThe output is thus:\n\nNoncentrality parameter λ\n\nIn this context, the noncentrality parameter is used to measure the power of the F-test. Larger numbers denote higher possibilities of larger power, depending on other parameters (viz., α, β, & N). Although good to report, this isn’t critical to consider since it’s used to help compute the other values in the Output Parameters section.\nThere nonetheless may be a use of further explained the noncentrality parameter statistic: You will see that what is now called the Central and noncentral distributions` in the figure do not look normally distributed. This is because F-tests use F distributions to test significance—not normal distributions. This is very similar to what is done with χ2 tests, and F distributions strongly resemble χ2 distributions—including in becoming closer to a normal distribution as the degrees of freedom increase. However, part of the F-test uses the numerator degrees of freedom, which is usually quite small—here, it’s 2. So, as the figure shows, F tests are used under conditions in which the distributions actually used to compute significance themselves aren’t appreciably normally distributed: They are “non-central” since the “center” of the distribution is unclear—as it is in the red (null) distribution in the figure.\n\nCritical F\n\nThis is the level of the F-statistic needed to establish significance under these conditions\n\nNumerator df\n\nThis number will be one less than the Number of groups\n\nDenominator df\n\nThis is the number that would be needed in the lower part of the F-test in order to find significance under these conditions—the error degrees of freedom. This in turn translates into most of the sample size we expect to need.\n\nTotal sample size\n\nThis is the Numerator df plus the Denominator df plus one more degree of freedom needed to estimate the intercept.\n\nActual power\n\nGiven the rounding needed to create a whole number for the numerator and denominate degrees of freedom, actual actual power will often be a little different from what we entered in the Power (1 - β err prob) field in the Input Parameters section.\n\n\nMore about power analyses with one-way ANOVAs is presented by UCLA’s Statistical Methods and Data Analytics site.\n\n\nANOVA: Fixed effects, special, main effects and interactions\n(For those who prefer videos—and soothing piano music—this video also presents conducting sample size estimates for multi-way ANOVAs.)\nAs we increase the complexity of our analyses, we next move on to ANOVA: Fixed effects, special, main effects and interactions. Here, we can estimate sample sizes for ANOVAs with one or more nominal variables.\nThe main issue with sample size estimates for ANOVA-family analyses is correctly assigning numerator degrees of freedom. And the main issue with doing that when all of the input variables are nominal is to understand how degrees of freedom are computed for them:\n\nThe degrees of freedom for any nominal main effect is one less than the number of levels of that variable12\n\nUsing TNM cancer staging as an example, the number of degrees of freedom for its main effect would be 3 – 1 = 2.\n\nThe degrees of freedom for an interaction between two nominal input variables is the product of their main effect degrees of freedom.\n\nLet us assume that we wanted to look at the interaction between dichotomized gender (self-identifying as female or male) and TNM cancer stage.\n\nThe degrees of freedom for gender’s main effect would be 2 – 1 = 1.\nThe degrees of freedom for the TNM cancer stage main effect would, of course, be 2.\nThe degrees of freedom for the gender \\(\\times\\) cancer stage interaction would be 2 \\(\\times\\) 1 = 2.\n\n\n\nContinuing with that example, if we were interested in knowing the significance of both main effects and their interaction, then the total number of numerator degree of freedom I need to consider in my sample size estimate is:\n\\(\\text{Total}\\text{ Numerator } df\\text{s} = df_{\\text{Cancer} \\text{ Stage}} + df_{\\text{Dichotomized}\\text{ Gender}} + df_{\\text{Cancer} \\text{ Stage} \\times \\text{Gender}\\text{ Interaction}}\\)\n\\(\\text{Total}\\text{ Numerator } df\\text{s} = 2 + 1 + 2\\)\n\\(\\text{Total}\\text{ Numerator } df\\text{s} = 5\\)\nIt is thus 5 that we would enter into the Numerator df field.\nWith this understanding in hand, let us compute the estimate sample size for this model:\n\nMousing over Effect size f, we see that a “medium” effect for Cohen’s f is .25, again the default given by G*Power, and again what we will retain since we have no prior studies to guide us better.\nα err prob we will leave as 0.05.\nPower (1 - β err prob) we will change to .8.\nNumberator df is 5 (those two main effects and their interaction).\nNumber of groups is the product of the total number of levels of the input variables.\n\nHere, TNM stage has 3 levels and dichotomized gender has 2, so the Number of groups is 3 \\(\\times\\) 2 = 613.\n\nClicking Calculate returns this output:\n\n\nIndicating that we expect to need 211 participants to find significant effects for both main effects and their interaction. Of course, we couldn’t divide this number evenly between the groups, so there would be a small amount of imbalance between them.\nNote that we don’t need to worry about the significance of every term we enter into a model. As I discuss in Chapter 4, we can enter terms in a model solely to isolate (“partial out”) their effects on those other terms whose effects we are interested in. Of course, do this cautiously so to not delude yourself into thinking you need fewer participants than you actually will. Sure, data are expensive, but investing to get some but not enough data—and then having to redo a study—is more expensive than getting enough the first time.\n\n\n\n\n\n\n\nAmong the most, well, powerful packages in R is pwr, which can easily compute same size estimates, etc, with a few succinct lines of code. We’ll also use the pwr2 package. WebPower is also very useful for power analyses.\n\n16.2.5 Comparing Two Independent Correlations\nWe examine how large a sample is needed to detect a difference between two Pearson correlations:\n\nlibrary(pwr)\n\n# Define the correlations\nr1 &lt;- 0.5 # The correlation we expect in one group\nr2 &lt;- 0.3 # The correlation we expect in the other group\n\n# Compute Cohen's q\nq &lt;- abs(0.5 * log((1 + r1)/(1 - r1)) - 0.5 * log((1 + r2)/(1 - r2)))\nq\n\n# Estimate required sample size per group to detect difference in correlations\npwr.norm.test(d = q, sig.level = 0.05, power = 0.8,\n              alternative = \"two.sided\")\n\n\n\n16.2.6 Independent Samples t-Test\nSample size needed to detect a medium effect (d = 0.5):\n\npwr.t.test(d = 0.5,            # Coden's d, effect size for a mean difference\n           power = 0.8,        # desired power\n           sig.level = 0.05,   # significance level\n           type = \"two.sample\" # Type of t-test being evaluated\n           )\n\n\n\n16.2.7 Paired Samples t-Test\nSample size needed when using paired/matched data:\n\npwr.t.test(d = 0.5,            # Coden's d, effect size for a mean difference\n           power = 0.8,        # desired power\n           sig.level = 0.05,   # significance level\n           type = \"paired\" # Type of t-test being evaluated\n           )\n\n\n\n16.2.8 Point-Biserial Correlation\nThis is the correlation between a continuous variable and a dichotomous variable (coded as 0 or 1).\n\npwr.r.test(r = 0.3,            # Pearson (or phi) correlation\n           power = 0.8,        # desired power\n           sig.level = 0.05    # significance level\n           )\n\n\n\n16.2.9 One-Way ANOVA\n\npwr.anova.test(k = 3,             # The number of levels of the IV (e.g., 2 for Experimental vs. Control group)\n               f = 0.25,          # Cohen's f, the effect size for terms in an ANOVA\n               power = 0.8,       # desired power\n               sig.level = 0.05)  # significance level\n\n\n\n16.2.10 Two-Way ANOVA\nTwo IVs with 3 and 2 levels respectively (6 groups total):\n\npwr2::pwr.2way(a = 3,           # levels in factor A\n               b = 2,           # levels in factor B\n               alpha = 0.05,    # significance level\n               power = 0.8,     # desired power\n               f = 0.25,        # effect size (Cohen's f)\n               n = NULL)        # compute required sample size per cell\n\n\n\n16.2.11 Power Curves\n\nPower Curve for Independent-Samples t-Test\n\n# Plot the power curve for a range of sample sizes in a two-sample t-test\ncurve(\n  expr = pwr::pwr.t.test(\n    n = x,                         # sample size per group\n    d = 0.5,                       # Cohen's d effect size (medium effect)\n    sig.level = 0.05,              # significance level (alpha)\n    type = \"two.sample\"            # specifies independent-samples t-test\n  )$power,                         # extract power from the result\n  from = 10, to = 200,             # range of sample sizes (per group)\n  xlab = \"Sample Size per Group\",  # x-axis label\n  ylab = \"Power\",                  # y-axis label\n  main = \"Power Curve for d = 0.5\" # title of the plot (match d above)\n)\n\n# Add a reference line for the conventional 80% power threshold\nabline(h = 0.8, col = \"red\", lty = 2)\n\n\n\nPower Curve for One-Way ANOVA\n\n## Power Curve for One-Way ANOVA (e.g., 3 groups)\n\n# Plot power vs. total sample size for one-way ANOVA\ncurve(\n  expr = pwr::pwr.anova.test(\n    k = 3,                         # number of groups\n    n = x / k,                     # converts total sample size to per-group n\n    f = 0.25,                      # Cohen's f effect size (medium)\n    sig.level = 0.05               # significance level (alpha)\n  )$power,\n  from = 30, to = 300,             # total sample size range\n  xlab = \"Total Sample Size\",      # x-axis label\n  ylab = \"Power\",                  # y-axis label\n  main = \"Power Curve for One-Way ANOVA (k = 3, f = 0.25)\" # title\n)\n\n# Add conventional power threshold line\nabline(h = 0.8, col = \"red\", lty = 2)\n\n\n\nPower Curve for Two-Way ANOVA (Main Effects + Interaction)\npwr.f2.test() is used for general linear models including two-way ANOVA. To use this, you need:\n\nu: numerator degrees of freedom (e.g., 1 for each main effect, plus interaction)\nv: denominator degrees of freedom (sample size – predictors – 1)\nf2: Cohen’s \\(f^2\\) effect size. \\(f^2 = \\frac{f^2_{\\text{anova}}}{1 - f^2_{\\text{anova}}}\\), so for \\(f = 0.25\\), \\(f^2 ≈ 0.0625\\)\n\n\n## Power Curve for Two-Way ANOVA (2x3 design, 2 main effects + interaction)\n\n# Total df = (levels_A - 1) + (levels_B - 1) + (A*B interaction df)\nnumerator_df &lt;- (2 - 1) + (3 - 1) + ((2 - 1) * (3 - 1))  # = 1 + 2 + 2 = 5\nf2_value &lt;- 0.25^2 / (1 - 0.25^2)  # convert Cohen's f to f^2 ≈ 0.0625 / 0.9375 ≈ 0.0667\n\n# Power curve for GLM (e.g., 2x3 ANOVA with 5 df for predictors)\ncurve(\n  expr = pwr::pwr.f2.test(\n    u = numerator_df,                   # numerator df (main + interaction)\n    v = x - numerator_df - 1,           # denominator df = N - u - 1\n    f2 = f2_value,                      # Cohen's f^2 effect size\n    sig.level = 0.05                    # significance level (alpha)\n  )$power,\n  from = 60, to = 300,                  # total sample size range\n  xlab = \"Total Sample Size\",           # x-axis label\n  ylab = \"Power\",                       # y-axis label\n  main = \"Power Curve for Two-Way ANOVA (f = 0.25, df = 5)\" # title\n)\n\n# Add reference line at power = 0.8\nabline(h = 0.8, col = \"red\", lty = 2)"
  },
  {
    "objectID": "Intro_to_GPower.html#sec-additional_resources_power_analyses",
    "href": "Intro_to_GPower.html#sec-additional_resources_power_analyses",
    "title": "16  Introduction to Power and Sample Size Estimation Using Either G*Power or R",
    "section": "16.3 Additional Resources",
    "text": "16.3 Additional Resources\n\n16.3.1 G*Power Guides & Tutorials\n\nG*Power Manual, which is quite useful\nThe UCLA Guide to G*Power contains detailed but digestible guides to estimates for most of the analyses you’ll conduct (except maybe χ2 tests).\nMayr, S., Erdfelder, E., Buchner, A., & Faul, F. (2007). A short tutorial of GPower. Tutorials in Quantitative Methods for Psychology, 3(2), 51–59. doi: 10.20982/tqmp.03.2.p051.\nFaul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: a flexible statistical power analysis program for the social, behavioral, and biomedical sciences. Behavior Research Methods, 39(2), 175–191. doi: 10.3758/BF03193146. RIS\nKang, H. (2021). Sample size determination and power analysis using the GPower software. Journal of Educational Evaluation for Health Professions, 18, 1–17. doi: 10.3352/jeehp.2021.18.17. RIS\n\n\n\n16.3.2 Further Readings and Explanations\n\nBujang, M. A. (2021). A step-by-step process on sample size determination for medical research. The Malaysian Journal of Medical Sciences, 28(2), 15–27. doi: 10.21315/mjms2021.28.2.2\n\nProbably best is Table 1 which presents a nice list of other sources for more information about sample size estimations for various types of analyses from correlations to exploratory factor analysis.\n\nDas, S., Mitra, K., & Mandal, M. (2016) Sample size calculation: Basic principles. Indian Journal of Anaesthesia, 60(9), 652–656. doi: 10.4103/0019-5049.190621. PMID: 27729692; PMCID: PMC5037946. NBIB\nHunt, A. (n.d.). A researcher’s guide to power analysis.\n\n\n\n16.3.3 Sample Size Estimations and Guidelines for More Complex Designs\n\nANCOVAs\n\nBorm, G. F., Fransen, J., & Lemmens, W. A. J. . (2007). A simple sample size formula for analysis of covariance in randomized clinical trials. Journal of Clinical Epidemiology, 60(12), 1234–1238. doi: 10.1016/j.jclinepi.2007.02.006. RIS\nShieh, G. (2020). Power Analysis and Sample Size Planning in ANCOVA Designs. Psychometrika, 85(1), 101–120. doi: 10.1007/s11336-019-09692-3. RIS\nTeerenstra, S., Eldridge, S., Graff, M., de Hoop, E., & Borm, G. F. (2012). A simple sample size formula for analysis of covariance in cluster randomized trials. Statistics in Medicine, 31(20), 2169–2178. doi: 10.1002/sim.5352. RIS\n\n\n\nLogistic Regression\n\nMotrenko, A., Strijov, V., & Weber, G.-W. (2014). Sample size determination for logistic regression. Journal of Computational and Applied Mathematics, 255, 743–752. doi: 10.1016/j.cam.2013.06.031. RIS\n\n\n\nFactor Analysis and Structural Equation Models\n\nGrace-Martin, K. (n.d.). How big of a sample size fo you need for factor analysis? The Analysis Factor. https://www.theanalysisfactor.com/sample-size-needed-for-factor-analysis/\nKelley, K., Lai, K. (2018). Sample size planning for confirmatory factor models. In The Wiley Handbook of Psychometric Testing (pp. 113–138). John Wiley & Sons, Ltd. doi: https://doi.org/10.1002/9781118489772.ch5. RIS\nLa Du, T. J., & Tanaka, J. S. (1989). Influence of sample size, estimation method, and model specification on goodness-of-fit assessments in structural equation models. Journal of Applied Psychology, 74(4), 625–635. doi: 10.1037/0021-9010.74.4.625. RIS\nMundfrom, D. J., Shaw, D. G., & Ke, T. L. (2005). Minimum sample size recommendations for conducting factor analyses. International Journal of Testing, 5(2), 159–168. doi: 10.1207/s15327574ijt0502_4. RIS\nNicolaou, A. I., & Masoner, M. M. (2013). Sample size requirements in structural equation models under standard conditions. International Journal of Accounting Information Systems, 14(4), 256–274. doi: 10.1016/j.accinf.2013.11.001. RIS Pearson, R., H. & Mundform, D. J. (2010). Recommended sample size for conducting exploratory factor analysis on dichotomous data. Journal of Modern Applied Statistical Methods, 9(2), 359–368. doi: 10.22237/jmasm/1288584240.\nWolf, E. J., Harrington, K. M., Clark, S. L., & Miller, M. W. (2013). Sample size requirements for structural equation models: An evaluation of power, bias, and solution propriety. Educational and Psychological Measurement, 73(6), 913–934. doi: 10.1177/0013164413495237. RIS\n\n\n\n\n16.3.4 Online Tools\n\nKovacs, M., van Ravenzwaaij, D., Hoekstra, R., Aczel, B.. (2022). SampleSizePlanner: A tool to estimate and justify sample size for two-group studies. Advances in Methods and Practices in Psychological Science. 2022;5(1). doi: 10.1177/25152459211054059. RIS\n\n\n\n\n\n\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nYuan, K.-H., & Maxwell, S. E. (2005). On the post hoc power in testing mean differences. Journal of Educational and Behavioral Statistics, 30(2), 141–167. https://articles.viriya.net/on_the_post_hoc_power_in_testing_mean_differences.pdf\n\n\nZhang, Y., Hedo, R., Rivera, A., Rull, R., Richardson, S., & Tu, X. M. (2019). Post hoc power analysis: Is it an informative and meaningful analysis? General Psychiatry, 32(4), e100069–e100069. https://doi.org/10.1136/gpsych-2019-100069"
  },
  {
    "objectID": "Intro_to_GPower.html#footnotes",
    "href": "Intro_to_GPower.html#footnotes",
    "title": "16  Introduction to Power and Sample Size Estimation Using Either G*Power or R",
    "section": "",
    "text": "Although we often think of figures like this showing differences in estimates of means, these could instead be estimating, e.g., population proportions (whether, e.g., the proportion of members of two populations have different mortality rates for a given disorder). Since we tend to think about this in terms of means, though, let’s just stick with that.↩︎\nSome analyses are not this straight forward, though, for various reasons. Among the reasons why it isn’t always this straight forward is because the N for the sample is divided out unevenly among the effects being tested—especially for nested effects; we will address this in one of the cases you are more likely to encounter (it’s different types of ANOVAs), but first let’s orient ourselves to G*Power and conduct some more straight-forward sample size estimates.↩︎\nWell, to be pedantic, R may be considered software.↩︎\nAlas, there is no GNU/Linux version.↩︎\nEven though using an apostrophe in r's is incorrect: It’s plural, not possessive, so should instead be rs. Just one of my many grammatical pet peeves.↩︎\nIt’s this monstrosity: \\(q = {\\frac{1}{2}\\text{log}_e\\frac{1 + r_2}{1 - r_2} -\\frac{1}{2}\\text{log}_e\\frac{1 + r_1}{1 - r_1}}\\) for correlations r1 and r2.↩︎\nChanging the order of them so that Correlation coefficient ρ1 is .5 and Correlation coefficient ρ2 is .3 will make the effect size measure, q, into a positive value, but that doesn’t matter: The positive or negative sign simple indicates which correlation is larger than which. Making it positive will also move the H1 distribution to the right of the H0 one in the figure at the top.↩︎\nG*Power was created and is maintained by Edgar Erdfelder, Franz Faul, and their colleagues at Heinrich-Heine-Universität Düsseldorf.↩︎\nI give resources in the Section 16.3 section about estimating sample sizes for factor analyses, etc. These are important…but much less straight forward.↩︎\n“Fixed” factors are those for which all possible levels are present in the data, e.g., if data for both those who survive and die from an illness are present. “Random” factors are those for which a random subset of all possible levels of a variable are present; these are typically continuous variables (such as height). The ways that error is estimated for these differ.↩︎\nYou may well have learned to call this the “independent variable” (IV). This is not wrong—it’s just not always correct. An IV is the term used to indicate a variable that researchers manipulate or measure in a true experimental design to observe their effects on some outcome—that which (in that context) is called a dependent variable and that I’m calling by the more general term of “output variable.” The input variable in an ANOVA is indeed often an IV. However, there are certainly times when we wish to test the effects of a variable that isn’t an IV, for example if we’re working with secondary data in which no variables are forcibly IVs or DVs.↩︎\nWe deduct “1” from each main effect because we only need to establish the values for all levels but one. That last level can be deduced from the other levels. As a simplified analogy, if I knew that x + y + z = 6, then I only need to know that x = 2 and that y = 2 to know that z also equals 2.↩︎\nNotice that, in this case, including the interaction term in the model doesn’t affect the estimating needed sample size.↩︎"
  },
  {
    "objectID": "data_exploration_with_r.html#common-exploration-commands",
    "href": "data_exploration_with_r.html#common-exploration-commands",
    "title": "17  Data Exploration with R",
    "section": "17.1 Common Exploration Commands",
    "text": "17.1 Common Exploration Commands\nhttps://www.r-bloggers.com/2018/11/explore-your-dataset-in-r/\nNearly the opposite of SPSS, R is a “quiet” language that only talks back to you when you explicitly ask it to. This isn’t always desirable when you’re exploring data since there may be aspects of the exploration you would have forgotten or not thought of doing if you hadn’t seen the output first. At least R makes up a bit for it with some pretty pithy commands—and even a few lengthy outputs—that we’ll cover here.\nWe’ll use some pre-existing data packaged with R for our examples here. Which data are available can be easily found with:\n\ndata(infert)\nhead(infert)\n\n  education age parity induced case spontaneous stratum pooled.stratum\n1    0-5yrs  26      6       1    1           2       1              3\n2    0-5yrs  42      1       1    1           0       2              1\n3    0-5yrs  39      6       2    1           0       3              4\n4    0-5yrs  34      4       2    1           0       4              2\n5   6-11yrs  35      3       1    1           1       5             32\n6   6-11yrs  36      4       2    1           1       6             36\n\nsummary(infert)\n\n   education        age            parity         induced      \n 0-5yrs : 12   Min.   :21.00   Min.   :1.000   Min.   :0.0000  \n 6-11yrs:120   1st Qu.:28.00   1st Qu.:1.000   1st Qu.:0.0000  \n 12+ yrs:116   Median :31.00   Median :2.000   Median :0.0000  \n               Mean   :31.50   Mean   :2.093   Mean   :0.5726  \n               3rd Qu.:35.25   3rd Qu.:3.000   3rd Qu.:1.0000  \n               Max.   :44.00   Max.   :6.000   Max.   :2.0000  \n      case         spontaneous        stratum      pooled.stratum \n Min.   :0.0000   Min.   :0.0000   Min.   : 1.00   Min.   : 1.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:21.00   1st Qu.:19.00  \n Median :0.0000   Median :0.0000   Median :42.00   Median :36.00  \n Mean   :0.3347   Mean   :0.5766   Mean   :41.87   Mean   :33.58  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:62.25   3rd Qu.:48.25  \n Max.   :1.0000   Max.   :2.0000   Max.   :83.00   Max.   :63.00  \n\ndplyr::glimpse(infert)\n\nRows: 248\nColumns: 8\n$ education      &lt;fct&gt; 0-5yrs, 0-5yrs, 0-5yrs, 0-5yrs, 6-11yrs, 6-11yrs, 6-11y…\n$ age            &lt;dbl&gt; 26, 42, 39, 34, 35, 36, 23, 32, 21, 28, 29, 37, 31, 29,…\n$ parity         &lt;dbl&gt; 6, 1, 6, 4, 3, 4, 1, 2, 1, 2, 2, 4, 1, 3, 2, 2, 5, 1, 3…\n$ induced        &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 2…\n$ case           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ spontaneous    &lt;dbl&gt; 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1…\n$ stratum        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ pooled.stratum &lt;dbl&gt; 3, 1, 4, 2, 32, 36, 6, 22, 5, 19, 20, 37, 9, 29, 21, 18…\n\n# skimr::skim(infert)\n# DataExplorer::create_report(infert)\n\nThe data() command can be used both to load data into R and—with the parentheses left blank—list out whatever data are currently available. Please note that data() will list dat sets that you loaded in addition to the ones that came pre-installed with R (or any packages you’ve invoked).\nThis Statistics Globe page provides several R commands that are handy for data exploration. More methods for visualizing the data while you explore it are given this Towards Data Science page."
  },
  {
    "objectID": "data_exploration_with_r.html#using-sql-and-tidyverse",
    "href": "data_exploration_with_r.html#using-sql-and-tidyverse",
    "title": "17  Data Exploration with R",
    "section": "17.2 Using SQL and tidyverse",
    "text": "17.2 Using SQL and tidyverse\nThis R Views newsletter post by Vachharajani presents a nice overview using SQL and the tidyverse “ecosystem” of packages for R.\nSQL is a venerable programming language used to manage and manipulate data—especially very large sets of data. R uses RAM to hold and manipulate data, and so can flounder with very large sets of data; using SQL can thus help. There are several ways to use SQL and R together, however the most common are either to first prepare the data in SQL before exporting it (or parts of it) into R or working from within R to make queries to the SQL-prepared data from within R.\ntidyverse is a set of packages designed to make common tasks—especially the manipulation and presentation of data—both more flexible and intuitive. Intuition is a relative thing, and as much as the tidyverse grammar and even vocabulary do make sense, they take some learning to understand—learning that is really in addition to learning core R syntax and grammar."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abelson, R. P. (1995). Statistics as Principled\nArgument. Lawrence Erlbaum Associates\nPublishers. https://articles.viriya.net/statistics_as_principled_argument.pdf\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R.\n(2011). Introduction to Meta-Analysis (1. Aufl.,\np. xxix). Wiley. https://doi.org/10.1002/9780470743386\n\n\nBreiman, L. (2001). Statistical modeling: The two cultures.\nStatistical Science, 16(3), 199–231. https://articles.viriya.net/statistical_modeling_the_two_cultures.pdf\n\n\nChen, H., Cohen, P., & Chen, S. (2010). How big is a big odds ratio?\nInterpreting the magnitudes of odds ratios in\nepidemiological studies. Communications in Statistics - Simulation\nand Computation, 39(4), 860–864. https://doi.org/10.1080/03610911003650383\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum Associates.\n\n\nDavis, L. L., Broome, M. E., & Cox, R. P. (2002). Maximizing\nretention in community-based clinical trials. Journal of Nursing\nScholarship, 34(1), 47–53. https://doi.org/10.1111/j.1547-5069.2002.00047.x\n\n\nGustavson, K., von Soest, T., Karevold, E., & Røysamb, E. (2012).\nAttrition and generalizability in longitudinal studies: Findings from a\n15-year population-based study and a Monte Carlo simulation\nstudy. BMC Public Health, 12, 918. https://doi.org/10.1186/1471-2458-12-918\n\n\nHausman, C., & Rapson, D. S. (2017). Regression discontinuity in\ntime: Considerations for empirical applications.\nNational Bureau of Economic Research Working Paper Series,\nNo. 23602. https://doi.org/10.3386/w23602\n\n\nHausman, C., & Rapson, D. S. (2018). Regression discontinuity in\ntime: Considerations for empirical applications. Annual\nReview of Resource Economics, 10(1), 533–552. https://doi.org/10.1146/annurev-resource-121517-033306\n\n\nKao, L. S., & Green, C. E. (2008). Analysis of variance:\nIs there a difference in means and what does it mean?\nJournal of Surgical Research, 144(1), 158–170. https://doi.org/10.1016/j.jss.2007.02.053\n\n\nKhamis, H. (2008). Measures of association: How to choose?\nJournal of Diagnostic Medical Sonography, 24(3),\n155–162. https://doi.org/10.1177/8756479308317006\n\n\nKraft, M. A. (2020). Interpreting effect sizes of education\ninterventions. Educational Researcher, 49(4), 241–253.\nhttps://doi.org/10.3102/0013189X20912798\n\n\nMonsalves, M. J., Bangdiwala, A. S., Thabane, A., & Bangdiwala, S.\nI. (2020). LEVEL (Logical Explanations &\nVisualizations of Estimates in\nLinear mixed models): Recommendations for reporting\nmultilevel data and analyses. BMC Medical Research Methodology,\n20, 1–9. https://doi.org/10.1186/s12874-019-0876-8\n\n\nOkada, K. (2013). Is omega squared less biased? A\ncomparison of three major effect size indices in one-way\nANOVA. Behaviormetrika, 40(2), 129–147.\nhttps://doi.org/10.2333/bhmk.40.129\n\n\nPolit, D. F., & Beck, C. E., Cheryl T. (2017). Nursing\nResearch: Generating and Assessing\nEvidence for Nursing Practice (Tenth).\nWolters Kluwer.\n\n\nRaper, S. (2020). Leo Breiman’s \"two cultures\".\nSignificance, 17, 34–37. https://doi.org/10.1111/j.1740-9713.2020.01357.x\n\n\nTeague, S., Youssef, G. J., Macdonald, J. A., Sciberras, E., Shatte, A.,\nFuller-Tyszkiewicz, M., Greenwood, C., McIntosh, J., Olsson, C. A.,\n& Hutchinson, D. (2018). Retention strategies in longitudinal cohort\nstudies: A systematic review and meta-analysis. BMC Medical Research\nMethodology, 18(1), 151–151. https://doi.org/10.1186/s12874-018-0586-7\n\n\nVisalakshi, J., & Jeyaseelan, L. (2014). Confidence interval for\nskewed distribution in outcome of change or difference between methods.\nClinical Epidemiology and Global Health, 2(3),\n117–120. https://doi.org/10.1016/j.cegh.2013.07.006\n\n\nWeisburd, D., & Britt, C. (2007). Measures of association for\nnominal and ordinal variables (pp. 335–380). Springer\nUS. https://doi.org/10.1007/978-0-387-34113-2_13\n\n\nYu, H., Jiang, S., & Land, K. C. (2015). Multicollinearity in\nhierarchical linear models. Social Science Research,\n53, 118–136. https://doi.org/10.1016/j.ssresearch.2015.04.008\n\n\nYuan, K.-H., & Maxwell, S. E. (2005). On the post hoc power in\ntesting mean differences. Journal of Educational and Behavioral\nStatistics, 30(2), 141–167. https://articles.viriya.net/on_the_post_hoc_power_in_testing_mean_differences.pdf\n\n\nZhang, Y., Hedo, R., Rivera, A., Rull, R., Richardson, S., & Tu, X.\nM. (2019). Post hoc power analysis: Is it an informative\nand meaningful analysis? General Psychiatry, 32(4),\ne100069–e100069. https://doi.org/10.1136/gpsych-2019-100069"
  },
  {
    "objectID": "common_statistical_symbols.html#footnotes",
    "href": "common_statistical_symbols.html#footnotes",
    "title": "Appendix A — Common Statistical Symbols",
    "section": "",
    "text": "More specifically, the number of values that can be computed in a given information space. If I have an equation like \\(x + 2 = 3\\), then I have only one degree of freedom: \\(x\\) can only have one possible value (here, a 1 for the mathematically disinclined). If I instead have an equation like \\(x + y = 3\\), there are two values to compute, and so I have two degree of freedom. Importantly, notice that once I know one value—once I “spend” one degree of freedom, I then can determine the other value: If \\(x\\) = 1, then we can compute that \\(y\\) = 2; if \\(x\\) = 0, then we can compute that \\(y\\) = 3. Therefore, “spending” degrees of freedom to estimate values (e.g., to compute the sample mean) uses up some of the total information available, but also lets us more accurately determine the value of other estimates, such as how far away a given person’s score is from that mean. We can use a data set’s degrees of freedom to make a certain number of insights, but the total number of insights is strictly limited by the size of our dataset. We could use those degrees of freedom to make different insights, but not an infinite number of insights.↩︎\nDevoting more of the available information to making a decision makes that decision more insightful. However, since any given set of data only has so many degrees of freedom—only so much information useful for making decisions—we should economize how much information we use to make a given decision.↩︎"
  },
  {
    "objectID": "vocab.html#commonconfusing-statistical-scientific-terms",
    "href": "vocab.html#commonconfusing-statistical-scientific-terms",
    "title": "Appendix B — Common/Confusing Statistical & Scientific Terms",
    "section": "B.1 Common/Confusing Statistical & Scientific Terms",
    "text": "B.1 Common/Confusing Statistical & Scientific Terms\n\n\nTable B.1: Common/Confusing Statistical & Scientific Terms\n\n\nTerm\nMeaning in Science & Statistics\n\n\n\n\nCriterion\nThe output/outcome variable, the variable that is measured to see the effects of other variables on it. (See Predictor, below).\nAlso called:\n• Dependent variable (DV)\n• Exogenous variable\n• Outcome (or “output”) variable\n• Response variable\n• Regressand\n• Target\nCalling it a criterion implies that it is being used as the basis or standard by which to test the importance of the predictors (or input variables) or even the success of our endeavors.\n\n\nDescriptive\nDescriptive statistics simply, well, describe a sample of data. They nearly always make no assumptions about the data—and make none about the population from which they were drawn (except perhaps that each datum is drawn independently from any other data and from the identical population.)\n\n\nFactor\nEither:\n• A predictor variable of any type\n• An independent variable manipulated or controlled by researchers; in this sense, a factor is usually—but not necessarily—a catagorical variable\n• A common source of variance/information in one or more ostensible variables\n\n\nFixed\nNo, nothing in science is every fixed (or broken : ). “Fixed” has two, mostly-different uses in statistics:\n1. A fixed variable is one in which all possible levels are present in the sample data. For example, if a sample of nursing home residents indicated that some had fallen while others had not (and there are no other possible categories), then falls would be a fixed factor.\nVariables in which not all levels are present in the data are called random. Often (but not always), these are continuous variables where it is imnpossible for all levels to be present in a sample of data, such as height which could be measured to nearly infinite levels.\nThe important difference between fixed and random factors is that analyses must estimate the effects of levels not present for random factors, an issue that can eat up degrees of freedom, etc.\n2. Fixed effects in a linear regression (like in SPSS’s MIXED function, Chapter 10) are terms that have the same coefficient (e.g., same β weight for that term in the model) for all participants. For example, if all participants were assigned to either the Experimental or Control group, regardless of which hospital they were admitted to, then this would be a fixed effect. In hierarchical models, these are usually the levels that have something else nested in them, e.g., if patients were nested in hospitals.\nIn this sense, random effects are terms that are nested within an other level2.\n\n\niid\nAn abbreviation that describes two, important characteristics of a set of data collected on a given variable. It stands for “independently and identically distributed,” meaning that:\n1. the value of each data point in a given variable is independent from the value of all/any other data point for that variable and\n2. each of those data points in that variable are drawn from the same distribution, e.g., they’re all drawn from a normal distribution.\nThat the data in a given variable is iid is one of the most important assumptions in inferential statistics. Often, it can’t be violated without us loosing the validity of conclusions drawn from those data.\n\n\nIndicator\nA synonym for a dummy variable that indicates whether something is present or not present, e.g., recovered versus not recovered.\n\n\nInferential\nInferential analyses rely on assumptions being made about the population from which those data were drawn. This often includes assuming that the population is normally distributed. They are typically distinguished from descriptive statistics that make no (or fewer) assumptions about the population.\n\n\nMean\nThe average of a set of data. I’m including it in this list of common/confusing terms simply to note the main ways a mean can be computed, and their respective uses:\n• Arthmetic mean: The one you know, in which values are summed and then divided by the number of values. It is used when there are no particular reasons to use an other method.\n• Geometric mean: Values are multiplied instead of added. We then take the nth root of this product. Geometric means are useful to compare very different values; to get the mean of percents, proportions, etc., and when the values are related to each other (like all the percents being of the same thing, like inflation).\n• Harmonic mean: It is computed as “the reciprocal of the average of the reciprocals of the data values”. It is used when we want to reduce the weight of larger values, such as when a distribution is positively skewed (i.e., has disproportionate number of large values). An example is length of time where events can’t be shorter than zero, but sometimes can take a lot longer than they should3\n• Weighted mean: Any of the above types of means can also be weighted. In a weighted mean, some of the values are given heavier4 weights (their values are multiplied by some number to make them affect the overall mean differently) than other values so that those weighted values contribute more to the overall mean. This is commonly done when we were unable to sample enough people of a certain type, such as when we were unable to sample enough members of a minoritized group.\n\n\nMultiple vs. multivariate\n(e.g., multiple vs. multivariate linear regression)\n• “Multiple” indicates that there is more than one linear regression equation; this would happen if there is more than one outcome\n• “Multivariate” indicates that there is more than one predictor.\n\n\nNon-ostensible\nA variable that cannot be directly observed or other perceived. These are usually theoretical and abstract concepts—constructed ideas—that are assumed to give rise to “ostensible” variables that can be empirically perceived. Other terms for these and related concepts are:\n• Latent\n• Unobserved\nThese are similar—sometimes even defined by—the factors that emerge from factor (latent variable) analysis.\n\n\nNon-parametric\nNon-parametric analyses make no (or few) assumptions about the population distribution, viz., that it is normally distributed. Non-parametric analyses tend to be more robust than parametric analyses; they also tend to be used for variables of lower measurement levels (e.g., for ordinal instead of interval/ratio data).\n\n\nOstensible\nA variable that can be directly, empirically observed. This is used to distinguish a variable from non-ostensible ones that are theorized to manifest in observable ways (through ostensible variables). Other terms for these and related concepts are:\n• Manifest variable\n• Observable variable\n\n\nParametric\nParametric analyses are inferential analyses that make assumptions about the mathematical values (“parameters”) about the population’s distribution. Nearly always, this is the assumption that the population distribution is normal. Therefore, parametric analyses are those that assume normality. The term is also nearly always used to contrast these analyses with non-parametric ones that do not require (as many) assumptions about the population distribution. Making fewer assumptions, non-parametric analyses tend to be more robust.\n\n\nParsimony\nThe desirable trait of communicating efficiently, saying a lot of information clearly and succinctly. It is also said of explanations and theories, suggesting a strong explanation that is “elegantly” simple yet generally useful.\n\n\nPopulation\nA well-defined and -delimited group of individuals (patients, nurses, etc.) about which insights are made based on a smaller sample of members of that population. The sample are those chosen to be studied directly; the population are those to whom conclusions made from the sample can be justifiably applied.\n\n\nPower\nThe probability to detect a real effect. The chance not to make a false negative (Type 2) error.\n\n\nPredict\nIn statistics, prediction is our ability to use what we know to make inferences about what we don’t. This could be information we have from the past and present that we use to guess at the future. But it could also be using known information about the past to make inferences about other, past events we don’t know about. And yes, this certainly includes using sample data to infer population values.\nA rather clever use of prediction is to randomly split a larger set of data in half. Use half to create a model with a given set of parameters & values. And then see how well those parameters, etc. predict the other half of the data. This allow us to conduct a very authentic test of how good our estimates were.\n\n\nPredictor\nOne of the many terms used to indicate the variables added to a linear regression model to test the effect on the outcome.\nAlso called:\n• Explanatory variable\n• Independent variable (IV)\n• Input variable\n• Regressor\n\n\nRandom\nGenerally, the crux of randomness is that the value is unbiased and—in the long run—therefore an accurate representation of the true state. However, it can also refere to:\n• The process of selecting a participant, level, etc. without bias so that any value is either equally likely to be chosen or at least chosen by the same rules & odds as any other value\n• A “random variable” is a rather generic term for any empirical value that can take multiple values, and that the value is takes is “iid”: independent from and identically distributed as all other measurements taken on that variable\n• A “random effect” is a term in a hierarchical linear modle (aka multilevel model, Chapter 10) that is nested within an other variable, like patients nested in hospitals.\n\n\nSpell\nIn longitudinal analyses, an occasion on which some outcome is present. For example, each time when a woman is pregnant or when someone with a substance abuse disorder recidivates. “Spell” is usually (but not always) used when an event can happen more than once in a longitudinal study.\n\n\nVariance\nA measure of dispersion. It is the square of the standard deviation (when computed for the dispersion of a variable); it is the square of the distance from a regression line (when computed in a linear regression).\nIt is also a measure of the total amount of information in a variable; the more information, the richer a variable is, but the more there is to try to understand.\n\n\nWave\nAn instance of data collection at a given point in time. This terms is usually only used when there are more than one data collection occasions, viz., when a study is longitudinal.\nAlso called:\n• Event\n• Endogenous variable\n• Instance\n• Period\n• Phase\n• Time point"
  },
  {
    "objectID": "vocab.html#terms-for-different-types-of-analyses",
    "href": "vocab.html#terms-for-different-types-of-analyses",
    "title": "Appendix B — Common/Confusing Statistical & Scientific Terms",
    "section": "B.2 Terms for Different Types of Analyses",
    "text": "B.2 Terms for Different Types of Analyses\n\n\nTable B.2: Terms for Different Types of Analyses\n\n\n\nMeasurement\nLevel\n\n\n\n\n\nAnalysis\nOutcome Variables(s)\nPredictor(s)\nUses & Notes\n\n\nANOVA\nContinuous\nNominal\n• Understood by many, so easily communicated\n• Variance determined by ordinary least squares\n• Typically only used to test significance of individual (main effect and/or interaction) terms\n\n\nOne-way ANOVA etc.\nContinuous\nOne Nominal\n• The “one-way” indicates that there is only one predictor.\n• If there are two predictors, it’s instead called a “two-way” ANOVA.\n• We could also use “three-way” ANOVA, etc., but we instead just give up and call them “multi-way” ANOVAs when there are ≥3 nomain predictors.\n\n\nANCOVA\nContinuous\n≥1 Nominal &\n≥1 Continuous\n• Contains one or more continuous “covariates”\n• Variance determined by ordinary least squares\n• Typically only used to test significance of individual (main effect and/or interaction) terms\n\n\nRepeated-measures ANOVA\nContinuous\nNominal\n• A repeated-measures ANOVA not only tests the effect of ≥1 predictors, but also whether the effect(s) of the predictor(s) changes over time.\n• The times when data are collected must be evenly spaced\n• An, e.g., “repeated-measures ANCOVA” is an ANCOVA (with ≥1 nominal and ≥1 continuous) that tests differences in predictors’ effects over time\n\n\nMANOVA\n≥2 Continuous\nNominal\n• A MANOVA includes two or more outcome variables.\n• The benefit of conducting a MANOVA over two (or more), separate ANOVAs (one for each outcome) is that a MANOVA also tests for (and accounts for) the relationships between the outcomes\n\n\nGeneral linear model\nContinuous\nNominal and/or continuous\n“General linear model” is the term for any linear model that:\n• Has a continuous outcome\n• Assumes a linear relationship between the predictors and the outcome.\nGeneral linear models include ANOVAs (and their ilk), t-tests, F-tests, etc. They are one of the types of generalized linear models.\n\n\nGeneralized linear model\nAny\nAny\n• “Generalized linear model” is the term used for any inferential analysis that uses the general formula of \\(Y = b_{0} + b_{1}X_{1} + ... + b_{k}X_{k} + e\\) to describe the relationship between \\(k\\) predictors and ≥1 outcomes. This includes general linear models (and thus ANOVAs, etc.), logistic regression, structural equation models, etc.\n• Although the formula is written as a linear equation, generalized linear models can model many types of non-linear relationships between predictors and outcomes.\n ◦ They can test dichotomous outcomes (viz., logistic regression), logarithmic outcomes, etc.\n• Data can be heteroskedastic.\n• Variables need not be normally distributed (but their distribution must still be correctly modeled by the equation)\nThe term “generalized linear model” is not as commonly used as “general linear model” (instead one uses the term for the type of model conducted), but it is still useful to know the difference.\n\n\nLogistic regression\nDichotomous\nNominal or continuous\nLogistic regression models are used to estimate the effects on a dichotomous outcome, such as whether or not a patient has a given condition.\n\n\nOrdinal regression\nOrdinal\nNominal or continuous\nAlthough I believe we can often treat ordinal variables as continuous (i.e., lump them in with interval & ratio variables), there are times when we should indeed treat a variable as assuming nothing more than ordinalism, such as when we have only a few levels and know the distances between them vary, but don’t know how much.\n\n\nMultinomial (logistic) regression\nNominal\nNominal or continuous\nThese are very similar to logistic regression, differing mainly in that there are more than two levels to the outcome variable. Like logistic regression, one of the main goals of multinomial regression is to determine (predict) the category one falls into based on one’s values for the predictors.\n\n\nMultiple linear regression\nContinuous\n≥2 Nominal or continuous\nThis is a general term for a linear model that has two or more predictors.\n\n\nt-test\nContinuous\nOne dichotomous\n• t-Tests are used to test the difference in two values. They are used, for example, to test:\n• The difference between the means of two groups\n ◦ Such as two study groups (e.g,. experimental & control),\n ◦ Or the means for each level of a nominal variable with two levels (e.g., those diagnosed / not diagnosed with a condition)\n• Changes in one outcome at two, different points in time (a “paired” t-test).\n• Whether a single mean is different than some value, e.g., if the mean is not zero (a “one-sample t-test”). These are often done in linear regressions to test of a given parameter weight is significantly different from zero."
  },
  {
    "objectID": "vocab.html#footnotes",
    "href": "vocab.html#footnotes",
    "title": "Appendix B — Common/Confusing Statistical & Scientific Terms",
    "section": "",
    "text": "“Generalized autoregressive conditional heteroskedasticity model” anyone?↩︎\nMore specifically, these are terms were the intercept and/or slope is allowed to vary for each level, e.g., for each patient.↩︎\nWhich, of course, never applies to dissertations.↩︎\nSure, you could also/instead give lighter weights to some values—e.g., values from over-represented groups—but we usually instead give heavier weights to members of underrepresented groups.↩︎"
  },
  {
    "objectID": "decision_trees.html#references-and-guides",
    "href": "decision_trees.html#references-and-guides",
    "title": "Appendix C — Statistical Analysis Decision Trees and Guides",
    "section": "C.1 References and Guides",
    "text": "C.1 References and Guides\nThe following are sources that discuss guidelines, etc. for which statistic to choose.\n\nC.1.1 Correlations & Associations\n\nKhamis (2008) clearly presents which measure of association/correlation to use with various types of data, along with some guides on interpreting the strengths of these measures. Their recommendations are summarized in this table, reproduced from their Summary section:\n\n\n\nTable C.1: Types of Correlation Statistics\n\n\n\n\nVariable X\n\n\n\n\n\nVariable Y\nNominal\nOrdinal\nContinuous\n\n\nNominal\n\\(\\phi\\) coefficient or\nGoodman & Kruskal’s \\(\\lambda\\)\nRank biserial1\nPoint biserial\n\n\nOrdinal\nRank biserial\nKendall’s \\(\\tau_{b}\\) or\nSpearman’s \\(\\rho\\)\nKendall’s \\(\\tau_{b}\\) or\nSpearman’s \\(\\rho\\)\n\n\nContinuous\nPoint biserial\nKendall’s \\(\\tau_{b}\\) or\nSpearman’s \\(\\rho\\)\nPearson’s r or\nSpearman’s \\(\\rho\\)"
  },
  {
    "objectID": "decision_trees.html#simple-graphics",
    "href": "decision_trees.html#simple-graphics",
    "title": "Appendix C — Statistical Analysis Decision Trees and Guides",
    "section": "C.2 Simple Graphics",
    "text": "C.2 Simple Graphics\nThe following trees are simple files that organize the analyses one commonly uses to test straight-forward hypothesis tests between relatively small groups of variables, e.g., one outcome and one or two predictors. They all top out at ANOVAs, and thus effectively fill in the gap I left unfilled for what is covered before the model building focused in our curriculum.\n\nHowell (2008) covers analyses from correlations to ANOVAs. The benefit of this tree is its simplicity; the deficit is it lack of specificity between parametric and non-parametric analyses.\nCorston & Colman’s (2000) tree is also a simple “cheat sheet” file like Howell’s, but contains more information about distinguishing between parametric and non-parametric tests (via the level of measurement of the given variables)."
  },
  {
    "objectID": "decision_trees.html#online-trees",
    "href": "decision_trees.html#online-trees",
    "title": "Appendix C — Statistical Analysis Decision Trees and Guides",
    "section": "C.3 Online Trees",
    "text": "C.3 Online Trees\nThese are website that let your choose the analysis by answering a series of questions. They tend to be more thorough than the simple graphics, but require a more involved process to get to the solution.\n\nMicrOsiris’s decision tree allows one to step through questions to determine what analysis to conduct; it also provides a nice summary page that indicates which function to use to conduct a given analysis in SPSS, SAS, and their own freeware stat program, MicrOsiris.\nNIST’s Decision Tree for Key Comparisons is more than just that. As the About page says, the tree “guides users through a series of hypothesis tests intended to help them in deciding upon an appropriate statistical model for their particular data.” One can first enter or upload (a .csv file) to the site and then see what their tree recommends as analyses for those data. Pretty cool, huh?\nStatistical Test Flowchart doesn’t take as many steps as, e.g., MicrOsisis’s tree. It thus presents less specific results at the end, but it gives more of an explanation of what it recommends along with links to how to conduct the given analysis in R, SPSS, and Stata.\n\n\n\n\n\n\n\n\n\n\nKhamis, H. (2008). Measures of association: How to choose? Journal of Diagnostic Medical Sonography, 24(3), 155–162. https://doi.org/10.1177/8756479308317006\n\n\nWeisburd, D., & Britt, C. (2007). Measures of association for nominal and ordinal variables (pp. 335–380). Springer US. https://doi.org/10.1007/978-0-387-34113-2_13"
  },
  {
    "objectID": "decision_trees.html#footnotes",
    "href": "decision_trees.html#footnotes",
    "title": "Appendix C — Statistical Analysis Decision Trees and Guides",
    "section": "",
    "text": "Weisburd and Britt (2007) give a good, further coverage of analyzing associations between nominal and ordinal data.↩︎"
  }
]